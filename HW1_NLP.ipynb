{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP HW 1\n",
    "\n",
    "### Niv Aharon Cohen and Omri Sason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1+2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import nltk\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Our libraries\n",
    "import utils\n",
    "import text_functions\n",
    "from  text_functions import nltk_tokenize, spacy_tokenize, nltk_lemmatize, spacy_lemmatize, nltk_stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "spam_df = utils.load_dataset(\"spam.csv\")\n",
    "\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SMS messages: 5572\n",
      "Number of spam messages: 747\n",
      "Total word count: 90106\n",
      "Average number of words per message: 16.17\n",
      "5 most frequent words: [('i', 3001), ('to', 2242), ('you', 2240), ('a', 1433), ('the', 1328)]\n",
      "Number of rare words: 4376\n"
     ]
    }
   ],
   "source": [
    "# Calculate data statistics\n",
    "stats = utils.calculate_statistics(spam_df)\n",
    "utils.display_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#### Loading necessary packages ####\n",
    "\n",
    "\n",
    "nltk_data_path = Path(r\"C:\\\\Users\\\\97254\\\\Desktop\\\\niv\\\\AFEKA\\\\M.sc Machine learning\\\\AI 2th year\\\\NLP\\\\NLP HOMEWORK\\\\NLP_HW1\")  # Adjust this path if necessary\n",
    "\n",
    "# Ensure the path exists\n",
    "if not nltk_data_path.exists():\n",
    "    print(f\"Warning: The path {nltk_data_path} does not exist!\")\n",
    "\n",
    "# Add the path to NLTK's data path\n",
    "nltk.data.path.append(str(nltk_data_path))\n",
    "\n",
    "# Ensure 'punkt' is downloaded to the specified location\n",
    "nltk.download('punkt', download_dir=str(nltk_data_path))\n",
    "nltk.download('punkt_tab',download_dir=str(nltk_data_path))\n",
    "\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today. \n",
      "\n",
      "NLTK Tokenization:\n",
      "['I', \"'m\", 'gon', 'na', 'be', 'home', 'soon', 'and', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', \"'ve\", 'cried', 'enough', 'today', '.']\n",
      "\n",
      "spaCy Tokenization:\n",
      "['I', \"'m\", 'gon', 'na', 'be', 'home', 'soon', 'and', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', \"'ve\", 'cried', 'enough', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_line = spam_df['message'].iloc[10]  # Modify the index if needed\n",
    "print(\"Original text:\")\n",
    "print(text_line, \"\\n\")\n",
    "\n",
    "# Tokenize using NLTK\n",
    "nltk_tokens = nltk_tokenize(text_line)\n",
    "print(\"NLTK Tokenization:\")\n",
    "print(nltk_tokens)\n",
    "\n",
    "# Tokenize using spaCy\n",
    "spacy_tokens = spacy_tokenize(text_line, nlp)\n",
    "print(\"\\nspaCy Tokenization:\")\n",
    "print(spacy_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens (NLTK): 11520\n",
      "Number of unique tokens (spaCy): 11582\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire dataframe\n",
    "nltk_tokens_set = text_functions.create_token_set_nltk(spam_df)\n",
    "spacy_tokens_set = text_functions.create_token_set_spacy(spam_df, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique tokens (NLTK):\", len(nltk_tokens_set))\n",
    "print(\"Number of unique tokens (spaCy):\", len(spacy_tokens_set))\n",
    "\n",
    "#print(nltk_tokens_set)\n",
    "#print(spacy_tokens_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: \n",
    "In the example from row 10 of the dataset , the tokenization output of NLTK and spaCy is similar overall. Although, we can see that by tokenizing the entire dataset, there is a difference in the size of the sets of NLTK and spaCy. Apparently, spaCy did additional tokenization to some of the words in the messages, thus, creating a bigger set than NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading additional libreries from nltk ###\n",
    "\n",
    "# Ensure NLTK data path and download WordNet data\n",
    "\n",
    "nltk.data.path.append(str(nltk_data_path))\n",
    "nltk.download('wordnet', download_dir=str(nltk_data_path))\n",
    "nltk.download('omw-1.4', download_dir=str(nltk_data_path))  # Required for WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "\n",
      "NLTK Lemmatization:\n",
      "['I', \"'m\", 'gon', 'na', 'be', 'home', 'soon', 'and', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', \"'ve\", 'cried', 'enough', 'today', '.']\n",
      "\n",
      "spaCy Lemmatization:\n",
      "['I', 'be', 'go', 'to', 'be', 'home', 'soon', 'and', 'I', 'do', 'not', 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', 'have', 'cry', 'enough', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "text_line = spam_df['message'].iloc[10]  # Modify the index if needed\n",
    "print(\"Original Text:\")\n",
    "print(text_line)\n",
    "\n",
    "# Perform NLTK lemmatization\n",
    "nltk_lemmas = nltk_lemmatize(text_line)\n",
    "print(\"\\nNLTK Lemmatization:\")\n",
    "print(nltk_lemmas)\n",
    "\n",
    "# Perform spaCy lemmatization\n",
    "spacy_lemmas = spacy_lemmatize(text_line, nlp)\n",
    "print(\"\\nspaCy Lemmatization:\")\n",
    "print(spacy_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (NLTK): 11052\n",
      "Number of unique lemmas (spaCy): 9843\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmas_set, spacy_lemmas_set = text_functions.create_lemma_set(nltk_tokens_set, spacy_tokens_set, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique lemmas (NLTK):\", len(nltk_lemmas_set))\n",
    "print(\"Number of unique lemmas (spaCy):\", len(spacy_lemmas_set))\n",
    "\n",
    "#print(nltk_lemmas_set)\n",
    "#print(spacy_lemmas_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "Difference between the two libraries is: \n",
    "\n",
    "NLTK results in tokens that are closer to the original text and keeps contractions split into components, for example, \"'m\" stayes the same. \n",
    "\n",
    "Spacy resolves contractions into their expanded or semantic forms and reduces words to their base forms, for example, \"'m\" becomes \"be\", \"na\" becomes \"to\".\n",
    "\n",
    "We see that NLTK's set is larger than spaCy's set, this is because NLTK's lemmatizer (WordNetLemmatizer) relies on the WordNet lexical database, which sometimes returns the input token itself when no clear lemma is found. This behavior can result in more lemmas.\n",
    "spaCy's lemmatizer has its own robust set of rules and models, which might consolidate more tokens into the same lemma, reducing the total count of unique lemmas.\n",
    "\n",
    "We can also observe that the length of the set decreased after the lemmatization process. This is because lemmatization reduces words to their base form, which can lead to duplicates from different forms of the same word. These duplicates are discarded, resulting in a smaller set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.\n",
    "\n",
    "(spaCy library does not have stemming method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "\n",
      "NLTK Stemming: [\"i'm\", 'gonna', 'be', 'home', 'soon', 'and', 'i', \"don't\", 'want', 'to', 'talk', 'about', 'thi', 'stuff', 'anymor', 'tonight,', 'k?', \"i'v\", 'cri', 'enough', 'today.']\n"
     ]
    }
   ],
   "source": [
    "text_line = spam_df['message'].iloc[10]  # Modify the index if needed\n",
    "nltk_stemmed = nltk_stemming(text_line)\n",
    "\n",
    "\n",
    "print(\"Original Text:\", text_line)\n",
    "print(\"\\nNLTK Stemming:\", nltk_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stemms (NLTK): 8110\n"
     ]
    }
   ],
   "source": [
    "nltk_stemmed_set = text_functions.nltk_set_stemming(nltk_tokens_set)\n",
    "print(\"Number of unique stemms (NLTK):\", len(nltk_stemmed_set))\n",
    "\n",
    "#print(nltk_stemmed_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    " NLTK stemming truncates words to their root forms without regard for their linguistic correctness, often producing incomplete or non-dictionary words, such as \"thi\" (from \"this\") and \"cri\" (from \"cried\").  \n",
    " \n",
    " The NLTK stemming process reduced the number of unique tokens to 8110. This reduction occurs because stemming removes affixes from words, often truncating them to a common root form. For example, \"running\", \"runner\", and \"ran\" may all be reduced to \"run\". As a result, different word forms that belong to the same root or word family are collapsed into a single stem, reducing the overall number of unique tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that satisfies the condition.\n"
     ]
    }
   ],
   "source": [
    "message_to_remove = text_functions.find_message_to_remove_stemm_less_lemma_equal(spam_df)\n",
    "\n",
    "if message_to_remove:\n",
    "    print(\"Message to remove:\", message_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results: \n",
    "\n",
    "Task 8 requires a message where stemming removes tokens, but lemmatization does not. This is unlikely because stemming reduces tokens more aggressively than lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that satisfies the condition.\n"
     ]
    }
   ],
   "source": [
    "message_to_remove_9 = text_functions.find_message_to_remove_lemma_less_stemm_equal(spam_df)\n",
    "\n",
    "if message_to_remove_9:\n",
    "    print(\"Message to remove:\", message_to_remove_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "\n",
    "Task 9 requires a message where lemmatization reduces tokens, but stemming does not. This is also unlikely because lemmatization is more conservative, and any reduction in lemmatized tokens would likely also reduce the count of stemmed tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's tokenization breaks the text into individual components, such as words, punctuation marks, and contractions, while preserving their original forms. For example, \"I'm\" becomes [\"I\", \"'m\"], \"gonna\" becomes [\"gon\", \"na\"], and punctuation like \"?\" and \".\" are treated as separate tokens.\n",
    "\n",
    "Lemmatization refines the tokens into their dictionary forms (lemmas) based on grammatical context. In this case, lemmatization yields the same result as tokenization because the words in the text are already in their base forms (e.g., \"cried\" remains \"cried\" as it's a past-tense verb in its base inflection).\n",
    "\n",
    "Stemming, on the other hand, reduces words to their root forms by truncating suffixes or prefixes. This approach is less context-sensitive and often creates non-standard outputs. For instance, \"cried\" is stemmed to \"cri,\" \"anymore\" to \"anymor,\" and \"this\" to \"thi.\" Unlike lemmatization, stemming disregards grammatical correctness and focuses solely on stripping affixes, sometimes leading to distorted forms.\n",
    "\n",
    "Overall, tokenization preserves the structure, lemmatization maintains grammatical accuracy, and stemming sacrifices precision for simplicity, making it less suitable for nuanced text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.\n",
    "\n",
    "##### Tokenization:\n",
    "SpaCy's tokenization splits the text into individual components, similar to NLTK. It identifies contractions like \"I'm\" as separate tokens (['I', \"'m\"]) and splits colloquial expressions like \"gonna\" into ['gon', 'na']. Punctuation marks are also treated as standalone tokens, such as ',', '?', and '.'. This tokenization aligns closely with NLTK, as it retains the text's structure without modifications.\n",
    "\n",
    "##### Lemmatization:\n",
    "SpaCy's lemmatization refines tokens into their base forms or lemmas, taking into account grammatical context. This approach is more sophisticated than NLTK's, as seen in several examples:\n",
    "\n",
    "\"gon\" (from \"gonna\") is lemmatized to \"go,\" capturing its intended meaning.\n",
    "\"n't\" is transformed into \"not,\" ensuring semantic clarity.\n",
    "\"cried\" becomes \"cry,\" its base verb form.\n",
    "\"I've\" splits into \"I\" and \"have,\" preserving grammatical accuracy.\n",
    "Unlike NLTK, SpaCy applies grammatical rules to produce more meaningful base forms, such as converting contractions and colloquialisms into their standard equivalents.\n",
    "\n",
    "##### Summary:\n",
    "SpaCy excels in lemmatization by providing grammatically accurate and meaningful base forms, surpassing NLTK in understanding the context. Both libraries perform similarly in tokenization, splitting text into logical units while retaining the original structure. For stemming, NLTK offers a basic tool, but SpaCy's advanced lemmatization effectively makes stemming unnecessary in many use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN — Google sued the Consumer Financial Protection Bureau on Friday, challenging the agency’s decision to place Google’s payment division under federal supervision. In a copy of the lawsuit provided by Google, the company said the CFPB’s supervision would be a “burdensome form of regulation” imposed based on a “small number of unsubstantiated user complaints.” The CFPB’s decision related to a Google peer-to-peer payment product no longer offered in the United States. The lawsuit, filed in the US district court in Washington, DC, comes after the CFPB published an order announcing supervisory authority of Google Payment Corp. The agency alleged that Google’s handling of its payment products may pose a risk to consumers. The CFPB cited customer complaints, including that Google failed to properly investigate instances where money was transferred in error. The legal fight between Google and the CFPB, the government agency founded to enforce consumer protection laws, comes amid a push by big tech companies, including Google, Apple and Samsung, into financial products. In a statement, Google spokesperson José Castañeda said Google’s payment products never posed a risk to users. “This is a clear case of government overreach involving Google Pay peer-to-peer payments, which never raised risks and is no longer provided in the U.S., and we are challenging it in court,” he said. In the company’s lawsuit, Google argued that the CFPB committed a legal error by setting an “exceedingly low bar” for what it counts as sufficient risks to consumers. “As a matter of common sense, a product that no longer exists is incapable of posing such risks,” the lawsuit said. However, the CFPB said the discontinuation of Google’s payment products did not release it from agency supervision. Supervisory authority over Google’s payments division would allow the CFPB to oversee its operations, ensuring they comply with consumer financial laws. In Google’s lawsuit, the company alleged the CFPB would subject Google to on-site examinations and requests for confidential documents and information. In 2022, the CFPB announced it would begin examining nonbank financial institutions that pose a risk to consumers. “This authority gives us critical agility to move as quickly as the market, allowing us to conduct examinations of financial companies posing risks to consumers and stop harm before it spreads,” said CFPB Director Rohit Chopra in 2022. This story has been updated with additional details and context. CNN’s Clare Duffy contributed to reporting. This report has been updated with additional content.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url = 'https://edition.cnn.com/2024/12/06/business/google-pay-lawsuit-cfpb/index.html'\n",
    "\n",
    "# Send a GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Remove unwanted sections (footers, ads, etc.)\n",
    "for unwanted_section in soup(['footer', 'aside', 'div', 'span', 'figcaption', 'figure']):\n",
    "    # You can refine this further by filtering specific classes or ids\n",
    "    if 'advertisement' in unwanted_section.get('class', []):\n",
    "        unwanted_section.decompose()  # Remove the section\n",
    "\n",
    "# Find the main article content (you may need to inspect the HTML structure to find the right tag)\n",
    "article_content = soup.find('div', {'class': 'article-body'})  # Adjust as per the website structure\n",
    "\n",
    "# If you cannot find the main article content directly, look for alternative tags/classes like 'content', 'main', etc.\n",
    "if not article_content:\n",
    "    article_content = soup.find('main')\n",
    "\n",
    "# Extract and clean the article text\n",
    "if article_content:\n",
    "    clean_text = article_content.get_text()\n",
    "    #print(clean_text)  # Print or process the article content\n",
    "else:\n",
    "    print(\"Article content not found\")\n",
    "\n",
    "\n",
    "# Find the index of the word \"CNN\" and extract text from there\n",
    "start_index = clean_text.find(\"CNN\")\n",
    "if start_index != -1:\n",
    "    extracted_text = clean_text[start_index:]\n",
    "else:\n",
    "    extracted_text = \"The word 'CNN' was not found in the text.\"\n",
    "\n",
    "# Remove excessive new lines from text\n",
    "extracted_text = extracted_text.replace(\"\\n\", \"\")\n",
    "\n",
    "# Remove newlines and normalize spacing\n",
    "cleaned_text = \" \".join(extracted_text.split())\n",
    "\n",
    "# Print the extracted text\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              message\n",
      "0   CNN — Google sued the Consumer Financial Prote...\n",
      "1   In a copy of the lawsuit provided by Google, t...\n",
      "2   ” The CFPB’s decision related to a Google peer...\n",
      "3   The lawsuit, filed in the US district court in...\n",
      "4   The agency alleged that Google’s handling of i...\n",
      "5   The CFPB cited customer complaints, including ...\n",
      "6   The legal fight between Google and the CFPB, t...\n",
      "7   In a statement, Google spokesperson José Casta...\n",
      "8   “This is a clear case of government overreach ...\n",
      "9                                                   S\n",
      "10     , and we are challenging it in court,” he said\n",
      "11  In the company’s lawsuit, Google argued that t...\n",
      "12  “As a matter of common sense, a product that n...\n",
      "13  However, the CFPB said the discontinuation of ...\n",
      "14  Supervisory authority over Google’s payments d...\n",
      "15  In Google’s lawsuit, the company alleged the C...\n",
      "16  In 2022, the CFPB announced it would begin exa...\n",
      "17  “This authority gives us critical agility to m...\n",
      "18  This story has been updated with additional de...\n",
      "19         CNN’s Clare Duffy contributed to reporting\n",
      "20  This report has been updated with additional c...\n"
     ]
    }
   ],
   "source": [
    "# Split the text into sentences by \".\"\n",
    "sentences_url = [sentence.strip() for sentence in cleaned_text.split('.') if sentence.strip()]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_clean_text = pd.DataFrame(sentences_url, columns=[\"message\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique url tokens (NLTK): 214\n",
      "Number of unique url tokens (spaCy): 214\n",
      "{'was', 'are', 'the', 'Friday', 'information', 'Chopra', 'small', '2022', 'tech', 'agency', 'As', 'companies', 'exists', 'incapable', 'order', 'Corp', 'market', 'unsubstantiated', 'and', 'matter', 'such', 'comply', 'us', 'that', 'details', 'payment', 'contributed', 'US', 'not', 'However', 'allowing', 'risk', 'they', 'posing', 'bar', 'nonbank', 'operations', 'discontinuation', 'of', 'requests', 'consumer', 'spokesperson', 'subject', '“', ',', 'company', 'sense', 'context', 'report', 'been', 'in', '’', 'updated', 'with', 'between', 'after', 'Google', 'form', 'offered', 'Clare', 'Duffy', 'CFPB', 'a', 'cited', 'protection', 'peer-to-peer', 'imposed', 'move', 'we', 'case', 'supervisory', 'on', 'Supervisory', 'enforce', 'sued', 'decision', 'examining', 'number', 'regulation', 'lawsuit', 'States', 'division', 'money', 'Castañeda', 'U', 'from', 'In', 'Washington', 'federal', 'investigate', 'setting', 'did', 'spreads', 'story', 'to', 'posed', 'raised', 'what', 'into', 'S', 'products', 'founded', 'for', 'CNN', 'oversee', 'Consumer', 'published', 'agility', 'is', 'Payment', 'laws', 'quickly', 'Apple', 'amid', 'low', 'risks', 'announced', 'fight', 'provided', 'burdensome', 'supervision', 'financial', 'complaints', 'confidential', 'exceedingly', 'by', 'This', 'Rohit', 'announcing', 'copy', 'court', 'Bureau', 'authority', 'payments', 'José', 'as', 'DC', 'release', 'handling', 'common', 'users', 'pose', 'Pay', 'Samsung', 'under', 'additional', 'push', '—', 'overreach', 'harm', 'stop', 'Financial', 'instances', 'comes', 'big', 'related', 'on-site', 'he', 'would', 'user', 'Director', 'argued', 'Protection', 'transferred', 'gives', 'involving', 'committed', 'counts', 'allow', 'longer', 'begin', 'it', 'has', 'place', 'The', 'ensuring', 'before', 'statement', 'including', 'no', 'may', 'reporting', 'an', 'be', 'district', 'critical', 'clear', 'said', 'sufficient', 'examinations', 'documents', 's', 'legal', 'alleged', 'content', '”', 'failed', 'its', 'government', 'over', 'never', 'where', 'customer', 'challenging', 'properly', 'error', 'institutions', 'United', 'product', 'which', 'consumers', 'based', 'filed', 'conduct'}\n",
      "{'was', 'are', 'the', 'Friday', 'information', 'Chopra', 'small', '2022', 'tech', 'agency', 'As', 'companies', 'exists', 'incapable', 'order', 'Corp', 'market', 'unsubstantiated', 'and', 'matter', 'such', 'comply', 'us', 'that', 'details', 'payment', 'contributed', 'US', 'not', 'However', 'allowing', 'risk', 'they', 'posing', 'bar', 'nonbank', 'operations', 'discontinuation', 'of', 'requests', 'consumer', 'spokesperson', 'subject', '“', ',', 'company', 'sense', 'context', 'report', 'been', 'in', 'updated', 'with', 'between', 'after', 'Google', 'form', '-', 'offered', 'Clare', 'Duffy', 'CFPB', 'a', 'cited', 'protection', 'move', 'imposed', 'we', 'case', 'supervisory', 'on', 'Supervisory', 'enforce', 'sued', 'decision', 'examining', 'number', 'regulation', 'lawsuit', 'States', 'division', 'money', 'Castañeda', 'U', 'from', 'In', 'Washington', 'federal', 'investigate', 'setting', 'did', 'spreads', 'story', 'to', 'posed', 'raised', 'what', 'into', 'S', 'products', 'founded', 'for', 'CNN', 'oversee', 'Consumer', 'published', 'agility', 'is', 'Payment', 'laws', 'quickly', 'Apple', 'amid', 'low', 'risks', 'announced', 'fight', 'provided', 'burdensome', 'supervision', 'financial', 'complaints', 'site', 'confidential', '’s', 'exceedingly', 'by', 'This', 'Rohit', 'announcing', 'copy', 'court', 'Bureau', 'authority', 'payments', 'José', 'as', 'DC', 'release', 'handling', 'common', 'users', 'pose', 'Pay', 'Samsung', 'under', 'additional', 'push', '—', 'overreach', 'peer', 'harm', 'stop', 'Financial', 'instances', 'comes', 'big', 'related', 'he', 'would', 'user', 'Director', 'argued', 'Protection', 'transferred', 'gives', 'involving', 'committed', 'counts', 'allow', 'longer', 'begin', 'it', 'has', 'place', 'The', 'ensuring', 'before', 'statement', 'including', 'no', 'may', 'reporting', 'an', 'be', 'district', 'critical', 'clear', 'said', 'sufficient', 'examinations', 'documents', 'legal', 'alleged', 'content', '”', 'failed', 'its', 'government', 'over', 'never', 'where', 'customer', 'challenging', 'properly', 'error', 'institutions', 'United', 'product', 'which', 'consumers', 'based', 'filed', 'conduct'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the entire dataframe\n",
    "nltk_tokens_set_url = text_functions.create_token_set_nltk(df_clean_text)\n",
    "spacy_tokens_set_url = text_functions.create_token_set_spacy(df_clean_text, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique url tokens (NLTK):\", len(nltk_tokens_set_url))\n",
    "print(\"Number of unique url tokens (spaCy):\", len(spacy_tokens_set_url))\n",
    "\n",
    "\n",
    "print(nltk_tokens_set_url)\n",
    "print(spacy_tokens_set_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "NLTK and spaCy produce nearly identical sets of tokens, with NLTK containing one additional token compared to spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (NLTK): 206\n",
      "Number of unique lemmas (spaCy): 192\n",
      "{'are', 'information', 'the', 'Friday', 'Chopra', 'small', '2022', 'examination', 'tech', 'agency', 'As', 'exists', 'incapable', 'wa', 'order', 'Corp', 'market', 'instance', 'institution', 'unsubstantiated', 'and', 'matter', 'such', 'comply', 'that', 'payment', 'contributed', 'US', 'not', 'However', 'allowing', 'risk', 'they', 'posing', 'bar', 'nonbank', 'discontinuation', 'of', 'consumer', 'spokesperson', 'subject', '“', ',', 'company', 'sense', 'document', 'context', 'been', 'updated', 'in', '’', 'with', 'between', 'after', 'Google', 'form', 'ha', 'offered', 'Clare', 'Duffy', 'CFPB', 'a', 'protection', 'peer-to-peer', 'imposed', 'move', 'we', 'case', 'supervisory', 'on', 'Supervisory', 'enforce', 'sued', 'decision', 'examining', 'number', 'regulation', 'lawsuit', 'States', 'division', 'money', 'Castañeda', 'U', 'from', 'In', 'Washington', 'federal', 'investigate', 'setting', 'did', 'story', 'count', 'to', 'posed', 'which', 'raised', 'what', 'into', 'S', 'for', 'founded', 'oversee', 'CNN', 'agility', 'Consumer', 'published', 'is', 'Payment', 'quickly', 'Apple', 'complaint', 'spread', 'amid', 'low', 'announced', 'fight', 'provided', 'burdensome', 'supervision', 'financial', 'confidential', 'law', 'exceedingly', 'by', 'This', 'Rohit', 'announcing', 'copy', 'court', 'Bureau', 'authority', 'José', 'DC', 'release', 'handling', 'common', 'pose', 'come', 'Pay', 'Samsung', 'under', 'additional', 'request', 'push', '—', 'overreach', 'harm', 'give', 'stop', 'Financial', 'big', 'related', 'on-site', 'he', 'would', 'user', 'Director', 'argued', 'Protection', 'transferred', 'involving', 'committed', 'allow', 'longer', 'begin', 'it', 'place', 'The', 'ensuring', 'u', 'operation', 'before', 'statement', 'including', 'no', 'may', 'reporting', 'an', 'be', 'district', 'critical', 'clear', 'said', 'sufficient', 'detail', 's', 'legal', 'alleged', 'content', '”', 'failed', 'government', 'over', 'never', 'where', 'customer', 'challenging', 'properly', 'error', 'United', 'product', 'report', 'cited', 'based', 'filed', 'conduct'}\n",
      "{'base', 'information', 'the', 'Friday', 'Chopra', 'small', '2022', 'examination', 'tech', 'agency', 'publish', 'unsubstantiate', 'incapable', 'order', 'Corp', 'market', 'instance', 'institution', 'file', 'and', 'matter', 'such', 'comply', 'that', 'payment', 'US', 'not', 'risk', 'relate', 'they', 'bar', 'nonbank', 'impose', 'discontinuation', 'of', 'update', 'consumer', 'spokesperson', 'subject', ',', 'company', 'sense', 'document', 'context', 'in', 'with', 'between', 'after', 'Google', 'form', '\"', '-', 'Clare', 'Duffy', 'CFPB', 'a', 'this', 'protection', 'move', 'do', 'long', 'we', 'case', 'supervisory', 'involve', 'on', 'Supervisory', 'enforce', 'decision', 'number', 'regulation', 'lawsuit', 'States', 'division', 'cite', 'money', 'Castañeda', 'U', 'from', 'Washington', 'federal', 'investigate', 'setting', 'story', 'count', 'to', 'contribute', 'which', 'what', 'into', 'S', 'for', 'oversee', 'agility', 'CNN', 'announce', 'Payment', 'quickly', 'Apple', 'complaint', 'spread', 'handle', 'amid', 'low', 'fight', 'burdensome', 'found', 'supervision', 'financial', 'site', 'confidential', 'law', '’s', 'argue', 'exceedingly', 'by', 'Rohit', 'commit', 'offer', 'copy', 'court', 'challenge', 'however', 'Bureau', 'authority', 'as', 'José', 'DC', 'release', 'common', 'pose', 'come', 'Pay', 'raise', 'have', 'Samsung', 'under', 'additional', 'request', 'push', '—', 'overreach', 'peer', 'harm', 'transfer', 'give', 'stop', 'big', 'he', 'would', 'user', 'Director', 'Protection', 'allow', 'begin', 'it', 'provide', 'fail', 'place', 'ensuring', 'before', 'operation', 'statement', 'exist', 'say', 'sue', 'no', 'may', 'an', 'be', 'district', 'critical', 'clear', 'sufficient', 'examine', 'detail', 'legal', 'content', 'its', 'government', 'over', 'allege', 'never', 'where', 'customer', 'properly', 'error', 'include', 'United', 'product', 'report', 'conduct'}\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmas_set_url, spacy_lemmas_set_url = text_functions.create_lemma_set(nltk_tokens_set_url, spacy_tokens_set_url, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique lemmas (NLTK):\", len(nltk_lemmas_set_url))\n",
    "print(\"Number of unique lemmas (spaCy):\", len(spacy_lemmas_set_url))\n",
    "\n",
    "print(nltk_lemmas_set_url)\n",
    "print(spacy_lemmas_set_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "\n",
    "The comparison between NLTK and spaCy on the lemmas of the provided text reveals several differences. NLTK identified 207 unique lemmas, while spaCy identified 192, indicating that NLTK retains more variations as separate lemmas. For example, NLTK includes lemmas like “are” and “been” that are missing in spaCy, whereas spaCy includes lemmas like “base” and “handle” not present in NLTK. SpaCy appears to apply broader normalization, reducing words like “argued” to “argue” and “handled” to “handle,” while NLTK keeps them distinct. Additionally, punctuation is handled differently; NLTK retains symbols like ““” and “,” separately, while spaCy consolidates some punctuation and includes symbols like “—” as lemmas. Both tools handle proper nouns and acronyms, such as “CFPB” and “Google,” similarly, preserving them as is. Overall, NLTK preserves more granular distinctions, while spaCy emphasizes broader reductions for normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stemms (NLTK): 191\n",
      "{'ensur', 'are', 'base', 'the', 'clare', 'small', '2022', 'tech', 'divis', 'publish', 'provid', 'unsubstanti', 'overse', 'cfpb', 'incap', 'wa', 'order', 'market', 'rohit', 'file', 'argu', 'and', 'matter', 'such', 'us', 'that', 'announc', 'payment', 'howev', 'not', 'corp', 'risk', 'they', 'bar', 'addit', 'instanc', 'nonbank', 'dc', 'institut', 'of', 'josé', 'subject', 'spokesperson', '“', ',', 'document', 'involv', 'context', 'enforc', 'feder', 'been', 'in', '’', 'challeng', 'with', 'releas', 'between', 'after', 'form', 'supervis', 'ha', 'confidenti', 'suffici', 'a', 'on-sit', 'move', 'we', 'case', 'director', 'includ', 'on', 'relat', 'contribut', 'number', 'regul', 'lawsuit', 'consum', 'money', 'cite', 'inform', 'from', 'washington', 'agil', 'agenc', 'did', 'count', 'to', 'copi', 'critic', 'set', 'financi', 'investig', 'which', 'what', 'into', 'for', 'discontinu', 'peer-to-p', 'is', 'complaint', 'spread', 'samsung', 'amid', 'unit', 'low', 'fight', 'found', 'sens', 'alleg', 'properli', 'law', 'by', 'commit', 'googl', 'offer', 'court', 'befor', 'as', 'compani', 'impos', 'handl', 'common', 'pose', 'custom', 'come', 'under', 'transfer', 'request', 'push', '—', 'overreach', 'harm', 'give', 'stop', 'protect', 'big', 'author', 'he', 'would', 'user', 'supervisori', 'allow', 'longer', 'burdensom', 'begin', 'it', 'thi', 'fail', 'place', 'duffi', 'updat', 'exceedingli', 'u', 'statement', 'stori', 'exist', 'no', 'quickli', 'su', 'may', 'an', 'oper', 'be', 'district', 'examin', 'clear', 'said', 'appl', 'cnn', 'detail', 'decis', 's', 'legal', 'content', '”', 'compli', 'pay', 'castañeda', 'over', 'never', 'where', 'govern', 'error', 'rais', 'state', 'friday', 'bureau', 'product', 'report', 'chopra', 'conduct'}\n"
     ]
    }
   ],
   "source": [
    "nltk_stemmed_set_url = text_functions.nltk_set_stemming(nltk_tokens_set_url)\n",
    "print(\"Number of unique stemms (NLTK):\", len(nltk_stemmed_set_url))\n",
    "\n",
    "print(nltk_stemmed_set_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "Stemming reduces the size of the set compared to lemmatization by transforming words to their root forms, effectively merging words with similar roots. This process often removes grammatical nuances, resulting in terms like \"ensur\" that lack complete grammatical meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.8-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function definition \n",
    "def find_message_to_remove(df):\n",
    "    # Initialize stemmer and lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    # Function to preprocess and calculate unique tokens\n",
    "    def get_unique_tokens(messages):\n",
    "        stemmed_tokens = set()\n",
    "        lemmatized_tokens = set()\n",
    "        for msg in messages:\n",
    "            tokens = word_tokenize(msg.lower())\n",
    "            stemmed_tokens.update(stemmer.stem(token) for token in tokens)\n",
    "            lemmatized_tokens.update(lemmatizer.lemmatize(token) for token in tokens)\n",
    "        return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "    # Calculate original token sets\n",
    "    original_stemmed, original_lemmatized = get_unique_tokens(df[\"message\"])\n",
    "\n",
    "    #init flags and string holders for the message removed that make the condition true\n",
    "    msg_less_stem_equal_lemma = \"\"\n",
    "    msg_less_lemma_equal_stem = \"\"\n",
    "\n",
    "    found_msg_1_flag = False\n",
    "    found_msg_2_flag = False\n",
    "\n",
    "\n",
    "    # Find the message to remove\n",
    "    for idx, row in df.iterrows():\n",
    "        # Remove one message\n",
    "        remaining_messages = df.drop(index=idx)[\"message\"]\n",
    "        stemmed, lemmatized = get_unique_tokens(remaining_messages)\n",
    "        \n",
    "        # Check the conditions\n",
    "        if len(stemmed) < len(original_stemmed) and len(lemmatized) == len(original_lemmatized):\n",
    "            print(f\"Message  at index {idx}  causes a change in stemmed tokens but not in lemmatized tokens. \\nmessage: \", row[\"message\"])\n",
    "            #return row[\"message\"]\n",
    "            msg_less_stem_equal_lemma = row[\"message\"] \n",
    "            found_msg_1_flag = True\n",
    "        \n",
    "        if len(lemmatized) < len(original_lemmatized) and len(stemmed) == len(original_stemmed):\n",
    "             print(f\"Message at index {idx} causes a change in lemmatized tokens but not in stemmed tokens. \\nmessage:\", row[\"message\"])\n",
    "            # return row[\"message\"]  # Return the message that causes the change\n",
    "             msg_less_lemma_equal_stem = row[\"message\"] \n",
    "             found_msg_2_flag = True\n",
    "\n",
    "        if (found_msg_1_flag == True and found_msg_2_flag== True):\n",
    "            break # Break the loop if such messages found\n",
    "\n",
    "            \n",
    "\n",
    "    if  (found_msg_1_flag == False and found_msg_2_flag == False):      \n",
    "        print(\"No message found that meets the criteria.\")\n",
    "    \n",
    "  \n",
    "   \n",
    "    #return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that meets the criteria.\n"
     ]
    }
   ],
   "source": [
    "find_message_to_remove(df_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "\n",
    "The absence of a message in the dataset that meets the specified conditions could be due to the inherent differences between stemming and lemmatization processes. Stemming is a more aggressive method that removes affixes to produce a root form, often discarding grammatical context. Lemmatization, on the other hand, generates linguistically valid base forms by considering the word's context and part of speech. This divergence makes it unlikely for a message to exist where removing it alters only one token count while leaving the other unchanged. Additionally, the lack of such messages may reflect the dataset's specific characteristics, where no individual message contains terms that exhibit these unique processing behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.10\n",
    "\n",
    "The comparison between NLTK tokens, lemmas, and stems highlights differences in linguistic processing and simplification. The tokenization process resulted in 214 unique tokens, which include raw words split by spaces and punctuation, preserving grammatical meaning but often containing redundancies such as variations in forms (e.g., \"was\" and \"wa\"). Lemmatization reduced the count to 206 unique lemmas by normalizing words to their base dictionary forms while retaining grammatical context (e.g., \"are\" remains unchanged). Stemming further simplified the text, reducing the count to 191 unique stems by stripping words to their root forms without considering grammatical correctness, leading to more aggressive reductions and potential loss of meaning (e.g., \"ensuring\" becomes \"ensur\"). This progression illustrates how lemmatization provides a balance between grammatical integrity and redundancy, while stemming focuses on simplification at the expense of linguistic nuance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.11\n",
    "\n",
    "The comparison of spaCy's tokenization and lemmatization reveals notable differences in the level of text simplification achieved. SpaCy's tokenization resulted in 214 unique tokens, which include raw words segmented from the text, preserving punctuation and grammatical structure but often containing duplicates or inflected forms (e.g., \"are\" and \"is\"). Lemmatization reduced this count to 192 unique lemmas by normalizing words to their base forms, eliminating redundancies while maintaining grammatical meaning (e.g., \"was\" and \"were\" become \"be\"). Compared to the tokenization step, lemmatization in spaCy offers a streamlined representation of the text by focusing on semantic equivalence, making it particularly useful for tasks requiring a deeper understanding of word meanings and relationships. The smaller number of lemmas compared to tokens reflects a more compact and meaningful vocabulary derived from the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 - 17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.2.2024, 19:26:47] ניב אפקה: ‏ההודעות והשיחות מוצפנות מקצה לקצה. לאף אחד מחוץ לצ'אט הזה, גם לא ל-WhatsApp, אין אפשרות לקרוא אותן ולהאזין להן.\n",
      "‏[13.2.2024, 19:26:47] Omri: CamScanner 2024-02-13 18.57.pdf • ‏11 דפים ‏מסמך הושמט\n",
      "[13.2.2024, 19:26:47] Omri: פתרון שלי להסתברות\n",
      "[13.2.2024, 19:26:47] Omri: אם מצאת טעות על הדרך תעדכן\n",
      "[13.2.2024, 22:29:55] ניב אפקה: איזה מלאך אתה❤️❤️\n",
      "[13.2.2024, 22:30:33] ניב אפקה: טנקיו! מחר לא אוכל לבוא לצערי אז לא אוכל לארח לך חברה\n",
      "[13.2.2024, 22:33:38] Omri: לא תהיה אווירה סטודנטיאלית\n",
      "[13.2.2024, 22:40:58] ניב אפקה: חחחח\n",
      "[21.2.2024, 17:28:21] ניב אפקה: אח שלי, תסלח לי אבל גם הפעם לא אבוא לארח לך חברה, אני איתך מרחוק, ראיתי שגם ברק לא בא לארח לך חברה חחח\n",
      "[21.2.2024, 18:08:57] Omri: בדד אלך גם תפילה אין לי\n",
      "[10.5.2024, 17:41:22] Omri: איפה אתה תגיד לי\n",
      "[11.5.2024, 21:54:00] ניב אפקה: יא מלאך, שבוע טוב שיהיה לך. אני הייתי בזום השבועיים האלה, גם גנבו לי את הקורקינט אז לא היה לי ממש איך להגיע, אבל אגיע בתקווה בשבוע הבא בשביל באווירה הסטודנטיאלית😉\n",
      "[11.5.2024, 21:54:05] ניב אפקה: מ ה שלומך?\n",
      "[11.5.2024, 21:55:36] Omri: היידה! קצת כפרת עוונות יאלה הלך הקורקינט🙈 קדימה ממתינים לך… אם תרצה יש 3 כיוונים לחניה חינם עם הרכב\n",
      "[11.5.2024, 21:55:56] Omri: אצלי בסדר מגיע בכוננות ספיגה לסימסטר\n",
      "[11.5.2024, 22:11:20] ניב אפקה: מעדיף את הקורקינט על פני כפרת עוונות, אולי זה בגלל שזה שנת שמיטה😛\n",
      "[11.5.2024, 22:11:27] ניב אפקה: החניה ששלחו בקבוצה אתה מתכוון?\n",
      "[11.5.2024, 22:12:03] ניב אפקה: זה נדמה לי או שבאמת  אין מבחן בלמידה עמוקה+\n",
      "[11.5.2024, 22:29:50] Omri: לא\n",
      "[11.5.2024, 22:29:57] Omri: יש עוד 3 אחרים\n",
      "[11.5.2024, 22:30:06] Omri: אין מבחן אבל יש פרוייקט\n",
      "[12.5.2024, 9:03:29] ניב אפקה: שמע אתה אשף החניות\n",
      "[12.5.2024, 9:15:28] Omri: יש קבוצה סגורה שהקמנו אם תגיע יותר לכיתה תזכה להצטרף\n",
      "‏[12.5.2024, 9:15:57] Omri: ‏התמונה הושמטה\n",
      "[12.5.2024, 9:16:11] Omri: זו חניה חורנית אבל חינמית ודיי קרובה\n",
      "‏[12.5.2024, 9:17:34] Omri: ‏התמונה הושמטה\n",
      "[12.5.2024, 9:17:50] Omri: לפעמים כאן יש חניה באפור תלוי במזל\n",
      "[12.5.2024, 17:53:05] ניב אפקה: איזה רציני אתה תשמע\n",
      "[12.5.2024, 17:53:23] ניב אפקה: איך המקום החדש? יותר שווה מאפקה? מרגיש הייטקס?\n",
      "[13.5.2024, 18:29:54] Omri: ממש הייטקס\n",
      "[13.5.2024, 18:30:08] Omri: רק אין הרבה אנשים, לא יודע אם זה טוב או לא אבל יש אווירה\n",
      "[15.5.2024, 16:51:23] ניב אפקה: מה זה אתה בזום, אני בשוק\n",
      "[15.5.2024, 16:51:33] Omri: מה אתה בכיתה\n",
      "[15.5.2024, 16:51:36] Omri: לא מאמין\n",
      "[15.5.2024, 16:52:06] ניב אפקה: לא אבל חשבתי לבוא היום לראות את זיו פניך\n",
      "[15.5.2024, 16:52:21] ניב אפקה: אבל אתה לא שם אין שום טעם\n",
      "[15.5.2024, 16:52:28] Omri: אח יקר\n",
      "[15.5.2024, 16:52:33] Omri: היה יום מקולקל בעבודה\n",
      "[15.5.2024, 16:52:55] Omri: רציתי לבוא אבל לא הספקתי להגיע פיזית\n",
      "[15.5.2024, 16:53:39] ניב אפקה: בסוודר גמור, מקווה שהקלקול יעבור במהרה בימינו\n",
      "[15.5.2024, 16:53:43] ניב אפקה: 🙏🏻\n",
      "‏[15.5.2024, 16:54:46] Omri: ‏סטיקר הושמט\n",
      "[1.9.2024, 19:31:28] ניב אפקה: אני עוד שניה מציג מותק\n",
      "[1.9.2024, 19:31:47] Omri: רציתי לוודא שאתם בכיתה\n",
      "[1.9.2024, 19:31:50] Omri: שלא שכחתם\n",
      "[1.9.2024, 21:15:05] ניב אפקה: טנקיו❤️\n",
      "[1.9.2024, 21:15:09] ניב אפקה: מה שלומך?\n",
      "‏[1.9.2024, 21:15:26] Omri: ‏התמונה הושמטה\n",
      "[1.9.2024, 21:15:29] Omri: מחיל אל חיל\n",
      "[1.9.2024, 21:20:42] ניב אפקה: תאמין לי אתה צדיק שיפרח\n",
      "[1.9.2024, 21:20:47] ניב אפקה: כבר פורח\n",
      "‏[23.11.2024, 23:25:41] Omri: finalProject.pdf • ‏7 דפים ‏מסמך הושמט\n",
      "[23.11.2024, 23:25:51] Omri: אהלן קינג מקווה שהיה סופש מוצלח\n",
      "[23.11.2024, 23:26:36] Omri: צריכים לבחור פרוייקט מתוך ה5 נושאים שהיא העלתה\n",
      "[23.11.2024, 23:27:02] Omri: תגיד מה עניין אותך.. או הכי פחות כאב ראש חח\n",
      "[23.11.2024, 23:27:41] Omri: בן ורן כבר כמעט סיימו את הפרוייקט והורידו לעצמם עומס אולי שווה לנסות להתחיל\n",
      "[23.11.2024, 23:31:28] ניב אפקה: יאללות נשמע טוב אסתכל על זה עכשיו, לך יש משהו שתפס את העין?\n",
      "[23.11.2024, 23:31:55] Omri: כן הנושא ל החלומות נשמע יחסית סבבה\n",
      "[23.11.2024, 23:32:13] Omri: אבל תגיד אם ראית משהו עדיף\n",
      "[23.11.2024, 23:34:46] ניב אפקה: מגניב\n",
      "[23.11.2024, 23:35:03] ניב אפקה: האמת גם פרויקט 2 נשמע מגניב, המסכם מאמרים הזה\n",
      "[23.11.2024, 23:35:17] ניב אפקה: אנסה להבין מה יהיה פחות כאב לבצע\n",
      "[23.11.2024, 23:35:25] ניב אפקה: זה גם זורם לך?\n",
      "[23.11.2024, 23:35:37] Omri: זה הקו המנחה\n",
      "[23.11.2024, 23:35:48] Omri: אבדוק עם חבר שמבין\n",
      "[23.11.2024, 23:35:54] Omri: אשאל אותו על 2 הפרוייקטים\n",
      "[23.11.2024, 23:36:14] ניב אפקה: אחלות\n",
      "[23.11.2024, 23:36:28] Omri: יאלה אז 2 או 4 אחזיר תשובה\n",
      "[23.11.2024, 23:36:53] ניב אפקה: 2 או 3 לא?\n",
      "[23.11.2024, 23:37:18] Omri: כן\n",
      "[23.11.2024, 23:37:22] Omri: התבלבלתי\n",
      "[23.11.2024, 23:41:06] ניב אפקה: נשארו לך רק עוד 2 בלבולים\n",
      "[23.11.2024, 23:41:36] Omri: 😆\n",
      "[23.11.2024, 23:41:54] Omri: טוב שלחתי ל2 חברים שיודעים LLM בטח יחזירו תשובה מחר\n",
      "[23.11.2024, 23:42:11] Omri: נראה לי עם ה gpt יהיה לנו סבבה\n",
      "[23.11.2024, 23:43:13] ניב אפקה: אני ב LM\n",
      "[23.11.2024, 23:43:48] ניב אפקה: לגמרי, בתכלס אנחנו שלושה בקבוצה ביחד איתו\n",
      "[23.11.2024, 23:46:58] Omri: אתה שנון מדיי\n",
      "[23.11.2024, 23:47:55] Omri: ביחד איתו אנחנו אחד\n",
      "Inf+2=inf\n",
      "[24.11.2024, 0:00:31] ניב אפקה: חחח לגמרי\n",
      "[24.11.2024, 8:42:38] ניב אפקה: https://youtu.be/eC6Hd1hFvos?si=1zU02x3mj7n-f2ea\n",
      "[24.11.2024, 8:52:39] ניב אפקה: https://medium.com/bitgrit-data-science-publication/fine-tuning-llms-to-write-data-science-code-3a76c23ff6b7\n",
      "[24.11.2024, 8:53:17] ניב אפקה: מסביר איך לקחת מודל מוכן של keras nlp ולעשות לו fine tuning\n",
      "‏[24.11.2024, 9:12:57] Omri: ‏התמונה הושמטה\n",
      "[24.11.2024, 9:17:11] ניב אפקה: מגניביישן\n",
      "[25.11.2024, 8:53:57] ניב אפקה: מה קורה מתוקה? אני התחלתי את חלק 1 של פרויקט 2 ככה לקרוא מ pdf ודברים כאלה, איך אתה רוצה שננהל את זה? נפתח איזה גיט נחמדת?\n",
      "[25.11.2024, 9:12:52] Omri: גם אני התחלתי אתמול\n",
      "[25.11.2024, 9:12:55] Omri: כמעט סיימתי את 1\n",
      "[25.11.2024, 9:13:09] Omri: גיט נאה\n",
      "[25.11.2024, 9:13:18] Omri: מתי אתה זמין היום לטלפון\n",
      "[25.11.2024, 9:15:02] ניב אפקה: אשכרה\n",
      "[25.11.2024, 9:15:09] ניב אפקה: אתה זריז כארנב\n",
      "[25.11.2024, 9:15:21] ניב אפקה: ב2 כזה?\n",
      "[25.11.2024, 9:16:13] Omri: כ צבי\n",
      "[25.11.2024, 9:16:24] Omri: יאלה מעולה\n",
      "[25.11.2024, 9:17:11] ניב אפקה: יאללות\n",
      "[26.11.2024, 15:42:09] ניב אפקה: מה קורה אחי? אני לא מרגיש כל כך טוב אז לא אבוא היום, אני רוצה להתקדם קצת בפרויקט, יכול אולי לשלוח לי מה שעשית בינתיים?\n",
      "‏[26.11.2024, 16:16:49] Omri: ‏סטיקר הושמט\n",
      "[26.11.2024, 16:16:58] Omri: כן תכף אשלח\n",
      "‏[26.11.2024, 16:25:26] Omri: Attention Is All You Need.pdf • ‏15 דפים ‏מסמך הושמט\n",
      "‏[26.11.2024, 16:25:27] Omri: main.py ‏מסמך הושמט\n",
      "‏[26.11.2024, 16:25:27] Omri: parsing.py ‏מסמך הושמט\n",
      "‏[26.11.2024, 16:25:27] Omri: storage.py ‏מסמך הושמט\n",
      "[26.11.2024, 17:30:44] ניב אפקה: טנקיו🙃\n",
      "[26.11.2024, 22:11:49] ניב אפקה: תראה אם אתה מצליח להתחבר לזה:\n",
      "https://github.com/nivco12/finalProjectNLP\n",
      "[27.11.2024, 0:32:31] Omri: הרגשנו בחסרונך היום\n",
      "[27.11.2024, 0:33:04] Omri: אבדוק את זה מחר ב״נ תודה יאח\n",
      "[27.11.2024, 9:22:28] ניב אפקה: ☺️\n",
      "[27.11.2024, 9:22:34] ניב אפקה: תעדכן\n",
      "[27.11.2024, 11:00:37] Omri: הנפקת גיט מרגש\n",
      "[28.11.2024, 9:38:29] ניב אפקה: מה קורה אחי? אני מהיום עד שבת בערב בים המלח עם המשפוחי, בשבוע הבא אני בחופש אז יהיה לי זמן לשבת לקדם את הפרויקט\n",
      "[28.11.2024, 11:03:45] Omri: תהנה יאח אני היום ומחר אנסה לשבת לקדם קצת\n",
      "[28.11.2024, 15:56:11] ניב אפקה: בסדר גמור, אם אתה מסיים משהו תעשה קומיט פוש לגיט\n",
      "[28.11.2024, 15:56:34] ניב אפקה: ואמשיך משם\n",
      "[28.11.2024, 15:56:42] Omri: הלו הלו בלי לדחוף\n",
      "[28.11.2024, 15:56:56] Omri: האמת צריך לעשות ריענון שימוש בגיט\n",
      "[28.11.2024, 16:00:51] ניב אפקה: זה המערב הפרוע פה\n",
      "[28.11.2024, 16:10:41] Omri: קלאב הוטל אילת\n",
      "[1.12.2024, 12:21:07] ניב אפקה: מה קורה מותק? חזרתי מחופשתי ואתחיל לשבת עכשיו נורמלי על הפרויקט שלנו, שינית משהו בסופש או שאני ממשיך מאותו מקום?\n",
      "[1.12.2024, 12:21:29] Omri: היו לי עומס פניות השבוע\n",
      "[1.12.2024, 12:21:37] Omri: אני גם רק עכשיו חוזר לעניינים\n",
      "[1.12.2024, 12:21:48] Omri: 3 דייטים לא בא ברגל\n",
      "[1.12.2024, 12:21:57] Omri: זהו אני עושה הפסקה קצת\n",
      "[1.12.2024, 15:45:05] ניב אפקה: שמע איזה פרפר אתה, צריך לבחור מהרשימה המתנה, תשאיר קצת גם לאחרים\n",
      "[1.12.2024, 16:41:17] ניב אפקה: יש שיעורי בית ב nlp🥲\n",
      "[1.12.2024, 16:41:34] Omri: בדיוק התחלתי לשבת על זה\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whatsapp_text_file_path = 'whatsapp_chat.txt'\n",
    "\n",
    "# Read the file content into the variable\n",
    "with open(whatsapp_text_file_path, 'r', encoding='utf-8') as file:\n",
    "    whatsapp_text = file.read()\n",
    "\n",
    "# Print the content (optional)\n",
    "print(whatsapp_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               DateTime    Sender  \\\n",
      "0   13.2.2024, 19:26:47  ניב אפקה   \n",
      "1   13.2.2024, 19:26:47      Omri   \n",
      "2   13.2.2024, 19:26:47      Omri   \n",
      "3   13.2.2024, 19:26:47      Omri   \n",
      "4   13.2.2024, 22:29:55  ניב אפקה   \n",
      "..                  ...       ...   \n",
      "92  1.12.2024, 12:21:37      Omri   \n",
      "93  1.12.2024, 12:21:48      Omri   \n",
      "94  1.12.2024, 12:21:57      Omri   \n",
      "95  1.12.2024, 15:45:05  ניב אפקה   \n",
      "96  1.12.2024, 16:41:17  ניב אפקה   \n",
      "\n",
      "                                              message  \n",
      "0   ‏ההודעות והשיחות מוצפנות מקצה לקצה. לאף אחד מח...  \n",
      "1   CamScanner 2024-02-13 18.57.pdf • ‏11 דפים ‏מס...  \n",
      "2                                  פתרון שלי להסתברות  \n",
      "3                          אם מצאת טעות על הדרך תעדכן  \n",
      "4                                   איזה מלאך אתה❤️❤️  \n",
      "..                                                ...  \n",
      "92                      אני גם רק עכשיו חוזר לעניינים  \n",
      "93                                3 דייטים לא בא ברגל  \n",
      "94                             זהו אני עושה הפסקה קצת  \n",
      "95  שמע איזה פרפר אתה, צריך לבחור מהרשימה המתנה, ת...  \n",
      "96                               יש שיעורי בית ב nlp🥲  \n",
      "\n",
      "[97 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define regex for splitting based on sender names\n",
    "\n",
    "names_pattern = r\"(?=\\[\\d{1,2}\\.\\d{1,2}\\.\\d{4}, \\d{2}:\\d{2}:\\d{2}\\] (Omri|ניב אפקה))\"\n",
    "\n",
    "# Split the text into messages\n",
    "messages = re.split(names_pattern, whatsapp_text)\n",
    "\n",
    "# Process messages into structured format\n",
    "structured_messages = []\n",
    "for i in range(1, len(messages), 2):  # Skip every alternate index (matched name)\n",
    "    structured_messages.append(messages[i - 1] + messages[i])\n",
    "\n",
    "# Extract message details into a DataFrame\n",
    "message_pattern = r\"\\[(.*?)\\] (.*?): (.*)\"\n",
    "data = []\n",
    "for msg in structured_messages:\n",
    "    match = re.match(message_pattern, msg.strip())\n",
    "    if match:\n",
    "        date_time, sender, content = match.groups()\n",
    "        data.append({\"DateTime\": date_time, \"Sender\": sender, \"message\": content})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_whatsapp_processed_text = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_whatsapp_processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique url tokens (NLTK): 387\n",
      "Number of unique url tokens (spaCy): 394\n",
      "{'כאב', 'בסופש', 'לא', 'מקולקל', 'גנבו', 'ה', 'אין', 'אותו', 'זהו', '?', 'שם', 'הקורקינט', 'יעבור', 'ברגל', 'הקו', 'משהו', 'תעשה', 'לקצה', '\\u200fמסמך', 'בסוודר', 'להן', 'עוונות', 'לנו', ',', 'פרויקט', 'הייתי', 'שבוע', 'שבאמת', 'מצליח', 'היה', 'הכי', 'ולהאזין', 'להתחיל', 'מסיים', 'מחופשתי', 'שלחתי', 'ל', 'קומיט', 'ומחר', 'חחח', 'You', 'פוש', 'תגיד', 'מקצה', 'אשאל', 'תסלח', 'יותר', 'השבוע', 'גיט', 'פרוייקט', 'אחד', 'גם', 'פתרון', 'סיימו', \"לצ'אט\", 'התבלבלתי', 'נורמלי', 'מאפקה', 'מתכוון', 'כפרת', 'בן', 'LLM', 'האלה', 'פחות', 'מחוץ', 'אותן', 'בדד', 'אגיע', 'פורח', 'אהלן', 'הפרויקט', '\\u200fסטיקר', 'אשלח', 'היו', 'צריכים', 'מרחוק', 'לוודא', 'ביחד', 'מרגש', 'החלומות', 'להתחבר', 'מאותו', 'חבר', 'ריענון', 'חברה', 'בלבולים', 'בשביל', 'מתוך', '\\u200f7', 'הושמטה', 'בינתיים', '\\u200f11', 'הקורקינט🙈', 'עם', 'הפרוייקטים', 'איך', 'שלומך', 'טוב', 'המקום', 'אז', 'תראה', 'הוטל', 'Attention', 'חינם', 'חזרתי', 'בית', 'זה', 'לי', 'נושאים', 'בכוננות', 'שניה', 'לשלוח', 'בשוק', 'בפרויקט', 'בקבוצה', 'מעדיף', 'אולי', 'ממתינים', 'יקר', 'העין', 'תהיה', 'חיל', 'כל', 'יא', '2', 'עכשיו', 'פניות', 'מגניב', 'אנחנו', 'חחחח', 'אם', 'איפה', 'מחר', 'הזה', 'לארח', 'אחרים', 'בגלל', 'אלך', 'מה', 'לבחור', 'מותק', 'שלנו', 'All', 'לגמרי', 'כך', 'אנסה', 'צריך', 'אבדוק', 'ל-WhatsApp', 'ממש', 'זורם', 'מלאך', 'שמיטה😛', 'פני', 'הרכב', 'מגיע', 'שלושה', '\\u200f15', 'טעם', 'אסתכל', 'היום', 'איתו', 'נדמה', 'גמור', 'לך…', 'עניין', 'אתה', 'לגיט', 'בעבודה', 'אבוא', 'לקרוא', 'זיו', 'אותך', 'מדיי', 'בגיט', '•', 'המערב', 'לאף', 'לראות', 'שנון', 'פיזית', 'ראש', 'ראית', 'יכול', 'להבין', 'שגם', 'יחסית', 'להגיע', 'או', 'לסימסטר', 'Need.pdf', 'מקווה', 'רק', 'פניך', 'לעצמם', 'באווירה', 'יודע', 'ואתחיל', 'חשבתי', 'חוזר', 'הסטודנטיאלית😉', 'על', 'שתפס', 'אח', 'הנפקת', 'טנקיו❤️', 'gpt', 'יש', 'קצת', '!', 'שלא', 'חברים', 'ברק', 'שינית', 'תעדכן', 'בטח', 'לצערי', 'להתקדם', 'שזה', 'תאמין', 'הרבה', 'הפרוע', 'אחלות', 'שלי', 'LM', '3', 'רציתי', 'אחי', 'שימוש', 'האמת', 'עומס', 'main.py', 'לעניינים', 'מצאת', 'לבוא', 'מחיל', 'בכיתה', 'סופש', 'בשבוע', 'storage.py', 'בלמידה', 'בתקווה', 'חח', ':', 'אני', 'אל', 'לנסות', 'היידה', 'תפילה', 'נשמע', 'והורידו', 'יאללות', 'עושה', 'הלו', 'שווה', 'ממשיך', 'הספקתי', 'שום', 'כמעט', 'תשובה', 'תכף', 'יום', 'לקדם', 'סטודנטיאלית', 'ואמשיך', 'מציג', '😆', 'כיוונים', 'לשבת', '4', 'שהיא', 'צדיק', 'הייטקס', 'בתכלס', 'נראה', 'המתנה', 'תשאיר', '2024-02-13', 'להסתברות', 'והשיחות', 'דייטים', 'שיהיה', 'פרפר', 'לדחוף', 'השבועיים', 'יחזירו', 'המסכם', 'לחניה', 'שיודעים', '18.57.pdf', 'טנקיו🙃', 'אילת', 'finalProject.pdf', 'המנחה', 'שכחתם', 'אנשים', 'שמבין', '\\u200fההודעות', 'החדש', 'עוד', 'מהרשימה', 'הפרוייקט', 'מבחן', '.', 'parsing.py', 'בלי', 'תהנה', 'שיעורי', 'כן', 'בימינו', 'רוצה', 'אצלי', 'לאחרים', 'טעות', 'אבל', 'משם', 'פה', 'שאתם', 'הלך', 'העלתה', 'מ', 'מרגיש', 'יאח', 'תרצה', 'עדיף', 'שמע', 'רציני', 'לזה', 'שיפרח', 'בסדר', 'קורה', 'הושמט', 'קינג', 'מוצפנות', 'מאמרים', '..', 'קלאב', 'הדרך', 'בא', 'הבא', 'nlp🥲', 'שאני', 'ששלחו', '\\u200fהתמונה', 'ה5', 'שעשית', 'דפים', 'מקום', 'ספיגה', 'שנת', 'טנקיו', 'עמוקה+', 'קדימה', 'ב', 'שהיה', 'איתך', 'איזה', 'בזום', 'מוצלח', 'סבבה', 'Is', 'אפשרות', 'נשארו', 'יהיה', 'לך', 'אווירה', 'תשמע', 'הפעם', '🙏🏻', 'יאלה', 'לבצע', 'כבר', 'אחזיר', 'לעשות', 'את', 'CamScanner', 'ל2', 'הנושא', 'אתה❤️❤️', 'הפסקה', 'החניה', 'במהרה', 'אוכל', 'ראיתי', 'ורן', 'מאמין', 'שהקלקול'}\n",
      "{'כאב', 'בסופש', 'לא', 'מקולקל', 'גנבו', '🙈', 'ה', 'אין', 'אותו', 'זהו', '?', 'שם', 'הקורקינט', 'יעבור', 'ברגל', 'הקו', 'משהו', 'תעשה', 'לקצה', '\\u200fמסמך', 'בסוודר', 'להן', 'עוונות', 'לנו', ',', 'פרויקט', 'הייתי', 'שבוע', 'שבאמת', 'מצליח', 'היה', 'הכי', 'ולהאזין', 'להתחיל', 'מסיים', 'מחופשתי', '😉', 'שלחתי', 'ל', 'קומיט', 'ומחר', 'חחח', 'You', 'פוש', 'תגיד', 'מקצה', 'אשאל', 'תסלח', 'יותר', 'השבוע', 'גיט', 'פרוייקט', 'אחד', 'גם', 'פתרון', 'סיימו', \"לצ'אט\", 'התבלבלתי', '…', 'נורמלי', 'מאפקה', 'מתכוון', 'כפרת', 'בן', 'LLM', 'האלה', 'פחות', 'מחוץ', 'אותן', 'בדד', 'אגיע', '️', 'פורח', 'אהלן', 'הפרויקט', '\\u200fסטיקר', 'אשלח', 'היו', '🙏', 'מרחוק', 'לוודא', 'צריכים', 'ביחד', 'מרגש', 'החלומות', 'להתחבר', 'מאותו', 'חבר', 'ריענון', 'חברה', 'בלבולים', 'בשביל', 'מתוך', '😛', '\\u200f7', 'הושמטה', 'בינתיים', '\\u200f11', 'עם', 'הסטודנטיאלית', 'הפרוייקטים', 'איך', 'שלומך', 'טוב', 'המקום', 'אז', 'תראה', 'הוטל', 'Attention', 'חינם', 'חזרתי', 'בית', 'זה', 'לי', 'נושאים', 'בכוננות', 'שניה', 'לשלוח', '🙃', 'בשוק', 'בפרויקט', 'בקבוצה', 'מעדיף', 'אולי', 'ממתינים', 'יקר', 'העין', 'תהיה', 'חיל', 'WhatsApp', 'כל', 'יא', '2', 'עכשיו', 'פניות', 'מגניב', 'אנחנו', 'חחחח', 'אם', 'איפה', 'מחר', 'הזה', 'לארח', 'שמיטה', 'אחרים', 'בגלל', 'אלך', 'מה', 'לבחור', 'מותק', 'שלנו', 'All', 'לגמרי', 'כך', 'אנסה', 'צריך', 'אבדוק', 'ממש', 'זורם', 'מלאך', 'פני', 'הרכב', 'מגיע', 'שלושה', '\\u200f15', 'טעם', 'אסתכל', 'היום', 'איתו', 'נדמה', 'גמור', 'עניין', 'אתה', 'לגיט', 'בעבודה', 'אבוא', 'לקרוא', 'זיו', 'אותך', 'מדיי', 'בגיט', '•', 'המערב', 'לאף', 'לראות', 'שנון', 'פיזית', 'ראש', 'ראית', 'יכול', 'להבין', 'שגם', 'יחסית', 'להגיע', 'או', 'לסימסטר', 'Need.pdf', 'מקווה', 'רק', 'פניך', 'לעצמם', 'באווירה', 'יודע', 'ואתחיל', 'חשבתי', 'חוזר', 'על', 'שתפס', 'אח', 'הנפקת', 'gpt', 'יש', 'קצת', '!', 'שלא', 'חברים', 'ברק', 'שינית', '-', 'תעדכן', 'בטח', 'לצערי', 'להתקדם', 'שזה', 'תאמין', 'הרבה', 'הפרוע', 'אחלות', 'שלי', 'LM', '🏻', '3', 'רציתי', 'אחי', 'שימוש', 'האמת', 'עומס', 'main.py', 'לעניינים', 'מצאת', 'לבוא', 'מחיל', 'בכיתה', 'סופש', ' ', 'בשבוע', 'storage.py', 'בלמידה', 'בתקווה', 'חח', ':', 'אני', 'אל', 'לנסות', 'היידה', 'תפילה', 'נשמע', 'והורידו', 'יאללות', 'עושה', 'הלו', 'שווה', 'ממשיך', 'הספקתי', 'שום', 'כמעט', 'תשובה', 'תכף', 'יום', 'לקדם', 'סטודנטיאלית', 'ואמשיך', 'מציג', '😆', '02', 'כיוונים', 'לשבת', '4', 'שהיא', 'צדיק', 'הייטקס', 'בתכלס', 'נראה', 'המתנה', 'תשאיר', 'להסתברות', 'והשיחות', 'דייטים', 'שיהיה', 'פרפר', 'לדחוף', 'השבועיים', 'יחזירו', 'המסכם', 'לחניה', 'שיודעים', '18.57.pdf', 'אילת', 'finalProject.pdf', 'המנחה', 'שכחתם', 'אנשים', 'שמבין', '\\u200fההודעות', 'החדש', 'עוד', 'מהרשימה', 'הפרוייקט', 'מבחן', '.', 'parsing.py', 'בלי', 'תהנה', 'שיעורי', 'כן', 'בימינו', 'רוצה', 'אצלי', 'לאחרים', 'טעות', 'אבל', 'משם', 'פה', 'שאתם', 'הלך', 'העלתה', 'מ', 'מרגיש', 'יאח', 'תרצה', 'עדיף', 'שמע', '❤', 'רציני', 'לזה', '13', 'שיפרח', 'בסדר', 'קורה', 'הושמט', 'קינג', 'מוצפנות', 'מאמרים', '..', 'קלאב', 'הדרך', 'בא', 'הבא', 'nlp🥲', 'שאני', 'ששלחו', '\\u200fהתמונה', 'ה5', 'שעשית', 'דפים', 'מקום', 'ספיגה', 'שנת', 'טנקיו', 'עמוקה+', 'קדימה', 'ב', 'שהיה', 'איתך', 'איזה', 'בזום', 'מוצלח', 'סבבה', 'Is', 'אפשרות', 'נשארו', 'יהיה', 'לך', 'אווירה', 'תשמע', '2024', 'הפעם', 'יאלה', 'לבצע', 'כבר', 'אחזיר', 'לעשות', 'את', 'CamScanner', 'ל2', 'הנושא', 'הפסקה', 'החניה', 'במהרה', 'אוכל', 'ראיתי', 'ורן', 'מאמין', 'שהקלקול'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the entire dataframe\n",
    "nltk_tokens_set_whatsapp = text_functions.create_token_set_nltk(df_whatsapp_processed_text)\n",
    "spacy_tokens_set_whatsapp = text_functions.create_token_set_spacy(df_whatsapp_processed_text, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique url tokens (NLTK):\", len(nltk_tokens_set_whatsapp))\n",
    "print(\"Number of unique url tokens (spaCy):\", len(spacy_tokens_set_whatsapp))\n",
    "\n",
    "\n",
    "print(nltk_tokens_set_whatsapp)\n",
    "print(spacy_tokens_set_whatsapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences of tokenization between NLTK and spaCy: \n",
    "\n",
    "NLTK often treats punctuation as standalone tokens and may have limited support for modern elements like emojis, while spaCy is better equipped for handling such characters, treating emojis and symbols as distinct tokens. spaCy also combines or interprets certain adjacent special characters and compound tokens more effectively than NLTK. These differences make spaCy preferable for nuanced, modern applications and NLTK better for traditional text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (NLTK): 387\n",
      "Number of unique lemmas (spaCy): 394\n",
      "{'כאב', 'בסופש', 'לא', 'מקולקל', 'גנבו', 'ה', 'אין', 'אותו', 'זהו', '?', 'שם', 'הקורקינט', 'יעבור', 'ברגל', 'הקו', 'משהו', 'תעשה', 'לקצה', '\\u200fמסמך', 'בסוודר', 'להן', 'עוונות', 'לנו', ',', 'פרויקט', 'הייתי', 'שבוע', 'מצליח', 'שבאמת', 'היה', 'הכי', 'ולהאזין', 'להתחיל', 'מסיים', 'מחופשתי', 'שלחתי', 'ל', 'קומיט', 'ומחר', 'חחח', 'You', 'פוש', 'תגיד', 'מקצה', 'אשאל', 'תסלח', 'יותר', 'השבוע', 'גיט', 'פרוייקט', 'אחד', 'גם', 'פתרון', 'סיימו', \"לצ'אט\", 'התבלבלתי', 'נורמלי', 'מאפקה', 'מתכוון', 'כפרת', 'בן', 'LLM', 'האלה', 'פחות', 'מחוץ', 'אותן', 'בדד', 'אגיע', 'פורח', 'אהלן', 'הפרויקט', '\\u200fסטיקר', 'אשלח', 'היו', 'צריכים', 'מרחוק', 'לוודא', 'ביחד', 'מרגש', 'החלומות', 'להתחבר', 'מאותו', 'חבר', 'ריענון', 'חברה', 'בלבולים', 'בשביל', 'מתוך', '\\u200f7', 'הושמטה', 'בינתיים', '\\u200f11', 'הקורקינט🙈', 'עם', 'הפרוייקטים', 'איך', 'שלומך', 'טוב', 'המקום', 'אז', 'תראה', 'הוטל', 'Attention', 'חינם', 'חזרתי', 'בית', 'זה', 'לי', 'נושאים', 'בכוננות', 'שניה', 'לשלוח', 'בשוק', 'בפרויקט', 'בקבוצה', 'מעדיף', 'אולי', 'ממתינים', 'יקר', 'העין', 'תהיה', 'חיל', 'כל', 'יא', '2', 'עכשיו', 'פניות', 'מגניב', 'אנחנו', 'חחחח', 'אם', 'איפה', 'מחר', 'הזה', 'לארח', 'אחרים', 'בגלל', 'אלך', 'מה', 'לבחור', 'מותק', 'שלנו', 'All', 'לגמרי', 'כך', 'אנסה', 'צריך', 'אבדוק', 'ל-WhatsApp', 'ממש', 'זורם', 'מלאך', 'שמיטה😛', 'פני', 'הרכב', 'מגיע', 'שלושה', '\\u200f15', 'טעם', 'אסתכל', 'היום', 'איתו', 'נדמה', 'גמור', 'לך…', 'עניין', 'אתה', 'לגיט', 'בעבודה', 'אבוא', 'לקרוא', 'זיו', 'אותך', 'מדיי', 'בגיט', '•', 'המערב', 'לאף', 'לראות', 'שנון', 'פיזית', 'ראש', 'ראית', 'יכול', 'להבין', 'שגם', 'יחסית', 'להגיע', 'או', 'לסימסטר', 'Need.pdf', 'מקווה', 'רק', 'פניך', 'לעצמם', 'באווירה', 'יודע', 'ואתחיל', 'חשבתי', 'חוזר', 'הסטודנטיאלית😉', 'על', 'שתפס', 'אח', 'הנפקת', 'טנקיו❤️', 'gpt', 'יש', 'קצת', '!', 'שלא', 'חברים', 'ברק', 'שינית', 'תעדכן', 'בטח', 'לצערי', 'להתקדם', 'שזה', 'תאמין', 'הרבה', 'הפרוע', 'אחלות', 'שלי', 'LM', '3', 'רציתי', 'אחי', 'שימוש', 'האמת', 'עומס', 'main.py', 'לעניינים', 'מצאת', 'לבוא', 'מחיל', 'בכיתה', 'סופש', 'בשבוע', 'storage.py', 'בלמידה', 'בתקווה', 'חח', ':', 'אני', 'אל', 'לנסות', 'היידה', 'תפילה', 'נשמע', 'והורידו', 'יאללות', 'עושה', 'הלו', 'שווה', 'ממשיך', 'הספקתי', 'שום', 'כמעט', 'תשובה', 'תכף', 'יום', 'לקדם', 'סטודנטיאלית', 'ואמשיך', 'מציג', '😆', 'כיוונים', 'לשבת', '4', 'שהיא', 'צדיק', 'הייטקס', 'בתכלס', 'נראה', 'המתנה', 'תשאיר', '2024-02-13', 'להסתברות', 'והשיחות', 'דייטים', 'שיהיה', 'פרפר', 'לדחוף', 'השבועיים', 'יחזירו', 'המסכם', 'לחניה', 'שיודעים', '18.57.pdf', 'טנקיו🙃', 'אילת', 'finalProject.pdf', 'המנחה', 'שכחתם', 'אנשים', 'שמבין', '\\u200fההודעות', 'החדש', 'עוד', 'מהרשימה', 'הפרוייקט', 'מבחן', '.', 'parsing.py', 'בלי', 'תהנה', 'שיעורי', 'כן', 'בימינו', 'רוצה', 'אצלי', 'לאחרים', 'טעות', 'אבל', 'משם', 'פה', 'שאתם', 'הלך', 'העלתה', 'מ', 'מרגיש', 'יאח', 'תרצה', 'עדיף', 'שמע', 'רציני', 'לזה', 'שיפרח', 'בסדר', 'קורה', 'הושמט', 'קינג', 'מוצפנות', 'מאמרים', '..', 'קלאב', 'הדרך', 'בא', 'הבא', 'nlp🥲', 'שאני', 'ששלחו', '\\u200fהתמונה', 'ה5', 'שעשית', 'דפים', 'מקום', 'ספיגה', 'שנת', 'טנקיו', 'עמוקה+', 'קדימה', 'ב', 'שהיה', 'איתך', 'איזה', 'בזום', 'מוצלח', 'סבבה', 'Is', 'אפשרות', 'נשארו', 'יהיה', 'לך', 'אווירה', 'תשמע', 'הפעם', '🙏🏻', 'יאלה', 'לבצע', 'כבר', 'אחזיר', 'לעשות', 'את', 'CamScanner', 'ל2', 'הנושא', 'אתה❤️❤️', 'הפסקה', 'החניה', 'במהרה', 'אוכל', 'ראיתי', 'ורן', 'מאמין', 'שהקלקול'}\n",
      "{'כאב', 'בסופש', 'לא', 'מקולקל', 'גנבו', '🙈', 'ה', 'אין', 'אותו', 'זהו', '?', 'שם', 'הקורקינט', 'יעבור', 'ברגל', 'הקו', 'משהו', 'תעשה', 'לקצה', '\\u200fמסמך', 'בסוודר', 'להן', 'עוונות', 'לנו', ',', 'פרויקט', 'הייתי', 'שבוע', 'מצליח', 'שבאמת', 'היה', 'הכי', 'ולהאזין', 'להתחיל', 'מסיים', 'מחופשתי', '😉', 'שלחתי', 'ל', 'קומיט', 'ומחר', 'חחח', 'פוש', 'תגיד', 'מקצה', 'אשאל', 'תסלח', 'יותר', 'השבוע', 'גיט', 'פרוייקט', 'אחד', 'גם', 'פתרון', '  ', 'סיימו', \"לצ'אט\", 'התבלבלתי', '…', 'נורמלי', 'מאפקה', 'מתכוון', 'כפרת', 'בן', 'LLM', 'האלה', 'פחות', 'מחוץ', 'אותן', 'בדד', 'אגיע', '️', 'פורח', 'אהלן', 'הפרויקט', '\\u200fסטיקר', 'אשלח', 'you', 'היו', '🙏', 'מרחוק', 'לוודא', 'צריכים', 'ביחד', 'מרגש', 'החלומות', 'be', 'להתחבר', 'מאותו', 'חבר', 'ריענון', 'חברה', 'בלבולים', 'בשביל', 'מתוך', '😛', '\\u200f7', 'הושמטה', 'בינתיים', '\\u200f11', 'עם', 'הסטודנטיאלית', 'הפרוייקטים', 'איך', 'שלומך', 'טוב', 'המקום', 'אז', 'תראה', 'הוטל', 'Attention', 'חינם', 'חזרתי', 'בית', 'זה', 'לי', 'נושאים', 'בכוננות', 'שניה', 'לשלוח', '🙃', 'בשוק', 'בפרויקט', 'בקבוצה', 'מעדיף', 'אולי', 'ממתינים', 'יקר', 'העין', 'תהיה', 'חיל', 'WhatsApp', 'כל', 'יא', 'finalproject.pdf', '2', 'עכשיו', 'פניות', 'מגניב', 'אנחנו', 'חחחח', 'אם', 'איפה', 'מחר', 'הזה', 'לארח', 'שמיטה', 'אחרים', 'בגלל', 'אלך', 'מה', 'לבחור', 'מותק', 'שלנו', 'לגמרי', 'כך', 'אנסה', 'צריך', 'אבדוק', 'ממש', 'זורם', 'מלאך', 'פני', 'הרכב', 'מגיע', 'שלושה', '\\u200f15', 'טעם', 'אסתכל', 'היום', 'איתו', 'נדמה', 'גמור', 'עניין', 'אתה', 'לגיט', 'בעבודה', 'אבוא', 'לקרוא', 'זיו', 'אותך', 'מדיי', 'בגיט', '•', 'המערב', 'לאף', 'לראות', 'שנון', 'פיזית', 'ראש', 'ראית', 'יכול', 'להבין', 'שגם', 'יחסית', 'להגיע', 'או', 'לסימסטר', 'Need.pdf', 'מקווה', 'רק', 'פניך', 'לעצמם', 'באווירה', 'יודע', 'ואתחיל', 'חשבתי', 'חוזר', 'על', 'שתפס', 'אח', 'הנפקת', 'gpt', 'יש', 'קצת', '!', 'שלא', 'חברים', 'ברק', 'שינית', '-', 'תעדכן', 'בטח', 'לצערי', 'להתקדם', 'שזה', 'תאמין', 'הרבה', 'הפרוע', 'אחלות', 'שלי', 'LM', '🏻', '3', 'רציתי', 'אחי', 'שימוש', 'האמת', 'עומס', 'main.py', 'לעניינים', 'מצאת', 'לבוא', 'מחיל', 'בכיתה', 'סופש', 'בשבוע', 'storage.py', 'בלמידה', 'בתקווה', 'all', 'חח', ':', 'אני', 'אל', 'לנסות', 'היידה', 'תפילה', 'נשמע', 'והורידו', 'יאללות', 'עושה', 'הלו', 'שווה', 'ממשיך', 'הספקתי', 'שום', 'כמעט', 'תשובה', 'תכף', 'יום', 'לקדם', 'סטודנטיאלית', 'ואמשיך', 'מציג', '😆', '02', 'כיוונים', 'לשבת', '4', 'שהיא', 'צדיק', 'הייטקס', 'בתכלס', 'נראה', 'המתנה', 'תשאיר', 'להסתברות', 'והשיחות', 'דייטים', 'שיהיה', 'פרפר', 'לדחוף', 'השבועיים', 'יחזירו', 'המסכם', 'לחניה', 'שיודעים', '18.57.pdf', 'אילת', 'המנחה', 'שכחתם', 'אנשים', 'שמבין', '\\u200fההודעות', 'החדש', 'עוד', 'מהרשימה', 'הפרוייקט', 'מבחן', '.', 'parsing.py', 'בלי', 'תהנה', 'שיעורי', 'כן', 'בימינו', 'רוצה', 'אצלי', 'לאחרים', 'טעות', 'אבל', 'משם', 'פה', 'שאתם', 'הלך', 'העלתה', 'מ', 'מרגיש', 'יאח', 'תרצה', 'עדיף', 'שמע', '❤', 'רציני', 'לזה', '13', 'שיפרח', 'בסדר', 'קורה', 'הושמט', 'קינג', 'מוצפנות', 'מאמרים', '..', 'קלאב', 'הדרך', 'בא', 'הבא', 'nlp🥲', 'שאני', 'ששלחו', '\\u200fהתמונה', 'ה5', 'שעשית', 'דפים', 'מקום', 'ספיגה', 'שנת', 'טנקיו', 'עמוקה+', 'קדימה', 'ב', 'שהיה', 'איתך', 'איזה', 'בזום', 'מוצלח', 'סבבה', 'אפשרות', 'נשארו', 'יהיה', 'לך', 'אווירה', 'תשמע', '2024', 'הפעם', 'יאלה', 'לבצע', 'כבר', 'אחזיר', 'לעשות', 'את', 'CamScanner', 'ל2', 'הנושא', 'הפסקה', 'החניה', 'במהרה', 'אוכל', 'ראיתי', 'ורן', 'מאמין', 'שהקלקול'}\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmas_set_whatsapp, spacy_lemmas_set_whatsapp = text_functions.create_lemma_set(nltk_tokens_set_whatsapp, spacy_tokens_set_whatsapp, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique lemmas (NLTK):\", len(nltk_lemmas_set_whatsapp))\n",
    "print(\"Number of unique lemmas (spaCy):\", len(spacy_lemmas_set_whatsapp))\n",
    "\n",
    "print(nltk_lemmas_set_whatsapp)\n",
    "print(spacy_lemmas_set_whatsapp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stemms (NLTK): 387\n",
      "{'כאב', 'בסופש', 'לא', 'מקולקל', 'גנבו', 'ה', 'אין', 'אותו', 'זהו', '?', 'שם', 'הקורקינט', 'יעבור', 'ברגל', 'הקו', 'משהו', 'תעשה', 'לקצה', '\\u200fמסמך', 'בסוודר', 'להן', 'עוונות', 'לנו', 'llm', ',', 'פרויקט', 'הייתי', 'שבוע', 'מצליח', 'שבאמת', 'היה', 'הכי', 'ולהאזין', 'להתחיל', 'מסיים', 'ל-whatsapp', 'מחופשתי', 'שלחתי', 'ל', 'קומיט', 'ומחר', 'חחח', 'פוש', 'תגיד', 'מקצה', 'אשאל', 'תסלח', 'יותר', 'השבוע', 'גיט', 'פרוייקט', 'אחד', 'is', 'גם', 'פתרון', 'סיימו', \"לצ'אט\", 'התבלבלתי', 'נורמלי', 'מאפקה', 'מתכוון', 'כפרת', 'בן', 'האלה', 'פחות', 'מחוץ', 'אותן', 'בדד', 'אגיע', 'פורח', 'אהלן', 'הפרויקט', '\\u200fסטיקר', 'אשלח', 'you', 'היו', 'צריכים', 'מרחוק', 'לוודא', 'ביחד', 'מרגש', 'החלומות', 'parsing.pi', 'להתחבר', 'מאותו', 'חבר', 'ריענון', 'חברה', 'בלבולים', 'בשביל', 'מתוך', '\\u200f7', 'הושמטה', 'בינתיים', '\\u200f11', 'הקורקינט🙈', 'עם', 'הפרוייקטים', 'איך', 'שלומך', 'טוב', 'המקום', 'אז', 'תראה', 'הוטל', 'חינם', 'חזרתי', 'בית', 'זה', 'לי', 'נושאים', 'בכוננות', 'שניה', 'לשלוח', 'בשוק', 'בפרויקט', 'בקבוצה', 'מעדיף', 'אולי', 'ממתינים', 'יקר', 'העין', 'תהיה', 'חיל', 'כל', 'יא', 'finalproject.pdf', '2', 'עכשיו', 'פניות', 'מגניב', 'אנחנו', 'חחחח', 'אם', 'איפה', 'lm', 'need.pdf', 'מחר', 'הזה', 'לארח', 'אחרים', 'בגלל', 'אלך', 'מה', 'לבחור', 'מותק', 'שלנו', 'לגמרי', 'כך', 'אנסה', 'צריך', 'אבדוק', 'ממש', 'זורם', 'מלאך', 'שמיטה😛', 'פני', 'הרכב', 'מגיע', 'שלושה', 'attent', '\\u200f15', 'טעם', 'main.pi', 'אסתכל', 'היום', 'איתו', 'נדמה', 'גמור', 'לך…', 'עניין', 'אתה', 'לגיט', 'בעבודה', 'אבוא', 'לקרוא', 'זיו', 'אותך', 'מדיי', 'בגיט', '•', 'המערב', 'לאף', 'לראות', 'שנון', 'פיזית', 'ראש', 'ראית', 'יכול', 'להבין', 'שגם', 'יחסית', 'להגיע', 'או', 'לסימסטר', 'מקווה', 'רק', 'פניך', 'לעצמם', 'באווירה', 'יודע', 'ואתחיל', 'חשבתי', 'חוזר', 'הסטודנטיאלית😉', 'על', 'שתפס', 'אח', 'הנפקת', 'טנקיו❤️', 'gpt', 'יש', 'קצת', '!', 'שלא', 'חברים', 'ברק', 'שינית', 'תעדכן', 'בטח', 'לצערי', 'להתקדם', 'שזה', 'תאמין', 'הרבה', 'הפרוע', 'אחלות', 'שלי', '3', 'רציתי', 'אחי', 'שימוש', 'האמת', 'עומס', 'לעניינים', 'מצאת', 'לבוא', 'מחיל', 'בכיתה', 'סופש', 'בשבוע', 'בלמידה', 'בתקווה', 'all', 'חח', ':', 'אני', 'אל', 'לנסות', 'היידה', 'תפילה', 'נשמע', 'והורידו', 'יאללות', 'עושה', 'הלו', 'שווה', 'ממשיך', 'הספקתי', 'שום', 'כמעט', 'תשובה', 'תכף', 'יום', 'לקדם', 'סטודנטיאלית', 'ואמשיך', 'מציג', '😆', 'כיוונים', 'לשבת', '4', 'שהיא', 'צדיק', 'הייטקס', 'בתכלס', 'נראה', 'המתנה', 'תשאיר', '2024-02-13', 'להסתברות', 'והשיחות', 'דייטים', 'שיהיה', 'פרפר', 'לדחוף', 'השבועיים', 'יחזירו', 'המסכם', 'לחניה', 'שיודעים', '18.57.pdf', 'טנקיו🙃', 'אילת', 'camscann', 'המנחה', 'שכחתם', 'אנשים', 'storage.pi', 'שמבין', '\\u200fההודעות', 'החדש', 'עוד', 'מהרשימה', 'הפרוייקט', 'מבחן', '.', 'בלי', 'תהנה', 'שיעורי', 'כן', 'בימינו', 'רוצה', 'אצלי', 'לאחרים', 'טעות', 'אבל', 'משם', 'פה', 'שאתם', 'הלך', 'העלתה', 'מ', 'מרגיש', 'יאח', 'תרצה', 'עדיף', 'שמע', 'רציני', 'לזה', 'שיפרח', 'בסדר', 'קורה', 'הושמט', 'קינג', 'מוצפנות', 'מאמרים', '..', 'קלאב', 'הדרך', 'בא', 'הבא', 'nlp🥲', 'שאני', 'ששלחו', '\\u200fהתמונה', 'ה5', 'שעשית', 'דפים', 'מקום', 'ספיגה', 'שנת', 'טנקיו', 'עמוקה+', 'קדימה', 'ב', 'שהיה', 'איתך', 'איזה', 'בזום', 'מוצלח', 'סבבה', 'אפשרות', 'נשארו', 'יהיה', 'לך', 'אווירה', 'תשמע', 'הפעם', '🙏🏻', 'יאלה', 'לבצע', 'כבר', 'אחזיר', 'לעשות', 'את', 'ל2', 'הנושא', 'אתה❤️❤️', 'הפסקה', 'החניה', 'במהרה', 'אוכל', 'ראיתי', 'ורן', 'מאמין', 'שהקלקול'}\n"
     ]
    }
   ],
   "source": [
    "nltk_stemmed_set_whatsapp = text_functions.nltk_set_stemming(nltk_tokens_set_whatsapp)\n",
    "print(\"Number of unique stemms (NLTK):\", len(nltk_stemmed_set_whatsapp))\n",
    "\n",
    "print(nltk_stemmed_set_whatsapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of 17.6 and 17.7:\n",
    "\n",
    "Both NLTK and spaCy appear to lack support for lemmatization and stemming in the Hebrew language. As a result, their functionality for Hebrew text is limited to tokenization, producing identical outputs for tokenization, lemmatization, and stemming operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.8-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that meets the criteria.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function definition\n",
    "def find_message_to_remove(df):\n",
    "    # Initialize stemmer and lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    # Function to preprocess and calculate unique tokens\n",
    "    def get_unique_tokens(messages):\n",
    "        stemmed_tokens = set()\n",
    "        lemmatized_tokens = set()\n",
    "        for msg in messages:\n",
    "            tokens = word_tokenize(msg.lower())\n",
    "            stemmed_tokens.update(stemmer.stem(token) for token in tokens)\n",
    "            lemmatized_tokens.update(lemmatizer.lemmatize(token) for token in tokens)\n",
    "        return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "    # Calculate original token sets\n",
    "    original_stemmed, original_lemmatized = get_unique_tokens(df[\"message\"])\n",
    "\n",
    "    #init flags and string holders for the message removed that make the condition true\n",
    "    msg_less_stem_equal_lemma = \"\"\n",
    "    msg_less_lemma_equal_stem = \"\"\n",
    "\n",
    "    found_msg_1_flag = False\n",
    "    found_msg_2_flag = False\n",
    "\n",
    "\n",
    "    # Find the message to remove\n",
    "    for idx, row in df.iterrows():\n",
    "        # Remove one message\n",
    "        remaining_messages = df.drop(index=idx)[\"message\"]\n",
    "        stemmed, lemmatized = get_unique_tokens(remaining_messages)\n",
    "        \n",
    "        # Check the conditions\n",
    "        if len(stemmed) < len(original_stemmed) and len(lemmatized) == len(original_lemmatized):\n",
    "            print(f\"Message  at index {idx}  causes a change in stemmed tokens but not in lemmatized tokens. \\nmessage: \", row[\"message\"])\n",
    "            #return row[\"message\"]\n",
    "            msg_less_stem_equal_lemma = row[\"message\"] \n",
    "            found_msg_1_flag = True\n",
    "        \n",
    "        if len(lemmatized) < len(original_lemmatized) and len(stemmed) == len(original_stemmed):\n",
    "             print(f\"Message at index {idx} causes a change in lemmatized tokens but not in stemmed tokens. \\nmessage:\", row[\"message\"])\n",
    "            # return row[\"message\"]  # Return the message that causes the change\n",
    "             msg_less_lemma_equal_stem = row[\"message\"] \n",
    "             found_msg_2_flag = True\n",
    "\n",
    "        if (found_msg_1_flag == True and found_msg_2_flag== True):\n",
    "            break # Break the loop if such messages found\n",
    "\n",
    "            \n",
    "\n",
    "    if  (found_msg_1_flag == False and found_msg_2_flag == False):      \n",
    "        print(\"No message found that meets the criteria.\")\n",
    "    \n",
    "  \n",
    "   \n",
    "    #return None\n",
    "\n",
    "\n",
    "find_message_to_remove(df_whatsapp_processed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results 17.8-9:\n",
    "\n",
    "For Hebrew messages, due to the limitations of the NLTK and library in handling the Hebrew language effectively, the results of lemmatization and stemming are often identical. This overlap makes it impossible to differentiate between the number of lemmas and stems within the same segment of text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.10-11\n",
    "\n",
    "Both NLTK and spaCy lack support for lemmatization and stemming in the Hebrew language. As a result, their functionality for Hebrew text is limited to tokenization, producing identical outputs for tokenization, lemmatization, and stemming operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
