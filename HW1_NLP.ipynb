{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP HW 1\n",
    "\n",
    "### Niv Aharon Cohen and Omri Sason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1+2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import nltk\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Our libraries\n",
    "import utils\n",
    "import text_functions\n",
    "from  text_functions import nltk_tokenize, spacy_tokenize, nltk_lemmatize, spacy_lemmatize, nltk_stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "spam_df = utils.load_dataset(\"spam.csv\")\n",
    "\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SMS messages: 5572\n",
      "Number of spam messages: 747\n",
      "Total word count: 90106\n",
      "Average number of words per message: 16.17\n",
      "5 most frequent words: [('i', 3001), ('to', 2242), ('you', 2240), ('a', 1433), ('the', 1328)]\n",
      "Number of rare words: 4376\n"
     ]
    }
   ],
   "source": [
    "# Calculate data statistics\n",
    "stats = utils.calculate_statistics(spam_df)\n",
    "utils.display_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#### Loading necessary packages ####\n",
    "\n",
    "\n",
    "nltk_data_path = Path(r\"C:\\\\Users\\\\97254\\\\Desktop\\\\niv\\\\AFEKA\\\\M.sc Machine learning\\\\AI 2th year\\\\NLP\\\\NLP HOMEWORK\\\\NLP_HW1\")  # Adjust this path if necessary\n",
    "\n",
    "# Ensure the path exists\n",
    "if not nltk_data_path.exists():\n",
    "    print(f\"Warning: The path {nltk_data_path} does not exist!\")\n",
    "\n",
    "# Add the path to NLTK's data path\n",
    "nltk.data.path.append(str(nltk_data_path))\n",
    "\n",
    "# Ensure 'punkt' is downloaded to the specified location\n",
    "nltk.download('punkt', download_dir=str(nltk_data_path))\n",
    "nltk.download('punkt_tab',download_dir=str(nltk_data_path))\n",
    "\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today. \n",
      "\n",
      "NLTK Tokenization:\n",
      "['I', \"'m\", 'gon', 'na', 'be', 'home', 'soon', 'and', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', \"'ve\", 'cried', 'enough', 'today', '.']\n",
      "\n",
      "spaCy Tokenization:\n",
      "['I', \"'m\", 'gon', 'na', 'be', 'home', 'soon', 'and', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', \"'ve\", 'cried', 'enough', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_line = spam_df['message'].iloc[10]  # Modify the index if needed\n",
    "print(\"Original text:\")\n",
    "print(text_line, \"\\n\")\n",
    "\n",
    "# Tokenize using NLTK\n",
    "nltk_tokens = nltk_tokenize(text_line)\n",
    "print(\"NLTK Tokenization:\")\n",
    "print(nltk_tokens)\n",
    "\n",
    "# Tokenize using spaCy\n",
    "spacy_tokens = spacy_tokenize(text_line, nlp)\n",
    "print(\"\\nspaCy Tokenization:\")\n",
    "print(spacy_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens (NLTK): 11520\n",
      "Number of unique tokens (spaCy): 11582\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire dataframe\n",
    "nltk_tokens_set = text_functions.create_token_set_nltk(spam_df)\n",
    "spacy_tokens_set = text_functions.create_token_set_spacy(spam_df, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique tokens (NLTK):\", len(nltk_tokens_set))\n",
    "print(\"Number of unique tokens (spaCy):\", len(spacy_tokens_set))\n",
    "\n",
    "#print(nltk_tokens_set)\n",
    "#print(spacy_tokens_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results: \n",
    "In the example from row 10 of the dataset , the tokenization output of NLTK and spaCy is similar overall. Although, we can see that by tokenizing the entire dataset, there is a difference in the size of the sets of NLTK and spaCy. Apparently, spaCy did additional tokenization to some of the words in the messages, thus, creating a bigger set than NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\97254\\Desktop\\niv\\AFEKA\\M.sc Machine\n",
      "[nltk_data]     learning\\AI 2th year\\NLP\\NLP HOMEWORK\\NLP_HW1...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading additional libreries from nltk ###\n",
    "\n",
    "# Ensure NLTK data path and download WordNet data\n",
    "\n",
    "nltk.data.path.append(str(nltk_data_path))\n",
    "nltk.download('wordnet', download_dir=str(nltk_data_path))\n",
    "nltk.download('omw-1.4', download_dir=str(nltk_data_path))  # Required for WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "\n",
      "NLTK Lemmatization:\n",
      "['I', \"'m\", 'gon', 'na', 'be', 'home', 'soon', 'and', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', \"'ve\", 'cried', 'enough', 'today', '.']\n",
      "\n",
      "spaCy Lemmatization:\n",
      "['I', 'be', 'go', 'to', 'be', 'home', 'soon', 'and', 'I', 'do', 'not', 'want', 'to', 'talk', 'about', 'this', 'stuff', 'anymore', 'tonight', ',', 'k', '?', 'I', 'have', 'cry', 'enough', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "text_line = spam_df['message'].iloc[10]  # Modify the index if needed\n",
    "print(\"Original Text:\")\n",
    "print(text_line)\n",
    "\n",
    "# Perform NLTK lemmatization\n",
    "nltk_lemmas = nltk_lemmatize(text_line)\n",
    "print(\"\\nNLTK Lemmatization:\")\n",
    "print(nltk_lemmas)\n",
    "\n",
    "# Perform spaCy lemmatization\n",
    "spacy_lemmas = spacy_lemmatize(text_line, nlp)\n",
    "print(\"\\nspaCy Lemmatization:\")\n",
    "print(spacy_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (NLTK): 11052\n",
      "Number of unique lemmas (spaCy): 9843\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmas_set, spacy_lemmas_set = text_functions.create_lemma_set(nltk_tokens_set, spacy_tokens_set, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique lemmas (NLTK):\", len(nltk_lemmas_set))\n",
    "print(\"Number of unique lemmas (spaCy):\", len(spacy_lemmas_set))\n",
    "\n",
    "#print(nltk_lemmas_set)\n",
    "#print(spacy_lemmas_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "Difference between the two libraries is: \n",
    "\n",
    "NLTK results in tokens that are closer to the original text and keeps contractions split into components, for example, \"'m\" stayes the same. \n",
    "\n",
    "Spacy resolves contractions into their expanded or semantic forms and reduces words to their base forms, for example, \"'m\" becomes \"be\", \"na\" becomes \"to\".\n",
    "\n",
    "We see that NLTK's set is larger than spaCy's set, this is because NLTK's lemmatizer (WordNetLemmatizer) relies on the WordNet lexical database, which sometimes returns the input token itself when no clear lemma is found. This behavior can result in more lemmas.\n",
    "spaCy's lemmatizer has its own robust set of rules and models, which might consolidate more tokens into the same lemma, reducing the total count of unique lemmas.\n",
    "\n",
    "We can also observe that the length of the set decreased after the lemmatization process. This is because lemmatization reduces words to their base form, which can lead to duplicates from different forms of the same word. These duplicates are discarded, resulting in a smaller set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.\n",
    "\n",
    "(spaCy library does not have stemming method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "\n",
      "NLTK Stemming: [\"i'm\", 'gonna', 'be', 'home', 'soon', 'and', 'i', \"don't\", 'want', 'to', 'talk', 'about', 'thi', 'stuff', 'anymor', 'tonight,', 'k?', \"i'v\", 'cri', 'enough', 'today.']\n"
     ]
    }
   ],
   "source": [
    "text_line = spam_df['message'].iloc[10]  # Modify the index if needed\n",
    "nltk_stemmed = nltk_stemming(text_line)\n",
    "\n",
    "\n",
    "print(\"Original Text:\", text_line)\n",
    "print(\"\\nNLTK Stemming:\", nltk_stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stemms (NLTK): 8110\n"
     ]
    }
   ],
   "source": [
    "nltk_stemmed_set = text_functions.nltk_set_stemming(nltk_tokens_set)\n",
    "print(\"Number of unique stemms (NLTK):\", len(nltk_stemmed_set))\n",
    "\n",
    "#print(nltk_stemmed_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    " NLTK stemming truncates words to their root forms without regard for their linguistic correctness, often producing incomplete or non-dictionary words, such as \"thi\" (from \"this\") and \"cri\" (from \"cried\").  \n",
    " \n",
    " The NLTK stemming process reduced the number of unique tokens to 8110. This reduction occurs because stemming removes affixes from words, often truncating them to a common root form. For example, \"running\", \"runner\", and \"ran\" may all be reduced to \"run\". As a result, different word forms that belong to the same root or word family are collapsed into a single stem, reducing the overall number of unique tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that satisfies the condition.\n"
     ]
    }
   ],
   "source": [
    "message_to_remove = text_functions.find_message_to_remove_stemm_less_lemma_equal(spam_df)\n",
    "\n",
    "if message_to_remove:\n",
    "    print(\"Message to remove:\", message_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results: \n",
    "\n",
    "Task 8 requires a message where stemming removes tokens, but lemmatization does not. This is unlikely because stemming reduces tokens more aggressively than lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that satisfies the condition.\n"
     ]
    }
   ],
   "source": [
    "message_to_remove_9 = text_functions.find_message_to_remove_lemma_less_stemm_equal(spam_df)\n",
    "\n",
    "if message_to_remove_9:\n",
    "    print(\"Message to remove:\", message_to_remove_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "\n",
    "Task 9 requires a message where lemmatization reduces tokens, but stemming does not. This is also unlikely because lemmatization is more conservative, and any reduction in lemmatized tokens would likely also reduce the count of stemmed tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK's tokenization breaks the text into individual components, such as words, punctuation marks, and contractions, while preserving their original forms. For example, \"I'm\" becomes [\"I\", \"'m\"], \"gonna\" becomes [\"gon\", \"na\"], and punctuation like \"?\" and \".\" are treated as separate tokens.\n",
    "\n",
    "Lemmatization refines the tokens into their dictionary forms (lemmas) based on grammatical context. In this case, lemmatization yields the same result as tokenization because the words in the text are already in their base forms (e.g., \"cried\" remains \"cried\" as it's a past-tense verb in its base inflection).\n",
    "\n",
    "Stemming, on the other hand, reduces words to their root forms by truncating suffixes or prefixes. This approach is less context-sensitive and often creates non-standard outputs. For instance, \"cried\" is stemmed to \"cri,\" \"anymore\" to \"anymor,\" and \"this\" to \"thi.\" Unlike lemmatization, stemming disregards grammatical correctness and focuses solely on stripping affixes, sometimes leading to distorted forms.\n",
    "\n",
    "Overall, tokenization preserves the structure, lemmatization maintains grammatical accuracy, and stemming sacrifices precision for simplicity, making it less suitable for nuanced text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.\n",
    "\n",
    "##### Tokenization:\n",
    "SpaCy's tokenization splits the text into individual components, similar to NLTK. It identifies contractions like \"I'm\" as separate tokens (['I', \"'m\"]) and splits colloquial expressions like \"gonna\" into ['gon', 'na']. Punctuation marks are also treated as standalone tokens, such as ',', '?', and '.'. This tokenization aligns closely with NLTK, as it retains the text's structure without modifications.\n",
    "\n",
    "##### Lemmatization:\n",
    "SpaCy's lemmatization refines tokens into their base forms or lemmas, taking into account grammatical context. This approach is more sophisticated than NLTK's, as seen in several examples:\n",
    "\n",
    "\"gon\" (from \"gonna\") is lemmatized to \"go,\" capturing its intended meaning.\n",
    "\"n't\" is transformed into \"not,\" ensuring semantic clarity.\n",
    "\"cried\" becomes \"cry,\" its base verb form.\n",
    "\"I've\" splits into \"I\" and \"have,\" preserving grammatical accuracy.\n",
    "Unlike NLTK, SpaCy applies grammatical rules to produce more meaningful base forms, such as converting contractions and colloquialisms into their standard equivalents.\n",
    "\n",
    "##### Summary:\n",
    "SpaCy excels in lemmatization by providing grammatically accurate and meaningful base forms, surpassing NLTK in understanding the context. Both libraries perform similarly in tokenization, splitting text into logical units while retaining the original structure. For stemming, NLTK offers a basic tool, but SpaCy's advanced lemmatization effectively makes stemming unnecessary in many use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN ‚Äî Google sued the Consumer Financial Protection Bureau on Friday, challenging the agency‚Äôs decision to place Google‚Äôs payment division under federal supervision. In a copy of the lawsuit provided by Google, the company said the CFPB‚Äôs supervision would be a ‚Äúburdensome form of regulation‚Äù imposed based on a ‚Äúsmall number of unsubstantiated user complaints.‚Äù The CFPB‚Äôs decision related to a Google peer-to-peer payment product no longer offered in the United States. The lawsuit, filed in the US district court in Washington, DC, comes after the CFPB published an order announcing supervisory authority of Google Payment Corp. The agency alleged that Google‚Äôs handling of its payment products may pose a risk to consumers. The CFPB cited customer complaints, including that Google failed to properly investigate instances where money was transferred in error. The legal fight between Google and the CFPB, the government agency founded to enforce consumer protection laws, comes amid a push by big tech companies, including Google, Apple and Samsung, into financial products. In a statement, Google spokesperson Jos√© Casta√±eda said Google‚Äôs payment products never posed a risk to users. ‚ÄúThis is a clear case of government overreach involving Google Pay peer-to-peer payments, which never raised risks and is no longer provided in the U.S., and we are challenging it in court,‚Äù he said. In the company‚Äôs lawsuit, Google argued that the CFPB committed a legal error by setting an ‚Äúexceedingly low bar‚Äù for what it counts as sufficient risks to consumers. ‚ÄúAs a matter of common sense, a product that no longer exists is incapable of posing such risks,‚Äù the lawsuit said. However, the CFPB said the discontinuation of Google‚Äôs payment products did not release it from agency supervision. Supervisory authority over Google‚Äôs payments division would allow the CFPB to oversee its operations, ensuring they comply with consumer financial laws. In Google‚Äôs lawsuit, the company alleged the CFPB would subject Google to on-site examinations and requests for confidential documents and information. In 2022, the CFPB announced it would begin examining nonbank financial institutions that pose a risk to consumers. ‚ÄúThis authority gives us critical agility to move as quickly as the market, allowing us to conduct examinations of financial companies posing risks to consumers and stop harm before it spreads,‚Äù said CFPB Director Rohit Chopra in 2022. This story has been updated with additional details and context. CNN‚Äôs Clare Duffy contributed to reporting. This report has been updated with additional content.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "url = 'https://edition.cnn.com/2024/12/06/business/google-pay-lawsuit-cfpb/index.html'\n",
    "\n",
    "# Send a GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Remove unwanted sections (footers, ads, etc.)\n",
    "for unwanted_section in soup(['footer', 'aside', 'div', 'span', 'figcaption', 'figure']):\n",
    "    # You can refine this further by filtering specific classes or ids\n",
    "    if 'advertisement' in unwanted_section.get('class', []):\n",
    "        unwanted_section.decompose()  # Remove the section\n",
    "\n",
    "# Find the main article content (you may need to inspect the HTML structure to find the right tag)\n",
    "article_content = soup.find('div', {'class': 'article-body'})  # Adjust as per the website structure\n",
    "\n",
    "# If you cannot find the main article content directly, look for alternative tags/classes like 'content', 'main', etc.\n",
    "if not article_content:\n",
    "    article_content = soup.find('main')\n",
    "\n",
    "# Extract and clean the article text\n",
    "if article_content:\n",
    "    clean_text = article_content.get_text()\n",
    "    #print(clean_text)  # Print or process the article content\n",
    "else:\n",
    "    print(\"Article content not found\")\n",
    "\n",
    "\n",
    "# Find the index of the word \"CNN\" and extract text from there\n",
    "start_index = clean_text.find(\"CNN\")\n",
    "if start_index != -1:\n",
    "    extracted_text = clean_text[start_index:]\n",
    "else:\n",
    "    extracted_text = \"The word 'CNN' was not found in the text.\"\n",
    "\n",
    "# Remove excessive new lines from text\n",
    "extracted_text = extracted_text.replace(\"\\n\", \"\")\n",
    "\n",
    "# Remove newlines and normalize spacing\n",
    "cleaned_text = \" \".join(extracted_text.split())\n",
    "\n",
    "# Print the extracted text\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              message\n",
      "0   CNN ‚Äî Google sued the Consumer Financial Prote...\n",
      "1   In a copy of the lawsuit provided by Google, t...\n",
      "2   ‚Äù The CFPB‚Äôs decision related to a Google peer...\n",
      "3   The lawsuit, filed in the US district court in...\n",
      "4   The agency alleged that Google‚Äôs handling of i...\n",
      "5   The CFPB cited customer complaints, including ...\n",
      "6   The legal fight between Google and the CFPB, t...\n",
      "7   In a statement, Google spokesperson Jos√© Casta...\n",
      "8   ‚ÄúThis is a clear case of government overreach ...\n",
      "9                                                   S\n",
      "10     , and we are challenging it in court,‚Äù he said\n",
      "11  In the company‚Äôs lawsuit, Google argued that t...\n",
      "12  ‚ÄúAs a matter of common sense, a product that n...\n",
      "13  However, the CFPB said the discontinuation of ...\n",
      "14  Supervisory authority over Google‚Äôs payments d...\n",
      "15  In Google‚Äôs lawsuit, the company alleged the C...\n",
      "16  In 2022, the CFPB announced it would begin exa...\n",
      "17  ‚ÄúThis authority gives us critical agility to m...\n",
      "18  This story has been updated with additional de...\n",
      "19         CNN‚Äôs Clare Duffy contributed to reporting\n",
      "20  This report has been updated with additional c...\n"
     ]
    }
   ],
   "source": [
    "# Split the text into sentences by \".\"\n",
    "sentences_url = [sentence.strip() for sentence in cleaned_text.split('.') if sentence.strip()]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_clean_text = pd.DataFrame(sentences_url, columns=[\"message\"])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique url tokens (NLTK): 214\n",
      "Number of unique url tokens (spaCy): 214\n",
      "{'was', 'are', 'the', 'Friday', 'information', 'Chopra', 'small', '2022', 'tech', 'agency', 'As', 'companies', 'exists', 'incapable', 'order', 'Corp', 'market', 'unsubstantiated', 'and', 'matter', 'such', 'comply', 'us', 'that', 'details', 'payment', 'contributed', 'US', 'not', 'However', 'allowing', 'risk', 'they', 'posing', 'bar', 'nonbank', 'operations', 'discontinuation', 'of', 'requests', 'consumer', 'spokesperson', 'subject', '‚Äú', ',', 'company', 'sense', 'context', 'report', 'been', 'in', '‚Äô', 'updated', 'with', 'between', 'after', 'Google', 'form', 'offered', 'Clare', 'Duffy', 'CFPB', 'a', 'cited', 'protection', 'peer-to-peer', 'imposed', 'move', 'we', 'case', 'supervisory', 'on', 'Supervisory', 'enforce', 'sued', 'decision', 'examining', 'number', 'regulation', 'lawsuit', 'States', 'division', 'money', 'Casta√±eda', 'U', 'from', 'In', 'Washington', 'federal', 'investigate', 'setting', 'did', 'spreads', 'story', 'to', 'posed', 'raised', 'what', 'into', 'S', 'products', 'founded', 'for', 'CNN', 'oversee', 'Consumer', 'published', 'agility', 'is', 'Payment', 'laws', 'quickly', 'Apple', 'amid', 'low', 'risks', 'announced', 'fight', 'provided', 'burdensome', 'supervision', 'financial', 'complaints', 'confidential', 'exceedingly', 'by', 'This', 'Rohit', 'announcing', 'copy', 'court', 'Bureau', 'authority', 'payments', 'Jos√©', 'as', 'DC', 'release', 'handling', 'common', 'users', 'pose', 'Pay', 'Samsung', 'under', 'additional', 'push', '‚Äî', 'overreach', 'harm', 'stop', 'Financial', 'instances', 'comes', 'big', 'related', 'on-site', 'he', 'would', 'user', 'Director', 'argued', 'Protection', 'transferred', 'gives', 'involving', 'committed', 'counts', 'allow', 'longer', 'begin', 'it', 'has', 'place', 'The', 'ensuring', 'before', 'statement', 'including', 'no', 'may', 'reporting', 'an', 'be', 'district', 'critical', 'clear', 'said', 'sufficient', 'examinations', 'documents', 's', 'legal', 'alleged', 'content', '‚Äù', 'failed', 'its', 'government', 'over', 'never', 'where', 'customer', 'challenging', 'properly', 'error', 'institutions', 'United', 'product', 'which', 'consumers', 'based', 'filed', 'conduct'}\n",
      "{'was', 'are', 'the', 'Friday', 'information', 'Chopra', 'small', '2022', 'tech', 'agency', 'As', 'companies', 'exists', 'incapable', 'order', 'Corp', 'market', 'unsubstantiated', 'and', 'matter', 'such', 'comply', 'us', 'that', 'details', 'payment', 'contributed', 'US', 'not', 'However', 'allowing', 'risk', 'they', 'posing', 'bar', 'nonbank', 'operations', 'discontinuation', 'of', 'requests', 'consumer', 'spokesperson', 'subject', '‚Äú', ',', 'company', 'sense', 'context', 'report', 'been', 'in', 'updated', 'with', 'between', 'after', 'Google', 'form', '-', 'offered', 'Clare', 'Duffy', 'CFPB', 'a', 'cited', 'protection', 'move', 'imposed', 'we', 'case', 'supervisory', 'on', 'Supervisory', 'enforce', 'sued', 'decision', 'examining', 'number', 'regulation', 'lawsuit', 'States', 'division', 'money', 'Casta√±eda', 'U', 'from', 'In', 'Washington', 'federal', 'investigate', 'setting', 'did', 'spreads', 'story', 'to', 'posed', 'raised', 'what', 'into', 'S', 'products', 'founded', 'for', 'CNN', 'oversee', 'Consumer', 'published', 'agility', 'is', 'Payment', 'laws', 'quickly', 'Apple', 'amid', 'low', 'risks', 'announced', 'fight', 'provided', 'burdensome', 'supervision', 'financial', 'complaints', 'site', 'confidential', '‚Äôs', 'exceedingly', 'by', 'This', 'Rohit', 'announcing', 'copy', 'court', 'Bureau', 'authority', 'payments', 'Jos√©', 'as', 'DC', 'release', 'handling', 'common', 'users', 'pose', 'Pay', 'Samsung', 'under', 'additional', 'push', '‚Äî', 'overreach', 'peer', 'harm', 'stop', 'Financial', 'instances', 'comes', 'big', 'related', 'he', 'would', 'user', 'Director', 'argued', 'Protection', 'transferred', 'gives', 'involving', 'committed', 'counts', 'allow', 'longer', 'begin', 'it', 'has', 'place', 'The', 'ensuring', 'before', 'statement', 'including', 'no', 'may', 'reporting', 'an', 'be', 'district', 'critical', 'clear', 'said', 'sufficient', 'examinations', 'documents', 'legal', 'alleged', 'content', '‚Äù', 'failed', 'its', 'government', 'over', 'never', 'where', 'customer', 'challenging', 'properly', 'error', 'institutions', 'United', 'product', 'which', 'consumers', 'based', 'filed', 'conduct'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the entire dataframe\n",
    "nltk_tokens_set_url = text_functions.create_token_set_nltk(df_clean_text)\n",
    "spacy_tokens_set_url = text_functions.create_token_set_spacy(df_clean_text, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique url tokens (NLTK):\", len(nltk_tokens_set_url))\n",
    "print(\"Number of unique url tokens (spaCy):\", len(spacy_tokens_set_url))\n",
    "\n",
    "\n",
    "print(nltk_tokens_set_url)\n",
    "print(spacy_tokens_set_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "NLTK and spaCy produce nearly identical sets of tokens, with NLTK containing one additional token compared to spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (NLTK): 206\n",
      "Number of unique lemmas (spaCy): 192\n",
      "{'are', 'information', 'the', 'Friday', 'Chopra', 'small', '2022', 'examination', 'tech', 'agency', 'As', 'exists', 'incapable', 'wa', 'order', 'Corp', 'market', 'instance', 'institution', 'unsubstantiated', 'and', 'matter', 'such', 'comply', 'that', 'payment', 'contributed', 'US', 'not', 'However', 'allowing', 'risk', 'they', 'posing', 'bar', 'nonbank', 'discontinuation', 'of', 'consumer', 'spokesperson', 'subject', '‚Äú', ',', 'company', 'sense', 'document', 'context', 'been', 'updated', 'in', '‚Äô', 'with', 'between', 'after', 'Google', 'form', 'ha', 'offered', 'Clare', 'Duffy', 'CFPB', 'a', 'protection', 'peer-to-peer', 'imposed', 'move', 'we', 'case', 'supervisory', 'on', 'Supervisory', 'enforce', 'sued', 'decision', 'examining', 'number', 'regulation', 'lawsuit', 'States', 'division', 'money', 'Casta√±eda', 'U', 'from', 'In', 'Washington', 'federal', 'investigate', 'setting', 'did', 'story', 'count', 'to', 'posed', 'which', 'raised', 'what', 'into', 'S', 'for', 'founded', 'oversee', 'CNN', 'agility', 'Consumer', 'published', 'is', 'Payment', 'quickly', 'Apple', 'complaint', 'spread', 'amid', 'low', 'announced', 'fight', 'provided', 'burdensome', 'supervision', 'financial', 'confidential', 'law', 'exceedingly', 'by', 'This', 'Rohit', 'announcing', 'copy', 'court', 'Bureau', 'authority', 'Jos√©', 'DC', 'release', 'handling', 'common', 'pose', 'come', 'Pay', 'Samsung', 'under', 'additional', 'request', 'push', '‚Äî', 'overreach', 'harm', 'give', 'stop', 'Financial', 'big', 'related', 'on-site', 'he', 'would', 'user', 'Director', 'argued', 'Protection', 'transferred', 'involving', 'committed', 'allow', 'longer', 'begin', 'it', 'place', 'The', 'ensuring', 'u', 'operation', 'before', 'statement', 'including', 'no', 'may', 'reporting', 'an', 'be', 'district', 'critical', 'clear', 'said', 'sufficient', 'detail', 's', 'legal', 'alleged', 'content', '‚Äù', 'failed', 'government', 'over', 'never', 'where', 'customer', 'challenging', 'properly', 'error', 'United', 'product', 'report', 'cited', 'based', 'filed', 'conduct'}\n",
      "{'base', 'information', 'the', 'Friday', 'Chopra', 'small', '2022', 'examination', 'tech', 'agency', 'publish', 'unsubstantiate', 'incapable', 'order', 'Corp', 'market', 'instance', 'institution', 'file', 'and', 'matter', 'such', 'comply', 'that', 'payment', 'US', 'not', 'risk', 'relate', 'they', 'bar', 'nonbank', 'impose', 'discontinuation', 'of', 'update', 'consumer', 'spokesperson', 'subject', ',', 'company', 'sense', 'document', 'context', 'in', 'with', 'between', 'after', 'Google', 'form', '\"', '-', 'Clare', 'Duffy', 'CFPB', 'a', 'this', 'protection', 'move', 'do', 'long', 'we', 'case', 'supervisory', 'involve', 'on', 'Supervisory', 'enforce', 'decision', 'number', 'regulation', 'lawsuit', 'States', 'division', 'cite', 'money', 'Casta√±eda', 'U', 'from', 'Washington', 'federal', 'investigate', 'setting', 'story', 'count', 'to', 'contribute', 'which', 'what', 'into', 'S', 'for', 'oversee', 'agility', 'CNN', 'announce', 'Payment', 'quickly', 'Apple', 'complaint', 'spread', 'handle', 'amid', 'low', 'fight', 'burdensome', 'found', 'supervision', 'financial', 'site', 'confidential', 'law', '‚Äôs', 'argue', 'exceedingly', 'by', 'Rohit', 'commit', 'offer', 'copy', 'court', 'challenge', 'however', 'Bureau', 'authority', 'as', 'Jos√©', 'DC', 'release', 'common', 'pose', 'come', 'Pay', 'raise', 'have', 'Samsung', 'under', 'additional', 'request', 'push', '‚Äî', 'overreach', 'peer', 'harm', 'transfer', 'give', 'stop', 'big', 'he', 'would', 'user', 'Director', 'Protection', 'allow', 'begin', 'it', 'provide', 'fail', 'place', 'ensuring', 'before', 'operation', 'statement', 'exist', 'say', 'sue', 'no', 'may', 'an', 'be', 'district', 'critical', 'clear', 'sufficient', 'examine', 'detail', 'legal', 'content', 'its', 'government', 'over', 'allege', 'never', 'where', 'customer', 'properly', 'error', 'include', 'United', 'product', 'report', 'conduct'}\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmas_set_url, spacy_lemmas_set_url = text_functions.create_lemma_set(nltk_tokens_set_url, spacy_tokens_set_url, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique lemmas (NLTK):\", len(nltk_lemmas_set_url))\n",
    "print(\"Number of unique lemmas (spaCy):\", len(spacy_lemmas_set_url))\n",
    "\n",
    "print(nltk_lemmas_set_url)\n",
    "print(spacy_lemmas_set_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "\n",
    "The comparison between NLTK and spaCy on the lemmas of the provided text reveals several differences. NLTK identified 207 unique lemmas, while spaCy identified 192, indicating that NLTK retains more variations as separate lemmas. For example, NLTK includes lemmas like ‚Äúare‚Äù and ‚Äúbeen‚Äù that are missing in spaCy, whereas spaCy includes lemmas like ‚Äúbase‚Äù and ‚Äúhandle‚Äù not present in NLTK. SpaCy appears to apply broader normalization, reducing words like ‚Äúargued‚Äù to ‚Äúargue‚Äù and ‚Äúhandled‚Äù to ‚Äúhandle,‚Äù while NLTK keeps them distinct. Additionally, punctuation is handled differently; NLTK retains symbols like ‚Äú‚Äú‚Äù and ‚Äú,‚Äù separately, while spaCy consolidates some punctuation and includes symbols like ‚Äú‚Äî‚Äù as lemmas. Both tools handle proper nouns and acronyms, such as ‚ÄúCFPB‚Äù and ‚ÄúGoogle,‚Äù similarly, preserving them as is. Overall, NLTK preserves more granular distinctions, while spaCy emphasizes broader reductions for normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stemms (NLTK): 191\n",
      "{'ensur', 'are', 'base', 'the', 'clare', 'small', '2022', 'tech', 'divis', 'publish', 'provid', 'unsubstanti', 'overse', 'cfpb', 'incap', 'wa', 'order', 'market', 'rohit', 'file', 'argu', 'and', 'matter', 'such', 'us', 'that', 'announc', 'payment', 'howev', 'not', 'corp', 'risk', 'they', 'bar', 'addit', 'instanc', 'nonbank', 'dc', 'institut', 'of', 'jos√©', 'subject', 'spokesperson', '‚Äú', ',', 'document', 'involv', 'context', 'enforc', 'feder', 'been', 'in', '‚Äô', 'challeng', 'with', 'releas', 'between', 'after', 'form', 'supervis', 'ha', 'confidenti', 'suffici', 'a', 'on-sit', 'move', 'we', 'case', 'director', 'includ', 'on', 'relat', 'contribut', 'number', 'regul', 'lawsuit', 'consum', 'money', 'cite', 'inform', 'from', 'washington', 'agil', 'agenc', 'did', 'count', 'to', 'copi', 'critic', 'set', 'financi', 'investig', 'which', 'what', 'into', 'for', 'discontinu', 'peer-to-p', 'is', 'complaint', 'spread', 'samsung', 'amid', 'unit', 'low', 'fight', 'found', 'sens', 'alleg', 'properli', 'law', 'by', 'commit', 'googl', 'offer', 'court', 'befor', 'as', 'compani', 'impos', 'handl', 'common', 'pose', 'custom', 'come', 'under', 'transfer', 'request', 'push', '‚Äî', 'overreach', 'harm', 'give', 'stop', 'protect', 'big', 'author', 'he', 'would', 'user', 'supervisori', 'allow', 'longer', 'burdensom', 'begin', 'it', 'thi', 'fail', 'place', 'duffi', 'updat', 'exceedingli', 'u', 'statement', 'stori', 'exist', 'no', 'quickli', 'su', 'may', 'an', 'oper', 'be', 'district', 'examin', 'clear', 'said', 'appl', 'cnn', 'detail', 'decis', 's', 'legal', 'content', '‚Äù', 'compli', 'pay', 'casta√±eda', 'over', 'never', 'where', 'govern', 'error', 'rais', 'state', 'friday', 'bureau', 'product', 'report', 'chopra', 'conduct'}\n"
     ]
    }
   ],
   "source": [
    "nltk_stemmed_set_url = text_functions.nltk_set_stemming(nltk_tokens_set_url)\n",
    "print(\"Number of unique stemms (NLTK):\", len(nltk_stemmed_set_url))\n",
    "\n",
    "print(nltk_stemmed_set_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "Stemming reduces the size of the set compared to lemmatization by transforming words to their root forms, effectively merging words with similar roots. This process often removes grammatical nuances, resulting in terms like \"ensur\" that lack complete grammatical meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.8-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function definition \n",
    "def find_message_to_remove(df):\n",
    "    # Initialize stemmer and lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    # Function to preprocess and calculate unique tokens\n",
    "    def get_unique_tokens(messages):\n",
    "        stemmed_tokens = set()\n",
    "        lemmatized_tokens = set()\n",
    "        for msg in messages:\n",
    "            tokens = word_tokenize(msg.lower())\n",
    "            stemmed_tokens.update(stemmer.stem(token) for token in tokens)\n",
    "            lemmatized_tokens.update(lemmatizer.lemmatize(token) for token in tokens)\n",
    "        return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "    # Calculate original token sets\n",
    "    original_stemmed, original_lemmatized = get_unique_tokens(df[\"message\"])\n",
    "\n",
    "    #init flags and string holders for the message removed that make the condition true\n",
    "    msg_less_stem_equal_lemma = \"\"\n",
    "    msg_less_lemma_equal_stem = \"\"\n",
    "\n",
    "    found_msg_1_flag = False\n",
    "    found_msg_2_flag = False\n",
    "\n",
    "\n",
    "    # Find the message to remove\n",
    "    for idx, row in df.iterrows():\n",
    "        # Remove one message\n",
    "        remaining_messages = df.drop(index=idx)[\"message\"]\n",
    "        stemmed, lemmatized = get_unique_tokens(remaining_messages)\n",
    "        \n",
    "        # Check the conditions\n",
    "        if len(stemmed) < len(original_stemmed) and len(lemmatized) == len(original_lemmatized):\n",
    "            print(f\"Message  at index {idx}  causes a change in stemmed tokens but not in lemmatized tokens. \\nmessage: \", row[\"message\"])\n",
    "            #return row[\"message\"]\n",
    "            msg_less_stem_equal_lemma = row[\"message\"] \n",
    "            found_msg_1_flag = True\n",
    "        \n",
    "        if len(lemmatized) < len(original_lemmatized) and len(stemmed) == len(original_stemmed):\n",
    "             print(f\"Message at index {idx} causes a change in lemmatized tokens but not in stemmed tokens. \\nmessage:\", row[\"message\"])\n",
    "            # return row[\"message\"]  # Return the message that causes the change\n",
    "             msg_less_lemma_equal_stem = row[\"message\"] \n",
    "             found_msg_2_flag = True\n",
    "\n",
    "        if (found_msg_1_flag == True and found_msg_2_flag== True):\n",
    "            break # Break the loop if such messages found\n",
    "\n",
    "            \n",
    "\n",
    "    if  (found_msg_1_flag == False and found_msg_2_flag == False):      \n",
    "        print(\"No message found that meets the criteria.\")\n",
    "    \n",
    "  \n",
    "   \n",
    "    #return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that meets the criteria.\n"
     ]
    }
   ],
   "source": [
    "find_message_to_remove(df_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "\n",
    "The absence of a message in the dataset that meets the specified conditions could be due to the inherent differences between stemming and lemmatization processes. Stemming is a more aggressive method that removes affixes to produce a root form, often discarding grammatical context. Lemmatization, on the other hand, generates linguistically valid base forms by considering the word's context and part of speech. This divergence makes it unlikely for a message to exist where removing it alters only one token count while leaving the other unchanged. Additionally, the lack of such messages may reflect the dataset's specific characteristics, where no individual message contains terms that exhibit these unique processing behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.10\n",
    "\n",
    "The comparison between NLTK tokens, lemmas, and stems highlights differences in linguistic processing and simplification. The tokenization process resulted in 214 unique tokens, which include raw words split by spaces and punctuation, preserving grammatical meaning but often containing redundancies such as variations in forms (e.g., \"was\" and \"wa\"). Lemmatization reduced the count to 206 unique lemmas by normalizing words to their base dictionary forms while retaining grammatical context (e.g., \"are\" remains unchanged). Stemming further simplified the text, reducing the count to 191 unique stems by stripping words to their root forms without considering grammatical correctness, leading to more aggressive reductions and potential loss of meaning (e.g., \"ensuring\" becomes \"ensur\"). This progression illustrates how lemmatization provides a balance between grammatical integrity and redundancy, while stemming focuses on simplification at the expense of linguistic nuance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.11\n",
    "\n",
    "The comparison of spaCy's tokenization and lemmatization reveals notable differences in the level of text simplification achieved. SpaCy's tokenization resulted in 214 unique tokens, which include raw words segmented from the text, preserving punctuation and grammatical structure but often containing duplicates or inflected forms (e.g., \"are\" and \"is\"). Lemmatization reduced this count to 192 unique lemmas by normalizing words to their base forms, eliminating redundancies while maintaining grammatical meaning (e.g., \"was\" and \"were\" become \"be\"). Compared to the tokenization step, lemmatization in spaCy offers a streamlined representation of the text by focusing on semantic equivalence, making it particularly useful for tasks requiring a deeper understanding of word meanings and relationships. The smaller number of lemmas compared to tokens reflects a more compact and meaningful vocabulary derived from the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 - 17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.2.2024, 19:26:47] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ‚Äè◊î◊î◊ï◊ì◊¢◊ï◊™ ◊ï◊î◊©◊ô◊ó◊ï◊™ ◊û◊ï◊¶◊§◊†◊ï◊™ ◊û◊ß◊¶◊î ◊ú◊ß◊¶◊î. ◊ú◊ê◊£ ◊ê◊ó◊ì ◊û◊ó◊ï◊• ◊ú◊¶'◊ê◊ò ◊î◊ñ◊î, ◊í◊ù ◊ú◊ê ◊ú-WhatsApp, ◊ê◊ô◊ü ◊ê◊§◊©◊®◊ï◊™ ◊ú◊ß◊®◊ï◊ê ◊ê◊ï◊™◊ü ◊ï◊ú◊î◊ê◊ñ◊ô◊ü ◊ú◊î◊ü.\n",
      "‚Äè[13.2.2024, 19:26:47] Omri: CamScanner 2024-02-13 18.57.pdf ‚Ä¢ ‚Äè11 ◊ì◊§◊ô◊ù ‚Äè◊û◊°◊û◊ö ◊î◊ï◊©◊û◊ò\n",
      "[13.2.2024, 19:26:47] Omri: ◊§◊™◊®◊ï◊ü ◊©◊ú◊ô ◊ú◊î◊°◊™◊ë◊®◊ï◊™\n",
      "[13.2.2024, 19:26:47] Omri: ◊ê◊ù ◊û◊¶◊ê◊™ ◊ò◊¢◊ï◊™ ◊¢◊ú ◊î◊ì◊®◊ö ◊™◊¢◊ì◊õ◊ü\n",
      "[13.2.2024, 22:29:55] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊ô◊ñ◊î ◊û◊ú◊ê◊ö ◊ê◊™◊î‚ù§Ô∏è‚ù§Ô∏è\n",
      "[13.2.2024, 22:30:33] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ò◊†◊ß◊ô◊ï! ◊û◊ó◊® ◊ú◊ê ◊ê◊ï◊õ◊ú ◊ú◊ë◊ï◊ê ◊ú◊¶◊¢◊®◊ô ◊ê◊ñ ◊ú◊ê ◊ê◊ï◊õ◊ú ◊ú◊ê◊®◊ó ◊ú◊ö ◊ó◊ë◊®◊î\n",
      "[13.2.2024, 22:33:38] Omri: ◊ú◊ê ◊™◊î◊ô◊î ◊ê◊ï◊ï◊ô◊®◊î ◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™\n",
      "[13.2.2024, 22:40:58] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ó◊ó◊ó◊ó\n",
      "[21.2.2024, 17:28:21] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊ó ◊©◊ú◊ô, ◊™◊°◊ú◊ó ◊ú◊ô ◊ê◊ë◊ú ◊í◊ù ◊î◊§◊¢◊ù ◊ú◊ê ◊ê◊ë◊ï◊ê ◊ú◊ê◊®◊ó ◊ú◊ö ◊ó◊ë◊®◊î, ◊ê◊†◊ô ◊ê◊ô◊™◊ö ◊û◊®◊ó◊ï◊ß, ◊®◊ê◊ô◊™◊ô ◊©◊í◊ù ◊ë◊®◊ß ◊ú◊ê ◊ë◊ê ◊ú◊ê◊®◊ó ◊ú◊ö ◊ó◊ë◊®◊î ◊ó◊ó◊ó\n",
      "[21.2.2024, 18:08:57] Omri: ◊ë◊ì◊ì ◊ê◊ú◊ö ◊í◊ù ◊™◊§◊ô◊ú◊î ◊ê◊ô◊ü ◊ú◊ô\n",
      "[10.5.2024, 17:41:22] Omri: ◊ê◊ô◊§◊î ◊ê◊™◊î ◊™◊í◊ô◊ì ◊ú◊ô\n",
      "[11.5.2024, 21:54:00] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ô◊ê ◊û◊ú◊ê◊ö, ◊©◊ë◊ï◊¢ ◊ò◊ï◊ë ◊©◊ô◊î◊ô◊î ◊ú◊ö. ◊ê◊†◊ô ◊î◊ô◊ô◊™◊ô ◊ë◊ñ◊ï◊ù ◊î◊©◊ë◊ï◊¢◊ô◊ô◊ù ◊î◊ê◊ú◊î, ◊í◊ù ◊í◊†◊ë◊ï ◊ú◊ô ◊ê◊™ ◊î◊ß◊ï◊®◊ß◊ô◊†◊ò ◊ê◊ñ ◊ú◊ê ◊î◊ô◊î ◊ú◊ô ◊û◊û◊© ◊ê◊ô◊ö ◊ú◊î◊í◊ô◊¢, ◊ê◊ë◊ú ◊ê◊í◊ô◊¢ ◊ë◊™◊ß◊ï◊ï◊î ◊ë◊©◊ë◊ï◊¢ ◊î◊ë◊ê ◊ë◊©◊ë◊ô◊ú ◊ë◊ê◊ï◊ï◊ô◊®◊î ◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™üòâ\n",
      "[11.5.2024, 21:54:05] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û ◊î ◊©◊ú◊ï◊û◊ö?\n",
      "[11.5.2024, 21:55:36] Omri: ◊î◊ô◊ô◊ì◊î! ◊ß◊¶◊™ ◊õ◊§◊®◊™ ◊¢◊ï◊ï◊†◊ï◊™ ◊ô◊ê◊ú◊î ◊î◊ú◊ö ◊î◊ß◊ï◊®◊ß◊ô◊†◊òüôà ◊ß◊ì◊ô◊û◊î ◊û◊û◊™◊ô◊†◊ô◊ù ◊ú◊ö‚Ä¶ ◊ê◊ù ◊™◊®◊¶◊î ◊ô◊© 3 ◊õ◊ô◊ï◊ï◊†◊ô◊ù ◊ú◊ó◊†◊ô◊î ◊ó◊ô◊†◊ù ◊¢◊ù ◊î◊®◊õ◊ë\n",
      "[11.5.2024, 21:55:56] Omri: ◊ê◊¶◊ú◊ô ◊ë◊°◊ì◊® ◊û◊í◊ô◊¢ ◊ë◊õ◊ï◊†◊†◊ï◊™ ◊°◊§◊ô◊í◊î ◊ú◊°◊ô◊û◊°◊ò◊®\n",
      "[11.5.2024, 22:11:20] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊¢◊ì◊ô◊£ ◊ê◊™ ◊î◊ß◊ï◊®◊ß◊ô◊†◊ò ◊¢◊ú ◊§◊†◊ô ◊õ◊§◊®◊™ ◊¢◊ï◊ï◊†◊ï◊™, ◊ê◊ï◊ú◊ô ◊ñ◊î ◊ë◊í◊ú◊ú ◊©◊ñ◊î ◊©◊†◊™ ◊©◊û◊ô◊ò◊îüòõ\n",
      "[11.5.2024, 22:11:27] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊î◊ó◊†◊ô◊î ◊©◊©◊ú◊ó◊ï ◊ë◊ß◊ë◊ï◊¶◊î ◊ê◊™◊î ◊û◊™◊õ◊ï◊ï◊ü?\n",
      "[11.5.2024, 22:12:03] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ñ◊î ◊†◊ì◊û◊î ◊ú◊ô ◊ê◊ï ◊©◊ë◊ê◊û◊™  ◊ê◊ô◊ü ◊û◊ë◊ó◊ü ◊ë◊ú◊û◊ô◊ì◊î ◊¢◊û◊ï◊ß◊î+\n",
      "[11.5.2024, 22:29:50] Omri: ◊ú◊ê\n",
      "[11.5.2024, 22:29:57] Omri: ◊ô◊© ◊¢◊ï◊ì 3 ◊ê◊ó◊®◊ô◊ù\n",
      "[11.5.2024, 22:30:06] Omri: ◊ê◊ô◊ü ◊û◊ë◊ó◊ü ◊ê◊ë◊ú ◊ô◊© ◊§◊®◊ï◊ô◊ô◊ß◊ò\n",
      "[12.5.2024, 9:03:29] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊©◊û◊¢ ◊ê◊™◊î ◊ê◊©◊£ ◊î◊ó◊†◊ô◊ï◊™\n",
      "[12.5.2024, 9:15:28] Omri: ◊ô◊© ◊ß◊ë◊ï◊¶◊î ◊°◊í◊ï◊®◊î ◊©◊î◊ß◊û◊†◊ï ◊ê◊ù ◊™◊í◊ô◊¢ ◊ô◊ï◊™◊® ◊ú◊õ◊ô◊™◊î ◊™◊ñ◊õ◊î ◊ú◊î◊¶◊ò◊®◊£\n",
      "‚Äè[12.5.2024, 9:15:57] Omri: ‚Äè◊î◊™◊û◊ï◊†◊î ◊î◊ï◊©◊û◊ò◊î\n",
      "[12.5.2024, 9:16:11] Omri: ◊ñ◊ï ◊ó◊†◊ô◊î ◊ó◊ï◊®◊†◊ô◊™ ◊ê◊ë◊ú ◊ó◊ô◊†◊û◊ô◊™ ◊ï◊ì◊ô◊ô ◊ß◊®◊ï◊ë◊î\n",
      "‚Äè[12.5.2024, 9:17:34] Omri: ‚Äè◊î◊™◊û◊ï◊†◊î ◊î◊ï◊©◊û◊ò◊î\n",
      "[12.5.2024, 9:17:50] Omri: ◊ú◊§◊¢◊û◊ô◊ù ◊õ◊ê◊ü ◊ô◊© ◊ó◊†◊ô◊î ◊ë◊ê◊§◊ï◊® ◊™◊ú◊ï◊ô ◊ë◊û◊ñ◊ú\n",
      "[12.5.2024, 17:53:05] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊ô◊ñ◊î ◊®◊¶◊ô◊†◊ô ◊ê◊™◊î ◊™◊©◊û◊¢\n",
      "[12.5.2024, 17:53:23] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊ô◊ö ◊î◊û◊ß◊ï◊ù ◊î◊ó◊ì◊©? ◊ô◊ï◊™◊® ◊©◊ï◊ï◊î ◊û◊ê◊§◊ß◊î? ◊û◊®◊í◊ô◊© ◊î◊ô◊ô◊ò◊ß◊°?\n",
      "[13.5.2024, 18:29:54] Omri: ◊û◊û◊© ◊î◊ô◊ô◊ò◊ß◊°\n",
      "[13.5.2024, 18:30:08] Omri: ◊®◊ß ◊ê◊ô◊ü ◊î◊®◊ë◊î ◊ê◊†◊©◊ô◊ù, ◊ú◊ê ◊ô◊ï◊ì◊¢ ◊ê◊ù ◊ñ◊î ◊ò◊ï◊ë ◊ê◊ï ◊ú◊ê ◊ê◊ë◊ú ◊ô◊© ◊ê◊ï◊ï◊ô◊®◊î\n",
      "[15.5.2024, 16:51:23] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊î ◊ñ◊î ◊ê◊™◊î ◊ë◊ñ◊ï◊ù, ◊ê◊†◊ô ◊ë◊©◊ï◊ß\n",
      "[15.5.2024, 16:51:33] Omri: ◊û◊î ◊ê◊™◊î ◊ë◊õ◊ô◊™◊î\n",
      "[15.5.2024, 16:51:36] Omri: ◊ú◊ê ◊û◊ê◊û◊ô◊ü\n",
      "[15.5.2024, 16:52:06] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ú◊ê ◊ê◊ë◊ú ◊ó◊©◊ë◊™◊ô ◊ú◊ë◊ï◊ê ◊î◊ô◊ï◊ù ◊ú◊®◊ê◊ï◊™ ◊ê◊™ ◊ñ◊ô◊ï ◊§◊†◊ô◊ö\n",
      "[15.5.2024, 16:52:21] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊ë◊ú ◊ê◊™◊î ◊ú◊ê ◊©◊ù ◊ê◊ô◊ü ◊©◊ï◊ù ◊ò◊¢◊ù\n",
      "[15.5.2024, 16:52:28] Omri: ◊ê◊ó ◊ô◊ß◊®\n",
      "[15.5.2024, 16:52:33] Omri: ◊î◊ô◊î ◊ô◊ï◊ù ◊û◊ß◊ï◊ú◊ß◊ú ◊ë◊¢◊ë◊ï◊ì◊î\n",
      "[15.5.2024, 16:52:55] Omri: ◊®◊¶◊ô◊™◊ô ◊ú◊ë◊ï◊ê ◊ê◊ë◊ú ◊ú◊ê ◊î◊°◊§◊ß◊™◊ô ◊ú◊î◊í◊ô◊¢ ◊§◊ô◊ñ◊ô◊™\n",
      "[15.5.2024, 16:53:39] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ë◊°◊ï◊ï◊ì◊® ◊í◊û◊ï◊®, ◊û◊ß◊ï◊ï◊î ◊©◊î◊ß◊ú◊ß◊ï◊ú ◊ô◊¢◊ë◊ï◊® ◊ë◊û◊î◊®◊î ◊ë◊ô◊û◊ô◊†◊ï\n",
      "[15.5.2024, 16:53:43] ◊†◊ô◊ë ◊ê◊§◊ß◊î: üôèüèª\n",
      "‚Äè[15.5.2024, 16:54:46] Omri: ‚Äè◊°◊ò◊ô◊ß◊® ◊î◊ï◊©◊û◊ò\n",
      "[1.9.2024, 19:31:28] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊†◊ô ◊¢◊ï◊ì ◊©◊†◊ô◊î ◊û◊¶◊ô◊í ◊û◊ï◊™◊ß\n",
      "[1.9.2024, 19:31:47] Omri: ◊®◊¶◊ô◊™◊ô ◊ú◊ï◊ï◊ì◊ê ◊©◊ê◊™◊ù ◊ë◊õ◊ô◊™◊î\n",
      "[1.9.2024, 19:31:50] Omri: ◊©◊ú◊ê ◊©◊õ◊ó◊™◊ù\n",
      "[1.9.2024, 21:15:05] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ò◊†◊ß◊ô◊ï‚ù§Ô∏è\n",
      "[1.9.2024, 21:15:09] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊î ◊©◊ú◊ï◊û◊ö?\n",
      "‚Äè[1.9.2024, 21:15:26] Omri: ‚Äè◊î◊™◊û◊ï◊†◊î ◊î◊ï◊©◊û◊ò◊î\n",
      "[1.9.2024, 21:15:29] Omri: ◊û◊ó◊ô◊ú ◊ê◊ú ◊ó◊ô◊ú\n",
      "[1.9.2024, 21:20:42] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊™◊ê◊û◊ô◊ü ◊ú◊ô ◊ê◊™◊î ◊¶◊ì◊ô◊ß ◊©◊ô◊§◊®◊ó\n",
      "[1.9.2024, 21:20:47] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊õ◊ë◊® ◊§◊ï◊®◊ó\n",
      "‚Äè[23.11.2024, 23:25:41] Omri: finalProject.pdf ‚Ä¢ ‚Äè7 ◊ì◊§◊ô◊ù ‚Äè◊û◊°◊û◊ö ◊î◊ï◊©◊û◊ò\n",
      "[23.11.2024, 23:25:51] Omri: ◊ê◊î◊ú◊ü ◊ß◊ô◊†◊í ◊û◊ß◊ï◊ï◊î ◊©◊î◊ô◊î ◊°◊ï◊§◊© ◊û◊ï◊¶◊ú◊ó\n",
      "[23.11.2024, 23:26:36] Omri: ◊¶◊®◊ô◊õ◊ô◊ù ◊ú◊ë◊ó◊ï◊® ◊§◊®◊ï◊ô◊ô◊ß◊ò ◊û◊™◊ï◊ö ◊î5 ◊†◊ï◊©◊ê◊ô◊ù ◊©◊î◊ô◊ê ◊î◊¢◊ú◊™◊î\n",
      "[23.11.2024, 23:27:02] Omri: ◊™◊í◊ô◊ì ◊û◊î ◊¢◊†◊ô◊ô◊ü ◊ê◊ï◊™◊ö.. ◊ê◊ï ◊î◊õ◊ô ◊§◊ó◊ï◊™ ◊õ◊ê◊ë ◊®◊ê◊© ◊ó◊ó\n",
      "[23.11.2024, 23:27:41] Omri: ◊ë◊ü ◊ï◊®◊ü ◊õ◊ë◊® ◊õ◊û◊¢◊ò ◊°◊ô◊ô◊û◊ï ◊ê◊™ ◊î◊§◊®◊ï◊ô◊ô◊ß◊ò ◊ï◊î◊ï◊®◊ô◊ì◊ï ◊ú◊¢◊¶◊û◊ù ◊¢◊ï◊û◊° ◊ê◊ï◊ú◊ô ◊©◊ï◊ï◊î ◊ú◊†◊°◊ï◊™ ◊ú◊î◊™◊ó◊ô◊ú\n",
      "[23.11.2024, 23:31:28] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ô◊ê◊ú◊ú◊ï◊™ ◊†◊©◊û◊¢ ◊ò◊ï◊ë ◊ê◊°◊™◊õ◊ú ◊¢◊ú ◊ñ◊î ◊¢◊õ◊©◊ô◊ï, ◊ú◊ö ◊ô◊© ◊û◊©◊î◊ï ◊©◊™◊§◊° ◊ê◊™ ◊î◊¢◊ô◊ü?\n",
      "[23.11.2024, 23:31:55] Omri: ◊õ◊ü ◊î◊†◊ï◊©◊ê ◊ú ◊î◊ó◊ú◊ï◊û◊ï◊™ ◊†◊©◊û◊¢ ◊ô◊ó◊°◊ô◊™ ◊°◊ë◊ë◊î\n",
      "[23.11.2024, 23:32:13] Omri: ◊ê◊ë◊ú ◊™◊í◊ô◊ì ◊ê◊ù ◊®◊ê◊ô◊™ ◊û◊©◊î◊ï ◊¢◊ì◊ô◊£\n",
      "[23.11.2024, 23:34:46] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊í◊†◊ô◊ë\n",
      "[23.11.2024, 23:35:03] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊î◊ê◊û◊™ ◊í◊ù ◊§◊®◊ï◊ô◊ß◊ò 2 ◊†◊©◊û◊¢ ◊û◊í◊†◊ô◊ë, ◊î◊û◊°◊õ◊ù ◊û◊ê◊û◊®◊ô◊ù ◊î◊ñ◊î\n",
      "[23.11.2024, 23:35:17] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊†◊°◊î ◊ú◊î◊ë◊ô◊ü ◊û◊î ◊ô◊î◊ô◊î ◊§◊ó◊ï◊™ ◊õ◊ê◊ë ◊ú◊ë◊¶◊¢\n",
      "[23.11.2024, 23:35:25] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ñ◊î ◊í◊ù ◊ñ◊ï◊®◊ù ◊ú◊ö?\n",
      "[23.11.2024, 23:35:37] Omri: ◊ñ◊î ◊î◊ß◊ï ◊î◊û◊†◊ó◊î\n",
      "[23.11.2024, 23:35:48] Omri: ◊ê◊ë◊ì◊ï◊ß ◊¢◊ù ◊ó◊ë◊® ◊©◊û◊ë◊ô◊ü\n",
      "[23.11.2024, 23:35:54] Omri: ◊ê◊©◊ê◊ú ◊ê◊ï◊™◊ï ◊¢◊ú 2 ◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ô◊ù\n",
      "[23.11.2024, 23:36:14] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊ó◊ú◊ï◊™\n",
      "[23.11.2024, 23:36:28] Omri: ◊ô◊ê◊ú◊î ◊ê◊ñ 2 ◊ê◊ï 4 ◊ê◊ó◊ñ◊ô◊® ◊™◊©◊ï◊ë◊î\n",
      "[23.11.2024, 23:36:53] ◊†◊ô◊ë ◊ê◊§◊ß◊î: 2 ◊ê◊ï 3 ◊ú◊ê?\n",
      "[23.11.2024, 23:37:18] Omri: ◊õ◊ü\n",
      "[23.11.2024, 23:37:22] Omri: ◊î◊™◊ë◊ú◊ë◊ú◊™◊ô\n",
      "[23.11.2024, 23:41:06] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊†◊©◊ê◊®◊ï ◊ú◊ö ◊®◊ß ◊¢◊ï◊ì 2 ◊ë◊ú◊ë◊ï◊ú◊ô◊ù\n",
      "[23.11.2024, 23:41:36] Omri: üòÜ\n",
      "[23.11.2024, 23:41:54] Omri: ◊ò◊ï◊ë ◊©◊ú◊ó◊™◊ô ◊ú2 ◊ó◊ë◊®◊ô◊ù ◊©◊ô◊ï◊ì◊¢◊ô◊ù LLM ◊ë◊ò◊ó ◊ô◊ó◊ñ◊ô◊®◊ï ◊™◊©◊ï◊ë◊î ◊û◊ó◊®\n",
      "[23.11.2024, 23:42:11] Omri: ◊†◊®◊ê◊î ◊ú◊ô ◊¢◊ù ◊î gpt ◊ô◊î◊ô◊î ◊ú◊†◊ï ◊°◊ë◊ë◊î\n",
      "[23.11.2024, 23:43:13] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊†◊ô ◊ë LM\n",
      "[23.11.2024, 23:43:48] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ú◊í◊û◊®◊ô, ◊ë◊™◊õ◊ú◊° ◊ê◊†◊ó◊†◊ï ◊©◊ú◊ï◊©◊î ◊ë◊ß◊ë◊ï◊¶◊î ◊ë◊ô◊ó◊ì ◊ê◊ô◊™◊ï\n",
      "[23.11.2024, 23:46:58] Omri: ◊ê◊™◊î ◊©◊†◊ï◊ü ◊û◊ì◊ô◊ô\n",
      "[23.11.2024, 23:47:55] Omri: ◊ë◊ô◊ó◊ì ◊ê◊ô◊™◊ï ◊ê◊†◊ó◊†◊ï ◊ê◊ó◊ì\n",
      "Inf+2=inf\n",
      "[24.11.2024, 0:00:31] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ó◊ó◊ó ◊ú◊í◊û◊®◊ô\n",
      "[24.11.2024, 8:42:38] ◊†◊ô◊ë ◊ê◊§◊ß◊î: https://youtu.be/eC6Hd1hFvos?si=1zU02x3mj7n-f2ea\n",
      "[24.11.2024, 8:52:39] ◊†◊ô◊ë ◊ê◊§◊ß◊î: https://medium.com/bitgrit-data-science-publication/fine-tuning-llms-to-write-data-science-code-3a76c23ff6b7\n",
      "[24.11.2024, 8:53:17] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊°◊ë◊ô◊® ◊ê◊ô◊ö ◊ú◊ß◊ó◊™ ◊û◊ï◊ì◊ú ◊û◊ï◊õ◊ü ◊©◊ú keras nlp ◊ï◊ú◊¢◊©◊ï◊™ ◊ú◊ï fine tuning\n",
      "‚Äè[24.11.2024, 9:12:57] Omri: ‚Äè◊î◊™◊û◊ï◊†◊î ◊î◊ï◊©◊û◊ò◊î\n",
      "[24.11.2024, 9:17:11] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊í◊†◊ô◊ë◊ô◊ô◊©◊ü\n",
      "[25.11.2024, 8:53:57] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊î ◊ß◊ï◊®◊î ◊û◊™◊ï◊ß◊î? ◊ê◊†◊ô ◊î◊™◊ó◊ú◊™◊ô ◊ê◊™ ◊ó◊ú◊ß 1 ◊©◊ú ◊§◊®◊ï◊ô◊ß◊ò 2 ◊õ◊õ◊î ◊ú◊ß◊®◊ï◊ê ◊û pdf ◊ï◊ì◊ë◊®◊ô◊ù ◊õ◊ê◊ú◊î, ◊ê◊ô◊ö ◊ê◊™◊î ◊®◊ï◊¶◊î ◊©◊†◊†◊î◊ú ◊ê◊™ ◊ñ◊î? ◊†◊§◊™◊ó ◊ê◊ô◊ñ◊î ◊í◊ô◊ò ◊†◊ó◊û◊ì◊™?\n",
      "[25.11.2024, 9:12:52] Omri: ◊í◊ù ◊ê◊†◊ô ◊î◊™◊ó◊ú◊™◊ô ◊ê◊™◊û◊ï◊ú\n",
      "[25.11.2024, 9:12:55] Omri: ◊õ◊û◊¢◊ò ◊°◊ô◊ô◊û◊™◊ô ◊ê◊™ 1\n",
      "[25.11.2024, 9:13:09] Omri: ◊í◊ô◊ò ◊†◊ê◊î\n",
      "[25.11.2024, 9:13:18] Omri: ◊û◊™◊ô ◊ê◊™◊î ◊ñ◊û◊ô◊ü ◊î◊ô◊ï◊ù ◊ú◊ò◊ú◊§◊ï◊ü\n",
      "[25.11.2024, 9:15:02] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊©◊õ◊®◊î\n",
      "[25.11.2024, 9:15:09] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ê◊™◊î ◊ñ◊®◊ô◊ñ ◊õ◊ê◊®◊†◊ë\n",
      "[25.11.2024, 9:15:21] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ë2 ◊õ◊ñ◊î?\n",
      "[25.11.2024, 9:16:13] Omri: ◊õ ◊¶◊ë◊ô\n",
      "[25.11.2024, 9:16:24] Omri: ◊ô◊ê◊ú◊î ◊û◊¢◊ï◊ú◊î\n",
      "[25.11.2024, 9:17:11] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ô◊ê◊ú◊ú◊ï◊™\n",
      "[26.11.2024, 15:42:09] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊î ◊ß◊ï◊®◊î ◊ê◊ó◊ô? ◊ê◊†◊ô ◊ú◊ê ◊û◊®◊í◊ô◊© ◊õ◊ú ◊õ◊ö ◊ò◊ï◊ë ◊ê◊ñ ◊ú◊ê ◊ê◊ë◊ï◊ê ◊î◊ô◊ï◊ù, ◊ê◊†◊ô ◊®◊ï◊¶◊î ◊ú◊î◊™◊ß◊ì◊ù ◊ß◊¶◊™ ◊ë◊§◊®◊ï◊ô◊ß◊ò, ◊ô◊õ◊ï◊ú ◊ê◊ï◊ú◊ô ◊ú◊©◊ú◊ï◊ó ◊ú◊ô ◊û◊î ◊©◊¢◊©◊ô◊™ ◊ë◊ô◊†◊™◊ô◊ô◊ù?\n",
      "‚Äè[26.11.2024, 16:16:49] Omri: ‚Äè◊°◊ò◊ô◊ß◊® ◊î◊ï◊©◊û◊ò\n",
      "[26.11.2024, 16:16:58] Omri: ◊õ◊ü ◊™◊õ◊£ ◊ê◊©◊ú◊ó\n",
      "‚Äè[26.11.2024, 16:25:26] Omri: Attention Is All You Need.pdf ‚Ä¢ ‚Äè15 ◊ì◊§◊ô◊ù ‚Äè◊û◊°◊û◊ö ◊î◊ï◊©◊û◊ò\n",
      "‚Äè[26.11.2024, 16:25:27] Omri: main.py ‚Äè◊û◊°◊û◊ö ◊î◊ï◊©◊û◊ò\n",
      "‚Äè[26.11.2024, 16:25:27] Omri: parsing.py ‚Äè◊û◊°◊û◊ö ◊î◊ï◊©◊û◊ò\n",
      "‚Äè[26.11.2024, 16:25:27] Omri: storage.py ‚Äè◊û◊°◊û◊ö ◊î◊ï◊©◊û◊ò\n",
      "[26.11.2024, 17:30:44] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ò◊†◊ß◊ô◊ïüôÉ\n",
      "[26.11.2024, 22:11:49] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊™◊®◊ê◊î ◊ê◊ù ◊ê◊™◊î ◊û◊¶◊ú◊ô◊ó ◊ú◊î◊™◊ó◊ë◊® ◊ú◊ñ◊î:\n",
      "https://github.com/nivco12/finalProjectNLP\n",
      "[27.11.2024, 0:32:31] Omri: ◊î◊®◊í◊©◊†◊ï ◊ë◊ó◊°◊®◊ï◊†◊ö ◊î◊ô◊ï◊ù\n",
      "[27.11.2024, 0:33:04] Omri: ◊ê◊ë◊ì◊ï◊ß ◊ê◊™ ◊ñ◊î ◊û◊ó◊® ◊ë◊¥◊† ◊™◊ï◊ì◊î ◊ô◊ê◊ó\n",
      "[27.11.2024, 9:22:28] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ‚ò∫Ô∏è\n",
      "[27.11.2024, 9:22:34] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊™◊¢◊ì◊õ◊ü\n",
      "[27.11.2024, 11:00:37] Omri: ◊î◊†◊§◊ß◊™ ◊í◊ô◊ò ◊û◊®◊í◊©\n",
      "[28.11.2024, 9:38:29] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊î ◊ß◊ï◊®◊î ◊ê◊ó◊ô? ◊ê◊†◊ô ◊û◊î◊ô◊ï◊ù ◊¢◊ì ◊©◊ë◊™ ◊ë◊¢◊®◊ë ◊ë◊ô◊ù ◊î◊û◊ú◊ó ◊¢◊ù ◊î◊û◊©◊§◊ï◊ó◊ô, ◊ë◊©◊ë◊ï◊¢ ◊î◊ë◊ê ◊ê◊†◊ô ◊ë◊ó◊ï◊§◊© ◊ê◊ñ ◊ô◊î◊ô◊î ◊ú◊ô ◊ñ◊û◊ü ◊ú◊©◊ë◊™ ◊ú◊ß◊ì◊ù ◊ê◊™ ◊î◊§◊®◊ï◊ô◊ß◊ò\n",
      "[28.11.2024, 11:03:45] Omri: ◊™◊î◊†◊î ◊ô◊ê◊ó ◊ê◊†◊ô ◊î◊ô◊ï◊ù ◊ï◊û◊ó◊® ◊ê◊†◊°◊î ◊ú◊©◊ë◊™ ◊ú◊ß◊ì◊ù ◊ß◊¶◊™\n",
      "[28.11.2024, 15:56:11] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ë◊°◊ì◊® ◊í◊û◊ï◊®, ◊ê◊ù ◊ê◊™◊î ◊û◊°◊ô◊ô◊ù ◊û◊©◊î◊ï ◊™◊¢◊©◊î ◊ß◊ï◊û◊ô◊ò ◊§◊ï◊© ◊ú◊í◊ô◊ò\n",
      "[28.11.2024, 15:56:34] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ï◊ê◊û◊©◊ô◊ö ◊û◊©◊ù\n",
      "[28.11.2024, 15:56:42] Omri: ◊î◊ú◊ï ◊î◊ú◊ï ◊ë◊ú◊ô ◊ú◊ì◊ó◊ï◊£\n",
      "[28.11.2024, 15:56:56] Omri: ◊î◊ê◊û◊™ ◊¶◊®◊ô◊ö ◊ú◊¢◊©◊ï◊™ ◊®◊ô◊¢◊†◊ï◊ü ◊©◊ô◊û◊ï◊© ◊ë◊í◊ô◊ò\n",
      "[28.11.2024, 16:00:51] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ñ◊î ◊î◊û◊¢◊®◊ë ◊î◊§◊®◊ï◊¢ ◊§◊î\n",
      "[28.11.2024, 16:10:41] Omri: ◊ß◊ú◊ê◊ë ◊î◊ï◊ò◊ú ◊ê◊ô◊ú◊™\n",
      "[1.12.2024, 12:21:07] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊û◊î ◊ß◊ï◊®◊î ◊û◊ï◊™◊ß? ◊ó◊ñ◊®◊™◊ô ◊û◊ó◊ï◊§◊©◊™◊ô ◊ï◊ê◊™◊ó◊ô◊ú ◊ú◊©◊ë◊™ ◊¢◊õ◊©◊ô◊ï ◊†◊ï◊®◊û◊ú◊ô ◊¢◊ú ◊î◊§◊®◊ï◊ô◊ß◊ò ◊©◊ú◊†◊ï, ◊©◊ô◊†◊ô◊™ ◊û◊©◊î◊ï ◊ë◊°◊ï◊§◊© ◊ê◊ï ◊©◊ê◊†◊ô ◊û◊û◊©◊ô◊ö ◊û◊ê◊ï◊™◊ï ◊û◊ß◊ï◊ù?\n",
      "[1.12.2024, 12:21:29] Omri: ◊î◊ô◊ï ◊ú◊ô ◊¢◊ï◊û◊° ◊§◊†◊ô◊ï◊™ ◊î◊©◊ë◊ï◊¢\n",
      "[1.12.2024, 12:21:37] Omri: ◊ê◊†◊ô ◊í◊ù ◊®◊ß ◊¢◊õ◊©◊ô◊ï ◊ó◊ï◊ñ◊® ◊ú◊¢◊†◊ô◊ô◊†◊ô◊ù\n",
      "[1.12.2024, 12:21:48] Omri: 3 ◊ì◊ô◊ô◊ò◊ô◊ù ◊ú◊ê ◊ë◊ê ◊ë◊®◊í◊ú\n",
      "[1.12.2024, 12:21:57] Omri: ◊ñ◊î◊ï ◊ê◊†◊ô ◊¢◊ï◊©◊î ◊î◊§◊°◊ß◊î ◊ß◊¶◊™\n",
      "[1.12.2024, 15:45:05] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊©◊û◊¢ ◊ê◊ô◊ñ◊î ◊§◊®◊§◊® ◊ê◊™◊î, ◊¶◊®◊ô◊ö ◊ú◊ë◊ó◊ï◊® ◊û◊î◊®◊©◊ô◊û◊î ◊î◊û◊™◊†◊î, ◊™◊©◊ê◊ô◊® ◊ß◊¶◊™ ◊í◊ù ◊ú◊ê◊ó◊®◊ô◊ù\n",
      "[1.12.2024, 16:41:17] ◊†◊ô◊ë ◊ê◊§◊ß◊î: ◊ô◊© ◊©◊ô◊¢◊ï◊®◊ô ◊ë◊ô◊™ ◊ë nlpü•≤\n",
      "[1.12.2024, 16:41:34] Omri: ◊ë◊ì◊ô◊ï◊ß ◊î◊™◊ó◊ú◊™◊ô ◊ú◊©◊ë◊™ ◊¢◊ú ◊ñ◊î\n",
      "\n"
     ]
    }
   ],
   "source": [
    "whatsapp_text_file_path = 'whatsapp_chat.txt'\n",
    "\n",
    "# Read the file content into the variable\n",
    "with open(whatsapp_text_file_path, 'r', encoding='utf-8') as file:\n",
    "    whatsapp_text = file.read()\n",
    "\n",
    "# Print the content (optional)\n",
    "print(whatsapp_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               DateTime    Sender  \\\n",
      "0   13.2.2024, 19:26:47  ◊†◊ô◊ë ◊ê◊§◊ß◊î   \n",
      "1   13.2.2024, 19:26:47      Omri   \n",
      "2   13.2.2024, 19:26:47      Omri   \n",
      "3   13.2.2024, 19:26:47      Omri   \n",
      "4   13.2.2024, 22:29:55  ◊†◊ô◊ë ◊ê◊§◊ß◊î   \n",
      "..                  ...       ...   \n",
      "92  1.12.2024, 12:21:37      Omri   \n",
      "93  1.12.2024, 12:21:48      Omri   \n",
      "94  1.12.2024, 12:21:57      Omri   \n",
      "95  1.12.2024, 15:45:05  ◊†◊ô◊ë ◊ê◊§◊ß◊î   \n",
      "96  1.12.2024, 16:41:17  ◊†◊ô◊ë ◊ê◊§◊ß◊î   \n",
      "\n",
      "                                              message  \n",
      "0   ‚Äè◊î◊î◊ï◊ì◊¢◊ï◊™ ◊ï◊î◊©◊ô◊ó◊ï◊™ ◊û◊ï◊¶◊§◊†◊ï◊™ ◊û◊ß◊¶◊î ◊ú◊ß◊¶◊î. ◊ú◊ê◊£ ◊ê◊ó◊ì ◊û◊ó...  \n",
      "1   CamScanner 2024-02-13 18.57.pdf ‚Ä¢ ‚Äè11 ◊ì◊§◊ô◊ù ‚Äè◊û◊°...  \n",
      "2                                  ◊§◊™◊®◊ï◊ü ◊©◊ú◊ô ◊ú◊î◊°◊™◊ë◊®◊ï◊™  \n",
      "3                          ◊ê◊ù ◊û◊¶◊ê◊™ ◊ò◊¢◊ï◊™ ◊¢◊ú ◊î◊ì◊®◊ö ◊™◊¢◊ì◊õ◊ü  \n",
      "4                                   ◊ê◊ô◊ñ◊î ◊û◊ú◊ê◊ö ◊ê◊™◊î‚ù§Ô∏è‚ù§Ô∏è  \n",
      "..                                                ...  \n",
      "92                      ◊ê◊†◊ô ◊í◊ù ◊®◊ß ◊¢◊õ◊©◊ô◊ï ◊ó◊ï◊ñ◊® ◊ú◊¢◊†◊ô◊ô◊†◊ô◊ù  \n",
      "93                                3 ◊ì◊ô◊ô◊ò◊ô◊ù ◊ú◊ê ◊ë◊ê ◊ë◊®◊í◊ú  \n",
      "94                             ◊ñ◊î◊ï ◊ê◊†◊ô ◊¢◊ï◊©◊î ◊î◊§◊°◊ß◊î ◊ß◊¶◊™  \n",
      "95  ◊©◊û◊¢ ◊ê◊ô◊ñ◊î ◊§◊®◊§◊® ◊ê◊™◊î, ◊¶◊®◊ô◊ö ◊ú◊ë◊ó◊ï◊® ◊û◊î◊®◊©◊ô◊û◊î ◊î◊û◊™◊†◊î, ◊™...  \n",
      "96                               ◊ô◊© ◊©◊ô◊¢◊ï◊®◊ô ◊ë◊ô◊™ ◊ë nlpü•≤  \n",
      "\n",
      "[97 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define regex for splitting based on sender names\n",
    "\n",
    "names_pattern = r\"(?=\\[\\d{1,2}\\.\\d{1,2}\\.\\d{4}, \\d{2}:\\d{2}:\\d{2}\\] (Omri|◊†◊ô◊ë ◊ê◊§◊ß◊î))\"\n",
    "\n",
    "# Split the text into messages\n",
    "messages = re.split(names_pattern, whatsapp_text)\n",
    "\n",
    "# Process messages into structured format\n",
    "structured_messages = []\n",
    "for i in range(1, len(messages), 2):  # Skip every alternate index (matched name)\n",
    "    structured_messages.append(messages[i - 1] + messages[i])\n",
    "\n",
    "# Extract message details into a DataFrame\n",
    "message_pattern = r\"\\[(.*?)\\] (.*?): (.*)\"\n",
    "data = []\n",
    "for msg in structured_messages:\n",
    "    match = re.match(message_pattern, msg.strip())\n",
    "    if match:\n",
    "        date_time, sender, content = match.groups()\n",
    "        data.append({\"DateTime\": date_time, \"Sender\": sender, \"message\": content})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_whatsapp_processed_text = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_whatsapp_processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique url tokens (NLTK): 387\n",
      "Number of unique url tokens (spaCy): 394\n",
      "{'◊õ◊ê◊ë', '◊ë◊°◊ï◊§◊©', '◊ú◊ê', '◊û◊ß◊ï◊ú◊ß◊ú', '◊í◊†◊ë◊ï', '◊î', '◊ê◊ô◊ü', '◊ê◊ï◊™◊ï', '◊ñ◊î◊ï', '?', '◊©◊ù', '◊î◊ß◊ï◊®◊ß◊ô◊†◊ò', '◊ô◊¢◊ë◊ï◊®', '◊ë◊®◊í◊ú', '◊î◊ß◊ï', '◊û◊©◊î◊ï', '◊™◊¢◊©◊î', '◊ú◊ß◊¶◊î', '\\u200f◊û◊°◊û◊ö', '◊ë◊°◊ï◊ï◊ì◊®', '◊ú◊î◊ü', '◊¢◊ï◊ï◊†◊ï◊™', '◊ú◊†◊ï', ',', '◊§◊®◊ï◊ô◊ß◊ò', '◊î◊ô◊ô◊™◊ô', '◊©◊ë◊ï◊¢', '◊©◊ë◊ê◊û◊™', '◊û◊¶◊ú◊ô◊ó', '◊î◊ô◊î', '◊î◊õ◊ô', '◊ï◊ú◊î◊ê◊ñ◊ô◊ü', '◊ú◊î◊™◊ó◊ô◊ú', '◊û◊°◊ô◊ô◊ù', '◊û◊ó◊ï◊§◊©◊™◊ô', '◊©◊ú◊ó◊™◊ô', '◊ú', '◊ß◊ï◊û◊ô◊ò', '◊ï◊û◊ó◊®', '◊ó◊ó◊ó', 'You', '◊§◊ï◊©', '◊™◊í◊ô◊ì', '◊û◊ß◊¶◊î', '◊ê◊©◊ê◊ú', '◊™◊°◊ú◊ó', '◊ô◊ï◊™◊®', '◊î◊©◊ë◊ï◊¢', '◊í◊ô◊ò', '◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊ê◊ó◊ì', '◊í◊ù', '◊§◊™◊®◊ï◊ü', '◊°◊ô◊ô◊û◊ï', \"◊ú◊¶'◊ê◊ò\", '◊î◊™◊ë◊ú◊ë◊ú◊™◊ô', '◊†◊ï◊®◊û◊ú◊ô', '◊û◊ê◊§◊ß◊î', '◊û◊™◊õ◊ï◊ï◊ü', '◊õ◊§◊®◊™', '◊ë◊ü', 'LLM', '◊î◊ê◊ú◊î', '◊§◊ó◊ï◊™', '◊û◊ó◊ï◊•', '◊ê◊ï◊™◊ü', '◊ë◊ì◊ì', '◊ê◊í◊ô◊¢', '◊§◊ï◊®◊ó', '◊ê◊î◊ú◊ü', '◊î◊§◊®◊ï◊ô◊ß◊ò', '\\u200f◊°◊ò◊ô◊ß◊®', '◊ê◊©◊ú◊ó', '◊î◊ô◊ï', '◊¶◊®◊ô◊õ◊ô◊ù', '◊û◊®◊ó◊ï◊ß', '◊ú◊ï◊ï◊ì◊ê', '◊ë◊ô◊ó◊ì', '◊û◊®◊í◊©', '◊î◊ó◊ú◊ï◊û◊ï◊™', '◊ú◊î◊™◊ó◊ë◊®', '◊û◊ê◊ï◊™◊ï', '◊ó◊ë◊®', '◊®◊ô◊¢◊†◊ï◊ü', '◊ó◊ë◊®◊î', '◊ë◊ú◊ë◊ï◊ú◊ô◊ù', '◊ë◊©◊ë◊ô◊ú', '◊û◊™◊ï◊ö', '\\u200f7', '◊î◊ï◊©◊û◊ò◊î', '◊ë◊ô◊†◊™◊ô◊ô◊ù', '\\u200f11', '◊î◊ß◊ï◊®◊ß◊ô◊†◊òüôà', '◊¢◊ù', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ô◊ù', '◊ê◊ô◊ö', '◊©◊ú◊ï◊û◊ö', '◊ò◊ï◊ë', '◊î◊û◊ß◊ï◊ù', '◊ê◊ñ', '◊™◊®◊ê◊î', '◊î◊ï◊ò◊ú', 'Attention', '◊ó◊ô◊†◊ù', '◊ó◊ñ◊®◊™◊ô', '◊ë◊ô◊™', '◊ñ◊î', '◊ú◊ô', '◊†◊ï◊©◊ê◊ô◊ù', '◊ë◊õ◊ï◊†◊†◊ï◊™', '◊©◊†◊ô◊î', '◊ú◊©◊ú◊ï◊ó', '◊ë◊©◊ï◊ß', '◊ë◊§◊®◊ï◊ô◊ß◊ò', '◊ë◊ß◊ë◊ï◊¶◊î', '◊û◊¢◊ì◊ô◊£', '◊ê◊ï◊ú◊ô', '◊û◊û◊™◊ô◊†◊ô◊ù', '◊ô◊ß◊®', '◊î◊¢◊ô◊ü', '◊™◊î◊ô◊î', '◊ó◊ô◊ú', '◊õ◊ú', '◊ô◊ê', '2', '◊¢◊õ◊©◊ô◊ï', '◊§◊†◊ô◊ï◊™', '◊û◊í◊†◊ô◊ë', '◊ê◊†◊ó◊†◊ï', '◊ó◊ó◊ó◊ó', '◊ê◊ù', '◊ê◊ô◊§◊î', '◊û◊ó◊®', '◊î◊ñ◊î', '◊ú◊ê◊®◊ó', '◊ê◊ó◊®◊ô◊ù', '◊ë◊í◊ú◊ú', '◊ê◊ú◊ö', '◊û◊î', '◊ú◊ë◊ó◊ï◊®', '◊û◊ï◊™◊ß', '◊©◊ú◊†◊ï', 'All', '◊ú◊í◊û◊®◊ô', '◊õ◊ö', '◊ê◊†◊°◊î', '◊¶◊®◊ô◊ö', '◊ê◊ë◊ì◊ï◊ß', '◊ú-WhatsApp', '◊û◊û◊©', '◊ñ◊ï◊®◊ù', '◊û◊ú◊ê◊ö', '◊©◊û◊ô◊ò◊îüòõ', '◊§◊†◊ô', '◊î◊®◊õ◊ë', '◊û◊í◊ô◊¢', '◊©◊ú◊ï◊©◊î', '\\u200f15', '◊ò◊¢◊ù', '◊ê◊°◊™◊õ◊ú', '◊î◊ô◊ï◊ù', '◊ê◊ô◊™◊ï', '◊†◊ì◊û◊î', '◊í◊û◊ï◊®', '◊ú◊ö‚Ä¶', '◊¢◊†◊ô◊ô◊ü', '◊ê◊™◊î', '◊ú◊í◊ô◊ò', '◊ë◊¢◊ë◊ï◊ì◊î', '◊ê◊ë◊ï◊ê', '◊ú◊ß◊®◊ï◊ê', '◊ñ◊ô◊ï', '◊ê◊ï◊™◊ö', '◊û◊ì◊ô◊ô', '◊ë◊í◊ô◊ò', '‚Ä¢', '◊î◊û◊¢◊®◊ë', '◊ú◊ê◊£', '◊ú◊®◊ê◊ï◊™', '◊©◊†◊ï◊ü', '◊§◊ô◊ñ◊ô◊™', '◊®◊ê◊©', '◊®◊ê◊ô◊™', '◊ô◊õ◊ï◊ú', '◊ú◊î◊ë◊ô◊ü', '◊©◊í◊ù', '◊ô◊ó◊°◊ô◊™', '◊ú◊î◊í◊ô◊¢', '◊ê◊ï', '◊ú◊°◊ô◊û◊°◊ò◊®', 'Need.pdf', '◊û◊ß◊ï◊ï◊î', '◊®◊ß', '◊§◊†◊ô◊ö', '◊ú◊¢◊¶◊û◊ù', '◊ë◊ê◊ï◊ï◊ô◊®◊î', '◊ô◊ï◊ì◊¢', '◊ï◊ê◊™◊ó◊ô◊ú', '◊ó◊©◊ë◊™◊ô', '◊ó◊ï◊ñ◊®', '◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™üòâ', '◊¢◊ú', '◊©◊™◊§◊°', '◊ê◊ó', '◊î◊†◊§◊ß◊™', '◊ò◊†◊ß◊ô◊ï‚ù§Ô∏è', 'gpt', '◊ô◊©', '◊ß◊¶◊™', '!', '◊©◊ú◊ê', '◊ó◊ë◊®◊ô◊ù', '◊ë◊®◊ß', '◊©◊ô◊†◊ô◊™', '◊™◊¢◊ì◊õ◊ü', '◊ë◊ò◊ó', '◊ú◊¶◊¢◊®◊ô', '◊ú◊î◊™◊ß◊ì◊ù', '◊©◊ñ◊î', '◊™◊ê◊û◊ô◊ü', '◊î◊®◊ë◊î', '◊î◊§◊®◊ï◊¢', '◊ê◊ó◊ú◊ï◊™', '◊©◊ú◊ô', 'LM', '3', '◊®◊¶◊ô◊™◊ô', '◊ê◊ó◊ô', '◊©◊ô◊û◊ï◊©', '◊î◊ê◊û◊™', '◊¢◊ï◊û◊°', 'main.py', '◊ú◊¢◊†◊ô◊ô◊†◊ô◊ù', '◊û◊¶◊ê◊™', '◊ú◊ë◊ï◊ê', '◊û◊ó◊ô◊ú', '◊ë◊õ◊ô◊™◊î', '◊°◊ï◊§◊©', '◊ë◊©◊ë◊ï◊¢', 'storage.py', '◊ë◊ú◊û◊ô◊ì◊î', '◊ë◊™◊ß◊ï◊ï◊î', '◊ó◊ó', ':', '◊ê◊†◊ô', '◊ê◊ú', '◊ú◊†◊°◊ï◊™', '◊î◊ô◊ô◊ì◊î', '◊™◊§◊ô◊ú◊î', '◊†◊©◊û◊¢', '◊ï◊î◊ï◊®◊ô◊ì◊ï', '◊ô◊ê◊ú◊ú◊ï◊™', '◊¢◊ï◊©◊î', '◊î◊ú◊ï', '◊©◊ï◊ï◊î', '◊û◊û◊©◊ô◊ö', '◊î◊°◊§◊ß◊™◊ô', '◊©◊ï◊ù', '◊õ◊û◊¢◊ò', '◊™◊©◊ï◊ë◊î', '◊™◊õ◊£', '◊ô◊ï◊ù', '◊ú◊ß◊ì◊ù', '◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™', '◊ï◊ê◊û◊©◊ô◊ö', '◊û◊¶◊ô◊í', 'üòÜ', '◊õ◊ô◊ï◊ï◊†◊ô◊ù', '◊ú◊©◊ë◊™', '4', '◊©◊î◊ô◊ê', '◊¶◊ì◊ô◊ß', '◊î◊ô◊ô◊ò◊ß◊°', '◊ë◊™◊õ◊ú◊°', '◊†◊®◊ê◊î', '◊î◊û◊™◊†◊î', '◊™◊©◊ê◊ô◊®', '2024-02-13', '◊ú◊î◊°◊™◊ë◊®◊ï◊™', '◊ï◊î◊©◊ô◊ó◊ï◊™', '◊ì◊ô◊ô◊ò◊ô◊ù', '◊©◊ô◊î◊ô◊î', '◊§◊®◊§◊®', '◊ú◊ì◊ó◊ï◊£', '◊î◊©◊ë◊ï◊¢◊ô◊ô◊ù', '◊ô◊ó◊ñ◊ô◊®◊ï', '◊î◊û◊°◊õ◊ù', '◊ú◊ó◊†◊ô◊î', '◊©◊ô◊ï◊ì◊¢◊ô◊ù', '18.57.pdf', '◊ò◊†◊ß◊ô◊ïüôÉ', '◊ê◊ô◊ú◊™', 'finalProject.pdf', '◊î◊û◊†◊ó◊î', '◊©◊õ◊ó◊™◊ù', '◊ê◊†◊©◊ô◊ù', '◊©◊û◊ë◊ô◊ü', '\\u200f◊î◊î◊ï◊ì◊¢◊ï◊™', '◊î◊ó◊ì◊©', '◊¢◊ï◊ì', '◊û◊î◊®◊©◊ô◊û◊î', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊û◊ë◊ó◊ü', '.', 'parsing.py', '◊ë◊ú◊ô', '◊™◊î◊†◊î', '◊©◊ô◊¢◊ï◊®◊ô', '◊õ◊ü', '◊ë◊ô◊û◊ô◊†◊ï', '◊®◊ï◊¶◊î', '◊ê◊¶◊ú◊ô', '◊ú◊ê◊ó◊®◊ô◊ù', '◊ò◊¢◊ï◊™', '◊ê◊ë◊ú', '◊û◊©◊ù', '◊§◊î', '◊©◊ê◊™◊ù', '◊î◊ú◊ö', '◊î◊¢◊ú◊™◊î', '◊û', '◊û◊®◊í◊ô◊©', '◊ô◊ê◊ó', '◊™◊®◊¶◊î', '◊¢◊ì◊ô◊£', '◊©◊û◊¢', '◊®◊¶◊ô◊†◊ô', '◊ú◊ñ◊î', '◊©◊ô◊§◊®◊ó', '◊ë◊°◊ì◊®', '◊ß◊ï◊®◊î', '◊î◊ï◊©◊û◊ò', '◊ß◊ô◊†◊í', '◊û◊ï◊¶◊§◊†◊ï◊™', '◊û◊ê◊û◊®◊ô◊ù', '..', '◊ß◊ú◊ê◊ë', '◊î◊ì◊®◊ö', '◊ë◊ê', '◊î◊ë◊ê', 'nlpü•≤', '◊©◊ê◊†◊ô', '◊©◊©◊ú◊ó◊ï', '\\u200f◊î◊™◊û◊ï◊†◊î', '◊î5', '◊©◊¢◊©◊ô◊™', '◊ì◊§◊ô◊ù', '◊û◊ß◊ï◊ù', '◊°◊§◊ô◊í◊î', '◊©◊†◊™', '◊ò◊†◊ß◊ô◊ï', '◊¢◊û◊ï◊ß◊î+', '◊ß◊ì◊ô◊û◊î', '◊ë', '◊©◊î◊ô◊î', '◊ê◊ô◊™◊ö', '◊ê◊ô◊ñ◊î', '◊ë◊ñ◊ï◊ù', '◊û◊ï◊¶◊ú◊ó', '◊°◊ë◊ë◊î', 'Is', '◊ê◊§◊©◊®◊ï◊™', '◊†◊©◊ê◊®◊ï', '◊ô◊î◊ô◊î', '◊ú◊ö', '◊ê◊ï◊ï◊ô◊®◊î', '◊™◊©◊û◊¢', '◊î◊§◊¢◊ù', 'üôèüèª', '◊ô◊ê◊ú◊î', '◊ú◊ë◊¶◊¢', '◊õ◊ë◊®', '◊ê◊ó◊ñ◊ô◊®', '◊ú◊¢◊©◊ï◊™', '◊ê◊™', 'CamScanner', '◊ú2', '◊î◊†◊ï◊©◊ê', '◊ê◊™◊î‚ù§Ô∏è‚ù§Ô∏è', '◊î◊§◊°◊ß◊î', '◊î◊ó◊†◊ô◊î', '◊ë◊û◊î◊®◊î', '◊ê◊ï◊õ◊ú', '◊®◊ê◊ô◊™◊ô', '◊ï◊®◊ü', '◊û◊ê◊û◊ô◊ü', '◊©◊î◊ß◊ú◊ß◊ï◊ú'}\n",
      "{'◊õ◊ê◊ë', '◊ë◊°◊ï◊§◊©', '◊ú◊ê', '◊û◊ß◊ï◊ú◊ß◊ú', '◊í◊†◊ë◊ï', 'üôà', '◊î', '◊ê◊ô◊ü', '◊ê◊ï◊™◊ï', '◊ñ◊î◊ï', '?', '◊©◊ù', '◊î◊ß◊ï◊®◊ß◊ô◊†◊ò', '◊ô◊¢◊ë◊ï◊®', '◊ë◊®◊í◊ú', '◊î◊ß◊ï', '◊û◊©◊î◊ï', '◊™◊¢◊©◊î', '◊ú◊ß◊¶◊î', '\\u200f◊û◊°◊û◊ö', '◊ë◊°◊ï◊ï◊ì◊®', '◊ú◊î◊ü', '◊¢◊ï◊ï◊†◊ï◊™', '◊ú◊†◊ï', ',', '◊§◊®◊ï◊ô◊ß◊ò', '◊î◊ô◊ô◊™◊ô', '◊©◊ë◊ï◊¢', '◊©◊ë◊ê◊û◊™', '◊û◊¶◊ú◊ô◊ó', '◊î◊ô◊î', '◊î◊õ◊ô', '◊ï◊ú◊î◊ê◊ñ◊ô◊ü', '◊ú◊î◊™◊ó◊ô◊ú', '◊û◊°◊ô◊ô◊ù', '◊û◊ó◊ï◊§◊©◊™◊ô', 'üòâ', '◊©◊ú◊ó◊™◊ô', '◊ú', '◊ß◊ï◊û◊ô◊ò', '◊ï◊û◊ó◊®', '◊ó◊ó◊ó', 'You', '◊§◊ï◊©', '◊™◊í◊ô◊ì', '◊û◊ß◊¶◊î', '◊ê◊©◊ê◊ú', '◊™◊°◊ú◊ó', '◊ô◊ï◊™◊®', '◊î◊©◊ë◊ï◊¢', '◊í◊ô◊ò', '◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊ê◊ó◊ì', '◊í◊ù', '◊§◊™◊®◊ï◊ü', '◊°◊ô◊ô◊û◊ï', \"◊ú◊¶'◊ê◊ò\", '◊î◊™◊ë◊ú◊ë◊ú◊™◊ô', '‚Ä¶', '◊†◊ï◊®◊û◊ú◊ô', '◊û◊ê◊§◊ß◊î', '◊û◊™◊õ◊ï◊ï◊ü', '◊õ◊§◊®◊™', '◊ë◊ü', 'LLM', '◊î◊ê◊ú◊î', '◊§◊ó◊ï◊™', '◊û◊ó◊ï◊•', '◊ê◊ï◊™◊ü', '◊ë◊ì◊ì', '◊ê◊í◊ô◊¢', 'Ô∏è', '◊§◊ï◊®◊ó', '◊ê◊î◊ú◊ü', '◊î◊§◊®◊ï◊ô◊ß◊ò', '\\u200f◊°◊ò◊ô◊ß◊®', '◊ê◊©◊ú◊ó', '◊î◊ô◊ï', 'üôè', '◊û◊®◊ó◊ï◊ß', '◊ú◊ï◊ï◊ì◊ê', '◊¶◊®◊ô◊õ◊ô◊ù', '◊ë◊ô◊ó◊ì', '◊û◊®◊í◊©', '◊î◊ó◊ú◊ï◊û◊ï◊™', '◊ú◊î◊™◊ó◊ë◊®', '◊û◊ê◊ï◊™◊ï', '◊ó◊ë◊®', '◊®◊ô◊¢◊†◊ï◊ü', '◊ó◊ë◊®◊î', '◊ë◊ú◊ë◊ï◊ú◊ô◊ù', '◊ë◊©◊ë◊ô◊ú', '◊û◊™◊ï◊ö', 'üòõ', '\\u200f7', '◊î◊ï◊©◊û◊ò◊î', '◊ë◊ô◊†◊™◊ô◊ô◊ù', '\\u200f11', '◊¢◊ù', '◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ô◊ù', '◊ê◊ô◊ö', '◊©◊ú◊ï◊û◊ö', '◊ò◊ï◊ë', '◊î◊û◊ß◊ï◊ù', '◊ê◊ñ', '◊™◊®◊ê◊î', '◊î◊ï◊ò◊ú', 'Attention', '◊ó◊ô◊†◊ù', '◊ó◊ñ◊®◊™◊ô', '◊ë◊ô◊™', '◊ñ◊î', '◊ú◊ô', '◊†◊ï◊©◊ê◊ô◊ù', '◊ë◊õ◊ï◊†◊†◊ï◊™', '◊©◊†◊ô◊î', '◊ú◊©◊ú◊ï◊ó', 'üôÉ', '◊ë◊©◊ï◊ß', '◊ë◊§◊®◊ï◊ô◊ß◊ò', '◊ë◊ß◊ë◊ï◊¶◊î', '◊û◊¢◊ì◊ô◊£', '◊ê◊ï◊ú◊ô', '◊û◊û◊™◊ô◊†◊ô◊ù', '◊ô◊ß◊®', '◊î◊¢◊ô◊ü', '◊™◊î◊ô◊î', '◊ó◊ô◊ú', 'WhatsApp', '◊õ◊ú', '◊ô◊ê', '2', '◊¢◊õ◊©◊ô◊ï', '◊§◊†◊ô◊ï◊™', '◊û◊í◊†◊ô◊ë', '◊ê◊†◊ó◊†◊ï', '◊ó◊ó◊ó◊ó', '◊ê◊ù', '◊ê◊ô◊§◊î', '◊û◊ó◊®', '◊î◊ñ◊î', '◊ú◊ê◊®◊ó', '◊©◊û◊ô◊ò◊î', '◊ê◊ó◊®◊ô◊ù', '◊ë◊í◊ú◊ú', '◊ê◊ú◊ö', '◊û◊î', '◊ú◊ë◊ó◊ï◊®', '◊û◊ï◊™◊ß', '◊©◊ú◊†◊ï', 'All', '◊ú◊í◊û◊®◊ô', '◊õ◊ö', '◊ê◊†◊°◊î', '◊¶◊®◊ô◊ö', '◊ê◊ë◊ì◊ï◊ß', '◊û◊û◊©', '◊ñ◊ï◊®◊ù', '◊û◊ú◊ê◊ö', '◊§◊†◊ô', '◊î◊®◊õ◊ë', '◊û◊í◊ô◊¢', '◊©◊ú◊ï◊©◊î', '\\u200f15', '◊ò◊¢◊ù', '◊ê◊°◊™◊õ◊ú', '◊î◊ô◊ï◊ù', '◊ê◊ô◊™◊ï', '◊†◊ì◊û◊î', '◊í◊û◊ï◊®', '◊¢◊†◊ô◊ô◊ü', '◊ê◊™◊î', '◊ú◊í◊ô◊ò', '◊ë◊¢◊ë◊ï◊ì◊î', '◊ê◊ë◊ï◊ê', '◊ú◊ß◊®◊ï◊ê', '◊ñ◊ô◊ï', '◊ê◊ï◊™◊ö', '◊û◊ì◊ô◊ô', '◊ë◊í◊ô◊ò', '‚Ä¢', '◊î◊û◊¢◊®◊ë', '◊ú◊ê◊£', '◊ú◊®◊ê◊ï◊™', '◊©◊†◊ï◊ü', '◊§◊ô◊ñ◊ô◊™', '◊®◊ê◊©', '◊®◊ê◊ô◊™', '◊ô◊õ◊ï◊ú', '◊ú◊î◊ë◊ô◊ü', '◊©◊í◊ù', '◊ô◊ó◊°◊ô◊™', '◊ú◊î◊í◊ô◊¢', '◊ê◊ï', '◊ú◊°◊ô◊û◊°◊ò◊®', 'Need.pdf', '◊û◊ß◊ï◊ï◊î', '◊®◊ß', '◊§◊†◊ô◊ö', '◊ú◊¢◊¶◊û◊ù', '◊ë◊ê◊ï◊ï◊ô◊®◊î', '◊ô◊ï◊ì◊¢', '◊ï◊ê◊™◊ó◊ô◊ú', '◊ó◊©◊ë◊™◊ô', '◊ó◊ï◊ñ◊®', '◊¢◊ú', '◊©◊™◊§◊°', '◊ê◊ó', '◊î◊†◊§◊ß◊™', 'gpt', '◊ô◊©', '◊ß◊¶◊™', '!', '◊©◊ú◊ê', '◊ó◊ë◊®◊ô◊ù', '◊ë◊®◊ß', '◊©◊ô◊†◊ô◊™', '-', '◊™◊¢◊ì◊õ◊ü', '◊ë◊ò◊ó', '◊ú◊¶◊¢◊®◊ô', '◊ú◊î◊™◊ß◊ì◊ù', '◊©◊ñ◊î', '◊™◊ê◊û◊ô◊ü', '◊î◊®◊ë◊î', '◊î◊§◊®◊ï◊¢', '◊ê◊ó◊ú◊ï◊™', '◊©◊ú◊ô', 'LM', 'üèª', '3', '◊®◊¶◊ô◊™◊ô', '◊ê◊ó◊ô', '◊©◊ô◊û◊ï◊©', '◊î◊ê◊û◊™', '◊¢◊ï◊û◊°', 'main.py', '◊ú◊¢◊†◊ô◊ô◊†◊ô◊ù', '◊û◊¶◊ê◊™', '◊ú◊ë◊ï◊ê', '◊û◊ó◊ô◊ú', '◊ë◊õ◊ô◊™◊î', '◊°◊ï◊§◊©', ' ', '◊ë◊©◊ë◊ï◊¢', 'storage.py', '◊ë◊ú◊û◊ô◊ì◊î', '◊ë◊™◊ß◊ï◊ï◊î', '◊ó◊ó', ':', '◊ê◊†◊ô', '◊ê◊ú', '◊ú◊†◊°◊ï◊™', '◊î◊ô◊ô◊ì◊î', '◊™◊§◊ô◊ú◊î', '◊†◊©◊û◊¢', '◊ï◊î◊ï◊®◊ô◊ì◊ï', '◊ô◊ê◊ú◊ú◊ï◊™', '◊¢◊ï◊©◊î', '◊î◊ú◊ï', '◊©◊ï◊ï◊î', '◊û◊û◊©◊ô◊ö', '◊î◊°◊§◊ß◊™◊ô', '◊©◊ï◊ù', '◊õ◊û◊¢◊ò', '◊™◊©◊ï◊ë◊î', '◊™◊õ◊£', '◊ô◊ï◊ù', '◊ú◊ß◊ì◊ù', '◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™', '◊ï◊ê◊û◊©◊ô◊ö', '◊û◊¶◊ô◊í', 'üòÜ', '02', '◊õ◊ô◊ï◊ï◊†◊ô◊ù', '◊ú◊©◊ë◊™', '4', '◊©◊î◊ô◊ê', '◊¶◊ì◊ô◊ß', '◊î◊ô◊ô◊ò◊ß◊°', '◊ë◊™◊õ◊ú◊°', '◊†◊®◊ê◊î', '◊î◊û◊™◊†◊î', '◊™◊©◊ê◊ô◊®', '◊ú◊î◊°◊™◊ë◊®◊ï◊™', '◊ï◊î◊©◊ô◊ó◊ï◊™', '◊ì◊ô◊ô◊ò◊ô◊ù', '◊©◊ô◊î◊ô◊î', '◊§◊®◊§◊®', '◊ú◊ì◊ó◊ï◊£', '◊î◊©◊ë◊ï◊¢◊ô◊ô◊ù', '◊ô◊ó◊ñ◊ô◊®◊ï', '◊î◊û◊°◊õ◊ù', '◊ú◊ó◊†◊ô◊î', '◊©◊ô◊ï◊ì◊¢◊ô◊ù', '18.57.pdf', '◊ê◊ô◊ú◊™', 'finalProject.pdf', '◊î◊û◊†◊ó◊î', '◊©◊õ◊ó◊™◊ù', '◊ê◊†◊©◊ô◊ù', '◊©◊û◊ë◊ô◊ü', '\\u200f◊î◊î◊ï◊ì◊¢◊ï◊™', '◊î◊ó◊ì◊©', '◊¢◊ï◊ì', '◊û◊î◊®◊©◊ô◊û◊î', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊û◊ë◊ó◊ü', '.', 'parsing.py', '◊ë◊ú◊ô', '◊™◊î◊†◊î', '◊©◊ô◊¢◊ï◊®◊ô', '◊õ◊ü', '◊ë◊ô◊û◊ô◊†◊ï', '◊®◊ï◊¶◊î', '◊ê◊¶◊ú◊ô', '◊ú◊ê◊ó◊®◊ô◊ù', '◊ò◊¢◊ï◊™', '◊ê◊ë◊ú', '◊û◊©◊ù', '◊§◊î', '◊©◊ê◊™◊ù', '◊î◊ú◊ö', '◊î◊¢◊ú◊™◊î', '◊û', '◊û◊®◊í◊ô◊©', '◊ô◊ê◊ó', '◊™◊®◊¶◊î', '◊¢◊ì◊ô◊£', '◊©◊û◊¢', '‚ù§', '◊®◊¶◊ô◊†◊ô', '◊ú◊ñ◊î', '13', '◊©◊ô◊§◊®◊ó', '◊ë◊°◊ì◊®', '◊ß◊ï◊®◊î', '◊î◊ï◊©◊û◊ò', '◊ß◊ô◊†◊í', '◊û◊ï◊¶◊§◊†◊ï◊™', '◊û◊ê◊û◊®◊ô◊ù', '..', '◊ß◊ú◊ê◊ë', '◊î◊ì◊®◊ö', '◊ë◊ê', '◊î◊ë◊ê', 'nlpü•≤', '◊©◊ê◊†◊ô', '◊©◊©◊ú◊ó◊ï', '\\u200f◊î◊™◊û◊ï◊†◊î', '◊î5', '◊©◊¢◊©◊ô◊™', '◊ì◊§◊ô◊ù', '◊û◊ß◊ï◊ù', '◊°◊§◊ô◊í◊î', '◊©◊†◊™', '◊ò◊†◊ß◊ô◊ï', '◊¢◊û◊ï◊ß◊î+', '◊ß◊ì◊ô◊û◊î', '◊ë', '◊©◊î◊ô◊î', '◊ê◊ô◊™◊ö', '◊ê◊ô◊ñ◊î', '◊ë◊ñ◊ï◊ù', '◊û◊ï◊¶◊ú◊ó', '◊°◊ë◊ë◊î', 'Is', '◊ê◊§◊©◊®◊ï◊™', '◊†◊©◊ê◊®◊ï', '◊ô◊î◊ô◊î', '◊ú◊ö', '◊ê◊ï◊ï◊ô◊®◊î', '◊™◊©◊û◊¢', '2024', '◊î◊§◊¢◊ù', '◊ô◊ê◊ú◊î', '◊ú◊ë◊¶◊¢', '◊õ◊ë◊®', '◊ê◊ó◊ñ◊ô◊®', '◊ú◊¢◊©◊ï◊™', '◊ê◊™', 'CamScanner', '◊ú2', '◊î◊†◊ï◊©◊ê', '◊î◊§◊°◊ß◊î', '◊î◊ó◊†◊ô◊î', '◊ë◊û◊î◊®◊î', '◊ê◊ï◊õ◊ú', '◊®◊ê◊ô◊™◊ô', '◊ï◊®◊ü', '◊û◊ê◊û◊ô◊ü', '◊©◊î◊ß◊ú◊ß◊ï◊ú'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the entire dataframe\n",
    "nltk_tokens_set_whatsapp = text_functions.create_token_set_nltk(df_whatsapp_processed_text)\n",
    "spacy_tokens_set_whatsapp = text_functions.create_token_set_spacy(df_whatsapp_processed_text, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique url tokens (NLTK):\", len(nltk_tokens_set_whatsapp))\n",
    "print(\"Number of unique url tokens (spaCy):\", len(spacy_tokens_set_whatsapp))\n",
    "\n",
    "\n",
    "print(nltk_tokens_set_whatsapp)\n",
    "print(spacy_tokens_set_whatsapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences of tokenization between NLTK and spaCy: \n",
    "\n",
    "NLTK often treats punctuation as standalone tokens and may have limited support for modern elements like emojis, while spaCy is better equipped for handling such characters, treating emojis and symbols as distinct tokens. spaCy also combines or interprets certain adjacent special characters and compound tokens more effectively than NLTK. These differences make spaCy preferable for nuanced, modern applications and NLTK better for traditional text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (NLTK): 387\n",
      "Number of unique lemmas (spaCy): 394\n",
      "{'◊õ◊ê◊ë', '◊ë◊°◊ï◊§◊©', '◊ú◊ê', '◊û◊ß◊ï◊ú◊ß◊ú', '◊í◊†◊ë◊ï', '◊î', '◊ê◊ô◊ü', '◊ê◊ï◊™◊ï', '◊ñ◊î◊ï', '?', '◊©◊ù', '◊î◊ß◊ï◊®◊ß◊ô◊†◊ò', '◊ô◊¢◊ë◊ï◊®', '◊ë◊®◊í◊ú', '◊î◊ß◊ï', '◊û◊©◊î◊ï', '◊™◊¢◊©◊î', '◊ú◊ß◊¶◊î', '\\u200f◊û◊°◊û◊ö', '◊ë◊°◊ï◊ï◊ì◊®', '◊ú◊î◊ü', '◊¢◊ï◊ï◊†◊ï◊™', '◊ú◊†◊ï', ',', '◊§◊®◊ï◊ô◊ß◊ò', '◊î◊ô◊ô◊™◊ô', '◊©◊ë◊ï◊¢', '◊û◊¶◊ú◊ô◊ó', '◊©◊ë◊ê◊û◊™', '◊î◊ô◊î', '◊î◊õ◊ô', '◊ï◊ú◊î◊ê◊ñ◊ô◊ü', '◊ú◊î◊™◊ó◊ô◊ú', '◊û◊°◊ô◊ô◊ù', '◊û◊ó◊ï◊§◊©◊™◊ô', '◊©◊ú◊ó◊™◊ô', '◊ú', '◊ß◊ï◊û◊ô◊ò', '◊ï◊û◊ó◊®', '◊ó◊ó◊ó', 'You', '◊§◊ï◊©', '◊™◊í◊ô◊ì', '◊û◊ß◊¶◊î', '◊ê◊©◊ê◊ú', '◊™◊°◊ú◊ó', '◊ô◊ï◊™◊®', '◊î◊©◊ë◊ï◊¢', '◊í◊ô◊ò', '◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊ê◊ó◊ì', '◊í◊ù', '◊§◊™◊®◊ï◊ü', '◊°◊ô◊ô◊û◊ï', \"◊ú◊¶'◊ê◊ò\", '◊î◊™◊ë◊ú◊ë◊ú◊™◊ô', '◊†◊ï◊®◊û◊ú◊ô', '◊û◊ê◊§◊ß◊î', '◊û◊™◊õ◊ï◊ï◊ü', '◊õ◊§◊®◊™', '◊ë◊ü', 'LLM', '◊î◊ê◊ú◊î', '◊§◊ó◊ï◊™', '◊û◊ó◊ï◊•', '◊ê◊ï◊™◊ü', '◊ë◊ì◊ì', '◊ê◊í◊ô◊¢', '◊§◊ï◊®◊ó', '◊ê◊î◊ú◊ü', '◊î◊§◊®◊ï◊ô◊ß◊ò', '\\u200f◊°◊ò◊ô◊ß◊®', '◊ê◊©◊ú◊ó', '◊î◊ô◊ï', '◊¶◊®◊ô◊õ◊ô◊ù', '◊û◊®◊ó◊ï◊ß', '◊ú◊ï◊ï◊ì◊ê', '◊ë◊ô◊ó◊ì', '◊û◊®◊í◊©', '◊î◊ó◊ú◊ï◊û◊ï◊™', '◊ú◊î◊™◊ó◊ë◊®', '◊û◊ê◊ï◊™◊ï', '◊ó◊ë◊®', '◊®◊ô◊¢◊†◊ï◊ü', '◊ó◊ë◊®◊î', '◊ë◊ú◊ë◊ï◊ú◊ô◊ù', '◊ë◊©◊ë◊ô◊ú', '◊û◊™◊ï◊ö', '\\u200f7', '◊î◊ï◊©◊û◊ò◊î', '◊ë◊ô◊†◊™◊ô◊ô◊ù', '\\u200f11', '◊î◊ß◊ï◊®◊ß◊ô◊†◊òüôà', '◊¢◊ù', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ô◊ù', '◊ê◊ô◊ö', '◊©◊ú◊ï◊û◊ö', '◊ò◊ï◊ë', '◊î◊û◊ß◊ï◊ù', '◊ê◊ñ', '◊™◊®◊ê◊î', '◊î◊ï◊ò◊ú', 'Attention', '◊ó◊ô◊†◊ù', '◊ó◊ñ◊®◊™◊ô', '◊ë◊ô◊™', '◊ñ◊î', '◊ú◊ô', '◊†◊ï◊©◊ê◊ô◊ù', '◊ë◊õ◊ï◊†◊†◊ï◊™', '◊©◊†◊ô◊î', '◊ú◊©◊ú◊ï◊ó', '◊ë◊©◊ï◊ß', '◊ë◊§◊®◊ï◊ô◊ß◊ò', '◊ë◊ß◊ë◊ï◊¶◊î', '◊û◊¢◊ì◊ô◊£', '◊ê◊ï◊ú◊ô', '◊û◊û◊™◊ô◊†◊ô◊ù', '◊ô◊ß◊®', '◊î◊¢◊ô◊ü', '◊™◊î◊ô◊î', '◊ó◊ô◊ú', '◊õ◊ú', '◊ô◊ê', '2', '◊¢◊õ◊©◊ô◊ï', '◊§◊†◊ô◊ï◊™', '◊û◊í◊†◊ô◊ë', '◊ê◊†◊ó◊†◊ï', '◊ó◊ó◊ó◊ó', '◊ê◊ù', '◊ê◊ô◊§◊î', '◊û◊ó◊®', '◊î◊ñ◊î', '◊ú◊ê◊®◊ó', '◊ê◊ó◊®◊ô◊ù', '◊ë◊í◊ú◊ú', '◊ê◊ú◊ö', '◊û◊î', '◊ú◊ë◊ó◊ï◊®', '◊û◊ï◊™◊ß', '◊©◊ú◊†◊ï', 'All', '◊ú◊í◊û◊®◊ô', '◊õ◊ö', '◊ê◊†◊°◊î', '◊¶◊®◊ô◊ö', '◊ê◊ë◊ì◊ï◊ß', '◊ú-WhatsApp', '◊û◊û◊©', '◊ñ◊ï◊®◊ù', '◊û◊ú◊ê◊ö', '◊©◊û◊ô◊ò◊îüòõ', '◊§◊†◊ô', '◊î◊®◊õ◊ë', '◊û◊í◊ô◊¢', '◊©◊ú◊ï◊©◊î', '\\u200f15', '◊ò◊¢◊ù', '◊ê◊°◊™◊õ◊ú', '◊î◊ô◊ï◊ù', '◊ê◊ô◊™◊ï', '◊†◊ì◊û◊î', '◊í◊û◊ï◊®', '◊ú◊ö‚Ä¶', '◊¢◊†◊ô◊ô◊ü', '◊ê◊™◊î', '◊ú◊í◊ô◊ò', '◊ë◊¢◊ë◊ï◊ì◊î', '◊ê◊ë◊ï◊ê', '◊ú◊ß◊®◊ï◊ê', '◊ñ◊ô◊ï', '◊ê◊ï◊™◊ö', '◊û◊ì◊ô◊ô', '◊ë◊í◊ô◊ò', '‚Ä¢', '◊î◊û◊¢◊®◊ë', '◊ú◊ê◊£', '◊ú◊®◊ê◊ï◊™', '◊©◊†◊ï◊ü', '◊§◊ô◊ñ◊ô◊™', '◊®◊ê◊©', '◊®◊ê◊ô◊™', '◊ô◊õ◊ï◊ú', '◊ú◊î◊ë◊ô◊ü', '◊©◊í◊ù', '◊ô◊ó◊°◊ô◊™', '◊ú◊î◊í◊ô◊¢', '◊ê◊ï', '◊ú◊°◊ô◊û◊°◊ò◊®', 'Need.pdf', '◊û◊ß◊ï◊ï◊î', '◊®◊ß', '◊§◊†◊ô◊ö', '◊ú◊¢◊¶◊û◊ù', '◊ë◊ê◊ï◊ï◊ô◊®◊î', '◊ô◊ï◊ì◊¢', '◊ï◊ê◊™◊ó◊ô◊ú', '◊ó◊©◊ë◊™◊ô', '◊ó◊ï◊ñ◊®', '◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™üòâ', '◊¢◊ú', '◊©◊™◊§◊°', '◊ê◊ó', '◊î◊†◊§◊ß◊™', '◊ò◊†◊ß◊ô◊ï‚ù§Ô∏è', 'gpt', '◊ô◊©', '◊ß◊¶◊™', '!', '◊©◊ú◊ê', '◊ó◊ë◊®◊ô◊ù', '◊ë◊®◊ß', '◊©◊ô◊†◊ô◊™', '◊™◊¢◊ì◊õ◊ü', '◊ë◊ò◊ó', '◊ú◊¶◊¢◊®◊ô', '◊ú◊î◊™◊ß◊ì◊ù', '◊©◊ñ◊î', '◊™◊ê◊û◊ô◊ü', '◊î◊®◊ë◊î', '◊î◊§◊®◊ï◊¢', '◊ê◊ó◊ú◊ï◊™', '◊©◊ú◊ô', 'LM', '3', '◊®◊¶◊ô◊™◊ô', '◊ê◊ó◊ô', '◊©◊ô◊û◊ï◊©', '◊î◊ê◊û◊™', '◊¢◊ï◊û◊°', 'main.py', '◊ú◊¢◊†◊ô◊ô◊†◊ô◊ù', '◊û◊¶◊ê◊™', '◊ú◊ë◊ï◊ê', '◊û◊ó◊ô◊ú', '◊ë◊õ◊ô◊™◊î', '◊°◊ï◊§◊©', '◊ë◊©◊ë◊ï◊¢', 'storage.py', '◊ë◊ú◊û◊ô◊ì◊î', '◊ë◊™◊ß◊ï◊ï◊î', '◊ó◊ó', ':', '◊ê◊†◊ô', '◊ê◊ú', '◊ú◊†◊°◊ï◊™', '◊î◊ô◊ô◊ì◊î', '◊™◊§◊ô◊ú◊î', '◊†◊©◊û◊¢', '◊ï◊î◊ï◊®◊ô◊ì◊ï', '◊ô◊ê◊ú◊ú◊ï◊™', '◊¢◊ï◊©◊î', '◊î◊ú◊ï', '◊©◊ï◊ï◊î', '◊û◊û◊©◊ô◊ö', '◊î◊°◊§◊ß◊™◊ô', '◊©◊ï◊ù', '◊õ◊û◊¢◊ò', '◊™◊©◊ï◊ë◊î', '◊™◊õ◊£', '◊ô◊ï◊ù', '◊ú◊ß◊ì◊ù', '◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™', '◊ï◊ê◊û◊©◊ô◊ö', '◊û◊¶◊ô◊í', 'üòÜ', '◊õ◊ô◊ï◊ï◊†◊ô◊ù', '◊ú◊©◊ë◊™', '4', '◊©◊î◊ô◊ê', '◊¶◊ì◊ô◊ß', '◊î◊ô◊ô◊ò◊ß◊°', '◊ë◊™◊õ◊ú◊°', '◊†◊®◊ê◊î', '◊î◊û◊™◊†◊î', '◊™◊©◊ê◊ô◊®', '2024-02-13', '◊ú◊î◊°◊™◊ë◊®◊ï◊™', '◊ï◊î◊©◊ô◊ó◊ï◊™', '◊ì◊ô◊ô◊ò◊ô◊ù', '◊©◊ô◊î◊ô◊î', '◊§◊®◊§◊®', '◊ú◊ì◊ó◊ï◊£', '◊î◊©◊ë◊ï◊¢◊ô◊ô◊ù', '◊ô◊ó◊ñ◊ô◊®◊ï', '◊î◊û◊°◊õ◊ù', '◊ú◊ó◊†◊ô◊î', '◊©◊ô◊ï◊ì◊¢◊ô◊ù', '18.57.pdf', '◊ò◊†◊ß◊ô◊ïüôÉ', '◊ê◊ô◊ú◊™', 'finalProject.pdf', '◊î◊û◊†◊ó◊î', '◊©◊õ◊ó◊™◊ù', '◊ê◊†◊©◊ô◊ù', '◊©◊û◊ë◊ô◊ü', '\\u200f◊î◊î◊ï◊ì◊¢◊ï◊™', '◊î◊ó◊ì◊©', '◊¢◊ï◊ì', '◊û◊î◊®◊©◊ô◊û◊î', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊û◊ë◊ó◊ü', '.', 'parsing.py', '◊ë◊ú◊ô', '◊™◊î◊†◊î', '◊©◊ô◊¢◊ï◊®◊ô', '◊õ◊ü', '◊ë◊ô◊û◊ô◊†◊ï', '◊®◊ï◊¶◊î', '◊ê◊¶◊ú◊ô', '◊ú◊ê◊ó◊®◊ô◊ù', '◊ò◊¢◊ï◊™', '◊ê◊ë◊ú', '◊û◊©◊ù', '◊§◊î', '◊©◊ê◊™◊ù', '◊î◊ú◊ö', '◊î◊¢◊ú◊™◊î', '◊û', '◊û◊®◊í◊ô◊©', '◊ô◊ê◊ó', '◊™◊®◊¶◊î', '◊¢◊ì◊ô◊£', '◊©◊û◊¢', '◊®◊¶◊ô◊†◊ô', '◊ú◊ñ◊î', '◊©◊ô◊§◊®◊ó', '◊ë◊°◊ì◊®', '◊ß◊ï◊®◊î', '◊î◊ï◊©◊û◊ò', '◊ß◊ô◊†◊í', '◊û◊ï◊¶◊§◊†◊ï◊™', '◊û◊ê◊û◊®◊ô◊ù', '..', '◊ß◊ú◊ê◊ë', '◊î◊ì◊®◊ö', '◊ë◊ê', '◊î◊ë◊ê', 'nlpü•≤', '◊©◊ê◊†◊ô', '◊©◊©◊ú◊ó◊ï', '\\u200f◊î◊™◊û◊ï◊†◊î', '◊î5', '◊©◊¢◊©◊ô◊™', '◊ì◊§◊ô◊ù', '◊û◊ß◊ï◊ù', '◊°◊§◊ô◊í◊î', '◊©◊†◊™', '◊ò◊†◊ß◊ô◊ï', '◊¢◊û◊ï◊ß◊î+', '◊ß◊ì◊ô◊û◊î', '◊ë', '◊©◊î◊ô◊î', '◊ê◊ô◊™◊ö', '◊ê◊ô◊ñ◊î', '◊ë◊ñ◊ï◊ù', '◊û◊ï◊¶◊ú◊ó', '◊°◊ë◊ë◊î', 'Is', '◊ê◊§◊©◊®◊ï◊™', '◊†◊©◊ê◊®◊ï', '◊ô◊î◊ô◊î', '◊ú◊ö', '◊ê◊ï◊ï◊ô◊®◊î', '◊™◊©◊û◊¢', '◊î◊§◊¢◊ù', 'üôèüèª', '◊ô◊ê◊ú◊î', '◊ú◊ë◊¶◊¢', '◊õ◊ë◊®', '◊ê◊ó◊ñ◊ô◊®', '◊ú◊¢◊©◊ï◊™', '◊ê◊™', 'CamScanner', '◊ú2', '◊î◊†◊ï◊©◊ê', '◊ê◊™◊î‚ù§Ô∏è‚ù§Ô∏è', '◊î◊§◊°◊ß◊î', '◊î◊ó◊†◊ô◊î', '◊ë◊û◊î◊®◊î', '◊ê◊ï◊õ◊ú', '◊®◊ê◊ô◊™◊ô', '◊ï◊®◊ü', '◊û◊ê◊û◊ô◊ü', '◊©◊î◊ß◊ú◊ß◊ï◊ú'}\n",
      "{'◊õ◊ê◊ë', '◊ë◊°◊ï◊§◊©', '◊ú◊ê', '◊û◊ß◊ï◊ú◊ß◊ú', '◊í◊†◊ë◊ï', 'üôà', '◊î', '◊ê◊ô◊ü', '◊ê◊ï◊™◊ï', '◊ñ◊î◊ï', '?', '◊©◊ù', '◊î◊ß◊ï◊®◊ß◊ô◊†◊ò', '◊ô◊¢◊ë◊ï◊®', '◊ë◊®◊í◊ú', '◊î◊ß◊ï', '◊û◊©◊î◊ï', '◊™◊¢◊©◊î', '◊ú◊ß◊¶◊î', '\\u200f◊û◊°◊û◊ö', '◊ë◊°◊ï◊ï◊ì◊®', '◊ú◊î◊ü', '◊¢◊ï◊ï◊†◊ï◊™', '◊ú◊†◊ï', ',', '◊§◊®◊ï◊ô◊ß◊ò', '◊î◊ô◊ô◊™◊ô', '◊©◊ë◊ï◊¢', '◊û◊¶◊ú◊ô◊ó', '◊©◊ë◊ê◊û◊™', '◊î◊ô◊î', '◊î◊õ◊ô', '◊ï◊ú◊î◊ê◊ñ◊ô◊ü', '◊ú◊î◊™◊ó◊ô◊ú', '◊û◊°◊ô◊ô◊ù', '◊û◊ó◊ï◊§◊©◊™◊ô', 'üòâ', '◊©◊ú◊ó◊™◊ô', '◊ú', '◊ß◊ï◊û◊ô◊ò', '◊ï◊û◊ó◊®', '◊ó◊ó◊ó', '◊§◊ï◊©', '◊™◊í◊ô◊ì', '◊û◊ß◊¶◊î', '◊ê◊©◊ê◊ú', '◊™◊°◊ú◊ó', '◊ô◊ï◊™◊®', '◊î◊©◊ë◊ï◊¢', '◊í◊ô◊ò', '◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊ê◊ó◊ì', '◊í◊ù', '◊§◊™◊®◊ï◊ü', '  ', '◊°◊ô◊ô◊û◊ï', \"◊ú◊¶'◊ê◊ò\", '◊î◊™◊ë◊ú◊ë◊ú◊™◊ô', '‚Ä¶', '◊†◊ï◊®◊û◊ú◊ô', '◊û◊ê◊§◊ß◊î', '◊û◊™◊õ◊ï◊ï◊ü', '◊õ◊§◊®◊™', '◊ë◊ü', 'LLM', '◊î◊ê◊ú◊î', '◊§◊ó◊ï◊™', '◊û◊ó◊ï◊•', '◊ê◊ï◊™◊ü', '◊ë◊ì◊ì', '◊ê◊í◊ô◊¢', 'Ô∏è', '◊§◊ï◊®◊ó', '◊ê◊î◊ú◊ü', '◊î◊§◊®◊ï◊ô◊ß◊ò', '\\u200f◊°◊ò◊ô◊ß◊®', '◊ê◊©◊ú◊ó', 'you', '◊î◊ô◊ï', 'üôè', '◊û◊®◊ó◊ï◊ß', '◊ú◊ï◊ï◊ì◊ê', '◊¶◊®◊ô◊õ◊ô◊ù', '◊ë◊ô◊ó◊ì', '◊û◊®◊í◊©', '◊î◊ó◊ú◊ï◊û◊ï◊™', 'be', '◊ú◊î◊™◊ó◊ë◊®', '◊û◊ê◊ï◊™◊ï', '◊ó◊ë◊®', '◊®◊ô◊¢◊†◊ï◊ü', '◊ó◊ë◊®◊î', '◊ë◊ú◊ë◊ï◊ú◊ô◊ù', '◊ë◊©◊ë◊ô◊ú', '◊û◊™◊ï◊ö', 'üòõ', '\\u200f7', '◊î◊ï◊©◊û◊ò◊î', '◊ë◊ô◊†◊™◊ô◊ô◊ù', '\\u200f11', '◊¢◊ù', '◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ô◊ù', '◊ê◊ô◊ö', '◊©◊ú◊ï◊û◊ö', '◊ò◊ï◊ë', '◊î◊û◊ß◊ï◊ù', '◊ê◊ñ', '◊™◊®◊ê◊î', '◊î◊ï◊ò◊ú', 'Attention', '◊ó◊ô◊†◊ù', '◊ó◊ñ◊®◊™◊ô', '◊ë◊ô◊™', '◊ñ◊î', '◊ú◊ô', '◊†◊ï◊©◊ê◊ô◊ù', '◊ë◊õ◊ï◊†◊†◊ï◊™', '◊©◊†◊ô◊î', '◊ú◊©◊ú◊ï◊ó', 'üôÉ', '◊ë◊©◊ï◊ß', '◊ë◊§◊®◊ï◊ô◊ß◊ò', '◊ë◊ß◊ë◊ï◊¶◊î', '◊û◊¢◊ì◊ô◊£', '◊ê◊ï◊ú◊ô', '◊û◊û◊™◊ô◊†◊ô◊ù', '◊ô◊ß◊®', '◊î◊¢◊ô◊ü', '◊™◊î◊ô◊î', '◊ó◊ô◊ú', 'WhatsApp', '◊õ◊ú', '◊ô◊ê', 'finalproject.pdf', '2', '◊¢◊õ◊©◊ô◊ï', '◊§◊†◊ô◊ï◊™', '◊û◊í◊†◊ô◊ë', '◊ê◊†◊ó◊†◊ï', '◊ó◊ó◊ó◊ó', '◊ê◊ù', '◊ê◊ô◊§◊î', '◊û◊ó◊®', '◊î◊ñ◊î', '◊ú◊ê◊®◊ó', '◊©◊û◊ô◊ò◊î', '◊ê◊ó◊®◊ô◊ù', '◊ë◊í◊ú◊ú', '◊ê◊ú◊ö', '◊û◊î', '◊ú◊ë◊ó◊ï◊®', '◊û◊ï◊™◊ß', '◊©◊ú◊†◊ï', '◊ú◊í◊û◊®◊ô', '◊õ◊ö', '◊ê◊†◊°◊î', '◊¶◊®◊ô◊ö', '◊ê◊ë◊ì◊ï◊ß', '◊û◊û◊©', '◊ñ◊ï◊®◊ù', '◊û◊ú◊ê◊ö', '◊§◊†◊ô', '◊î◊®◊õ◊ë', '◊û◊í◊ô◊¢', '◊©◊ú◊ï◊©◊î', '\\u200f15', '◊ò◊¢◊ù', '◊ê◊°◊™◊õ◊ú', '◊î◊ô◊ï◊ù', '◊ê◊ô◊™◊ï', '◊†◊ì◊û◊î', '◊í◊û◊ï◊®', '◊¢◊†◊ô◊ô◊ü', '◊ê◊™◊î', '◊ú◊í◊ô◊ò', '◊ë◊¢◊ë◊ï◊ì◊î', '◊ê◊ë◊ï◊ê', '◊ú◊ß◊®◊ï◊ê', '◊ñ◊ô◊ï', '◊ê◊ï◊™◊ö', '◊û◊ì◊ô◊ô', '◊ë◊í◊ô◊ò', '‚Ä¢', '◊î◊û◊¢◊®◊ë', '◊ú◊ê◊£', '◊ú◊®◊ê◊ï◊™', '◊©◊†◊ï◊ü', '◊§◊ô◊ñ◊ô◊™', '◊®◊ê◊©', '◊®◊ê◊ô◊™', '◊ô◊õ◊ï◊ú', '◊ú◊î◊ë◊ô◊ü', '◊©◊í◊ù', '◊ô◊ó◊°◊ô◊™', '◊ú◊î◊í◊ô◊¢', '◊ê◊ï', '◊ú◊°◊ô◊û◊°◊ò◊®', 'Need.pdf', '◊û◊ß◊ï◊ï◊î', '◊®◊ß', '◊§◊†◊ô◊ö', '◊ú◊¢◊¶◊û◊ù', '◊ë◊ê◊ï◊ï◊ô◊®◊î', '◊ô◊ï◊ì◊¢', '◊ï◊ê◊™◊ó◊ô◊ú', '◊ó◊©◊ë◊™◊ô', '◊ó◊ï◊ñ◊®', '◊¢◊ú', '◊©◊™◊§◊°', '◊ê◊ó', '◊î◊†◊§◊ß◊™', 'gpt', '◊ô◊©', '◊ß◊¶◊™', '!', '◊©◊ú◊ê', '◊ó◊ë◊®◊ô◊ù', '◊ë◊®◊ß', '◊©◊ô◊†◊ô◊™', '-', '◊™◊¢◊ì◊õ◊ü', '◊ë◊ò◊ó', '◊ú◊¶◊¢◊®◊ô', '◊ú◊î◊™◊ß◊ì◊ù', '◊©◊ñ◊î', '◊™◊ê◊û◊ô◊ü', '◊î◊®◊ë◊î', '◊î◊§◊®◊ï◊¢', '◊ê◊ó◊ú◊ï◊™', '◊©◊ú◊ô', 'LM', 'üèª', '3', '◊®◊¶◊ô◊™◊ô', '◊ê◊ó◊ô', '◊©◊ô◊û◊ï◊©', '◊î◊ê◊û◊™', '◊¢◊ï◊û◊°', 'main.py', '◊ú◊¢◊†◊ô◊ô◊†◊ô◊ù', '◊û◊¶◊ê◊™', '◊ú◊ë◊ï◊ê', '◊û◊ó◊ô◊ú', '◊ë◊õ◊ô◊™◊î', '◊°◊ï◊§◊©', '◊ë◊©◊ë◊ï◊¢', 'storage.py', '◊ë◊ú◊û◊ô◊ì◊î', '◊ë◊™◊ß◊ï◊ï◊î', 'all', '◊ó◊ó', ':', '◊ê◊†◊ô', '◊ê◊ú', '◊ú◊†◊°◊ï◊™', '◊î◊ô◊ô◊ì◊î', '◊™◊§◊ô◊ú◊î', '◊†◊©◊û◊¢', '◊ï◊î◊ï◊®◊ô◊ì◊ï', '◊ô◊ê◊ú◊ú◊ï◊™', '◊¢◊ï◊©◊î', '◊î◊ú◊ï', '◊©◊ï◊ï◊î', '◊û◊û◊©◊ô◊ö', '◊î◊°◊§◊ß◊™◊ô', '◊©◊ï◊ù', '◊õ◊û◊¢◊ò', '◊™◊©◊ï◊ë◊î', '◊™◊õ◊£', '◊ô◊ï◊ù', '◊ú◊ß◊ì◊ù', '◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™', '◊ï◊ê◊û◊©◊ô◊ö', '◊û◊¶◊ô◊í', 'üòÜ', '02', '◊õ◊ô◊ï◊ï◊†◊ô◊ù', '◊ú◊©◊ë◊™', '4', '◊©◊î◊ô◊ê', '◊¶◊ì◊ô◊ß', '◊î◊ô◊ô◊ò◊ß◊°', '◊ë◊™◊õ◊ú◊°', '◊†◊®◊ê◊î', '◊î◊û◊™◊†◊î', '◊™◊©◊ê◊ô◊®', '◊ú◊î◊°◊™◊ë◊®◊ï◊™', '◊ï◊î◊©◊ô◊ó◊ï◊™', '◊ì◊ô◊ô◊ò◊ô◊ù', '◊©◊ô◊î◊ô◊î', '◊§◊®◊§◊®', '◊ú◊ì◊ó◊ï◊£', '◊î◊©◊ë◊ï◊¢◊ô◊ô◊ù', '◊ô◊ó◊ñ◊ô◊®◊ï', '◊î◊û◊°◊õ◊ù', '◊ú◊ó◊†◊ô◊î', '◊©◊ô◊ï◊ì◊¢◊ô◊ù', '18.57.pdf', '◊ê◊ô◊ú◊™', '◊î◊û◊†◊ó◊î', '◊©◊õ◊ó◊™◊ù', '◊ê◊†◊©◊ô◊ù', '◊©◊û◊ë◊ô◊ü', '\\u200f◊î◊î◊ï◊ì◊¢◊ï◊™', '◊î◊ó◊ì◊©', '◊¢◊ï◊ì', '◊û◊î◊®◊©◊ô◊û◊î', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊û◊ë◊ó◊ü', '.', 'parsing.py', '◊ë◊ú◊ô', '◊™◊î◊†◊î', '◊©◊ô◊¢◊ï◊®◊ô', '◊õ◊ü', '◊ë◊ô◊û◊ô◊†◊ï', '◊®◊ï◊¶◊î', '◊ê◊¶◊ú◊ô', '◊ú◊ê◊ó◊®◊ô◊ù', '◊ò◊¢◊ï◊™', '◊ê◊ë◊ú', '◊û◊©◊ù', '◊§◊î', '◊©◊ê◊™◊ù', '◊î◊ú◊ö', '◊î◊¢◊ú◊™◊î', '◊û', '◊û◊®◊í◊ô◊©', '◊ô◊ê◊ó', '◊™◊®◊¶◊î', '◊¢◊ì◊ô◊£', '◊©◊û◊¢', '‚ù§', '◊®◊¶◊ô◊†◊ô', '◊ú◊ñ◊î', '13', '◊©◊ô◊§◊®◊ó', '◊ë◊°◊ì◊®', '◊ß◊ï◊®◊î', '◊î◊ï◊©◊û◊ò', '◊ß◊ô◊†◊í', '◊û◊ï◊¶◊§◊†◊ï◊™', '◊û◊ê◊û◊®◊ô◊ù', '..', '◊ß◊ú◊ê◊ë', '◊î◊ì◊®◊ö', '◊ë◊ê', '◊î◊ë◊ê', 'nlpü•≤', '◊©◊ê◊†◊ô', '◊©◊©◊ú◊ó◊ï', '\\u200f◊î◊™◊û◊ï◊†◊î', '◊î5', '◊©◊¢◊©◊ô◊™', '◊ì◊§◊ô◊ù', '◊û◊ß◊ï◊ù', '◊°◊§◊ô◊í◊î', '◊©◊†◊™', '◊ò◊†◊ß◊ô◊ï', '◊¢◊û◊ï◊ß◊î+', '◊ß◊ì◊ô◊û◊î', '◊ë', '◊©◊î◊ô◊î', '◊ê◊ô◊™◊ö', '◊ê◊ô◊ñ◊î', '◊ë◊ñ◊ï◊ù', '◊û◊ï◊¶◊ú◊ó', '◊°◊ë◊ë◊î', '◊ê◊§◊©◊®◊ï◊™', '◊†◊©◊ê◊®◊ï', '◊ô◊î◊ô◊î', '◊ú◊ö', '◊ê◊ï◊ï◊ô◊®◊î', '◊™◊©◊û◊¢', '2024', '◊î◊§◊¢◊ù', '◊ô◊ê◊ú◊î', '◊ú◊ë◊¶◊¢', '◊õ◊ë◊®', '◊ê◊ó◊ñ◊ô◊®', '◊ú◊¢◊©◊ï◊™', '◊ê◊™', 'CamScanner', '◊ú2', '◊î◊†◊ï◊©◊ê', '◊î◊§◊°◊ß◊î', '◊î◊ó◊†◊ô◊î', '◊ë◊û◊î◊®◊î', '◊ê◊ï◊õ◊ú', '◊®◊ê◊ô◊™◊ô', '◊ï◊®◊ü', '◊û◊ê◊û◊ô◊ü', '◊©◊î◊ß◊ú◊ß◊ï◊ú'}\n"
     ]
    }
   ],
   "source": [
    "nltk_lemmas_set_whatsapp, spacy_lemmas_set_whatsapp = text_functions.create_lemma_set(nltk_tokens_set_whatsapp, spacy_tokens_set_whatsapp, nlp)\n",
    "# Output the results\n",
    "print(\"Number of unique lemmas (NLTK):\", len(nltk_lemmas_set_whatsapp))\n",
    "print(\"Number of unique lemmas (spaCy):\", len(spacy_lemmas_set_whatsapp))\n",
    "\n",
    "print(nltk_lemmas_set_whatsapp)\n",
    "print(spacy_lemmas_set_whatsapp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stemms (NLTK): 387\n",
      "{'◊õ◊ê◊ë', '◊ë◊°◊ï◊§◊©', '◊ú◊ê', '◊û◊ß◊ï◊ú◊ß◊ú', '◊í◊†◊ë◊ï', '◊î', '◊ê◊ô◊ü', '◊ê◊ï◊™◊ï', '◊ñ◊î◊ï', '?', '◊©◊ù', '◊î◊ß◊ï◊®◊ß◊ô◊†◊ò', '◊ô◊¢◊ë◊ï◊®', '◊ë◊®◊í◊ú', '◊î◊ß◊ï', '◊û◊©◊î◊ï', '◊™◊¢◊©◊î', '◊ú◊ß◊¶◊î', '\\u200f◊û◊°◊û◊ö', '◊ë◊°◊ï◊ï◊ì◊®', '◊ú◊î◊ü', '◊¢◊ï◊ï◊†◊ï◊™', '◊ú◊†◊ï', 'llm', ',', '◊§◊®◊ï◊ô◊ß◊ò', '◊î◊ô◊ô◊™◊ô', '◊©◊ë◊ï◊¢', '◊û◊¶◊ú◊ô◊ó', '◊©◊ë◊ê◊û◊™', '◊î◊ô◊î', '◊î◊õ◊ô', '◊ï◊ú◊î◊ê◊ñ◊ô◊ü', '◊ú◊î◊™◊ó◊ô◊ú', '◊û◊°◊ô◊ô◊ù', '◊ú-whatsapp', '◊û◊ó◊ï◊§◊©◊™◊ô', '◊©◊ú◊ó◊™◊ô', '◊ú', '◊ß◊ï◊û◊ô◊ò', '◊ï◊û◊ó◊®', '◊ó◊ó◊ó', '◊§◊ï◊©', '◊™◊í◊ô◊ì', '◊û◊ß◊¶◊î', '◊ê◊©◊ê◊ú', '◊™◊°◊ú◊ó', '◊ô◊ï◊™◊®', '◊î◊©◊ë◊ï◊¢', '◊í◊ô◊ò', '◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊ê◊ó◊ì', 'is', '◊í◊ù', '◊§◊™◊®◊ï◊ü', '◊°◊ô◊ô◊û◊ï', \"◊ú◊¶'◊ê◊ò\", '◊î◊™◊ë◊ú◊ë◊ú◊™◊ô', '◊†◊ï◊®◊û◊ú◊ô', '◊û◊ê◊§◊ß◊î', '◊û◊™◊õ◊ï◊ï◊ü', '◊õ◊§◊®◊™', '◊ë◊ü', '◊î◊ê◊ú◊î', '◊§◊ó◊ï◊™', '◊û◊ó◊ï◊•', '◊ê◊ï◊™◊ü', '◊ë◊ì◊ì', '◊ê◊í◊ô◊¢', '◊§◊ï◊®◊ó', '◊ê◊î◊ú◊ü', '◊î◊§◊®◊ï◊ô◊ß◊ò', '\\u200f◊°◊ò◊ô◊ß◊®', '◊ê◊©◊ú◊ó', 'you', '◊î◊ô◊ï', '◊¶◊®◊ô◊õ◊ô◊ù', '◊û◊®◊ó◊ï◊ß', '◊ú◊ï◊ï◊ì◊ê', '◊ë◊ô◊ó◊ì', '◊û◊®◊í◊©', '◊î◊ó◊ú◊ï◊û◊ï◊™', 'parsing.pi', '◊ú◊î◊™◊ó◊ë◊®', '◊û◊ê◊ï◊™◊ï', '◊ó◊ë◊®', '◊®◊ô◊¢◊†◊ï◊ü', '◊ó◊ë◊®◊î', '◊ë◊ú◊ë◊ï◊ú◊ô◊ù', '◊ë◊©◊ë◊ô◊ú', '◊û◊™◊ï◊ö', '\\u200f7', '◊î◊ï◊©◊û◊ò◊î', '◊ë◊ô◊†◊™◊ô◊ô◊ù', '\\u200f11', '◊î◊ß◊ï◊®◊ß◊ô◊†◊òüôà', '◊¢◊ù', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò◊ô◊ù', '◊ê◊ô◊ö', '◊©◊ú◊ï◊û◊ö', '◊ò◊ï◊ë', '◊î◊û◊ß◊ï◊ù', '◊ê◊ñ', '◊™◊®◊ê◊î', '◊î◊ï◊ò◊ú', '◊ó◊ô◊†◊ù', '◊ó◊ñ◊®◊™◊ô', '◊ë◊ô◊™', '◊ñ◊î', '◊ú◊ô', '◊†◊ï◊©◊ê◊ô◊ù', '◊ë◊õ◊ï◊†◊†◊ï◊™', '◊©◊†◊ô◊î', '◊ú◊©◊ú◊ï◊ó', '◊ë◊©◊ï◊ß', '◊ë◊§◊®◊ï◊ô◊ß◊ò', '◊ë◊ß◊ë◊ï◊¶◊î', '◊û◊¢◊ì◊ô◊£', '◊ê◊ï◊ú◊ô', '◊û◊û◊™◊ô◊†◊ô◊ù', '◊ô◊ß◊®', '◊î◊¢◊ô◊ü', '◊™◊î◊ô◊î', '◊ó◊ô◊ú', '◊õ◊ú', '◊ô◊ê', 'finalproject.pdf', '2', '◊¢◊õ◊©◊ô◊ï', '◊§◊†◊ô◊ï◊™', '◊û◊í◊†◊ô◊ë', '◊ê◊†◊ó◊†◊ï', '◊ó◊ó◊ó◊ó', '◊ê◊ù', '◊ê◊ô◊§◊î', 'lm', 'need.pdf', '◊û◊ó◊®', '◊î◊ñ◊î', '◊ú◊ê◊®◊ó', '◊ê◊ó◊®◊ô◊ù', '◊ë◊í◊ú◊ú', '◊ê◊ú◊ö', '◊û◊î', '◊ú◊ë◊ó◊ï◊®', '◊û◊ï◊™◊ß', '◊©◊ú◊†◊ï', '◊ú◊í◊û◊®◊ô', '◊õ◊ö', '◊ê◊†◊°◊î', '◊¶◊®◊ô◊ö', '◊ê◊ë◊ì◊ï◊ß', '◊û◊û◊©', '◊ñ◊ï◊®◊ù', '◊û◊ú◊ê◊ö', '◊©◊û◊ô◊ò◊îüòõ', '◊§◊†◊ô', '◊î◊®◊õ◊ë', '◊û◊í◊ô◊¢', '◊©◊ú◊ï◊©◊î', 'attent', '\\u200f15', '◊ò◊¢◊ù', 'main.pi', '◊ê◊°◊™◊õ◊ú', '◊î◊ô◊ï◊ù', '◊ê◊ô◊™◊ï', '◊†◊ì◊û◊î', '◊í◊û◊ï◊®', '◊ú◊ö‚Ä¶', '◊¢◊†◊ô◊ô◊ü', '◊ê◊™◊î', '◊ú◊í◊ô◊ò', '◊ë◊¢◊ë◊ï◊ì◊î', '◊ê◊ë◊ï◊ê', '◊ú◊ß◊®◊ï◊ê', '◊ñ◊ô◊ï', '◊ê◊ï◊™◊ö', '◊û◊ì◊ô◊ô', '◊ë◊í◊ô◊ò', '‚Ä¢', '◊î◊û◊¢◊®◊ë', '◊ú◊ê◊£', '◊ú◊®◊ê◊ï◊™', '◊©◊†◊ï◊ü', '◊§◊ô◊ñ◊ô◊™', '◊®◊ê◊©', '◊®◊ê◊ô◊™', '◊ô◊õ◊ï◊ú', '◊ú◊î◊ë◊ô◊ü', '◊©◊í◊ù', '◊ô◊ó◊°◊ô◊™', '◊ú◊î◊í◊ô◊¢', '◊ê◊ï', '◊ú◊°◊ô◊û◊°◊ò◊®', '◊û◊ß◊ï◊ï◊î', '◊®◊ß', '◊§◊†◊ô◊ö', '◊ú◊¢◊¶◊û◊ù', '◊ë◊ê◊ï◊ï◊ô◊®◊î', '◊ô◊ï◊ì◊¢', '◊ï◊ê◊™◊ó◊ô◊ú', '◊ó◊©◊ë◊™◊ô', '◊ó◊ï◊ñ◊®', '◊î◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™üòâ', '◊¢◊ú', '◊©◊™◊§◊°', '◊ê◊ó', '◊î◊†◊§◊ß◊™', '◊ò◊†◊ß◊ô◊ï‚ù§Ô∏è', 'gpt', '◊ô◊©', '◊ß◊¶◊™', '!', '◊©◊ú◊ê', '◊ó◊ë◊®◊ô◊ù', '◊ë◊®◊ß', '◊©◊ô◊†◊ô◊™', '◊™◊¢◊ì◊õ◊ü', '◊ë◊ò◊ó', '◊ú◊¶◊¢◊®◊ô', '◊ú◊î◊™◊ß◊ì◊ù', '◊©◊ñ◊î', '◊™◊ê◊û◊ô◊ü', '◊î◊®◊ë◊î', '◊î◊§◊®◊ï◊¢', '◊ê◊ó◊ú◊ï◊™', '◊©◊ú◊ô', '3', '◊®◊¶◊ô◊™◊ô', '◊ê◊ó◊ô', '◊©◊ô◊û◊ï◊©', '◊î◊ê◊û◊™', '◊¢◊ï◊û◊°', '◊ú◊¢◊†◊ô◊ô◊†◊ô◊ù', '◊û◊¶◊ê◊™', '◊ú◊ë◊ï◊ê', '◊û◊ó◊ô◊ú', '◊ë◊õ◊ô◊™◊î', '◊°◊ï◊§◊©', '◊ë◊©◊ë◊ï◊¢', '◊ë◊ú◊û◊ô◊ì◊î', '◊ë◊™◊ß◊ï◊ï◊î', 'all', '◊ó◊ó', ':', '◊ê◊†◊ô', '◊ê◊ú', '◊ú◊†◊°◊ï◊™', '◊î◊ô◊ô◊ì◊î', '◊™◊§◊ô◊ú◊î', '◊†◊©◊û◊¢', '◊ï◊î◊ï◊®◊ô◊ì◊ï', '◊ô◊ê◊ú◊ú◊ï◊™', '◊¢◊ï◊©◊î', '◊î◊ú◊ï', '◊©◊ï◊ï◊î', '◊û◊û◊©◊ô◊ö', '◊î◊°◊§◊ß◊™◊ô', '◊©◊ï◊ù', '◊õ◊û◊¢◊ò', '◊™◊©◊ï◊ë◊î', '◊™◊õ◊£', '◊ô◊ï◊ù', '◊ú◊ß◊ì◊ù', '◊°◊ò◊ï◊ì◊†◊ò◊ô◊ê◊ú◊ô◊™', '◊ï◊ê◊û◊©◊ô◊ö', '◊û◊¶◊ô◊í', 'üòÜ', '◊õ◊ô◊ï◊ï◊†◊ô◊ù', '◊ú◊©◊ë◊™', '4', '◊©◊î◊ô◊ê', '◊¶◊ì◊ô◊ß', '◊î◊ô◊ô◊ò◊ß◊°', '◊ë◊™◊õ◊ú◊°', '◊†◊®◊ê◊î', '◊î◊û◊™◊†◊î', '◊™◊©◊ê◊ô◊®', '2024-02-13', '◊ú◊î◊°◊™◊ë◊®◊ï◊™', '◊ï◊î◊©◊ô◊ó◊ï◊™', '◊ì◊ô◊ô◊ò◊ô◊ù', '◊©◊ô◊î◊ô◊î', '◊§◊®◊§◊®', '◊ú◊ì◊ó◊ï◊£', '◊î◊©◊ë◊ï◊¢◊ô◊ô◊ù', '◊ô◊ó◊ñ◊ô◊®◊ï', '◊î◊û◊°◊õ◊ù', '◊ú◊ó◊†◊ô◊î', '◊©◊ô◊ï◊ì◊¢◊ô◊ù', '18.57.pdf', '◊ò◊†◊ß◊ô◊ïüôÉ', '◊ê◊ô◊ú◊™', 'camscann', '◊î◊û◊†◊ó◊î', '◊©◊õ◊ó◊™◊ù', '◊ê◊†◊©◊ô◊ù', 'storage.pi', '◊©◊û◊ë◊ô◊ü', '\\u200f◊î◊î◊ï◊ì◊¢◊ï◊™', '◊î◊ó◊ì◊©', '◊¢◊ï◊ì', '◊û◊î◊®◊©◊ô◊û◊î', '◊î◊§◊®◊ï◊ô◊ô◊ß◊ò', '◊û◊ë◊ó◊ü', '.', '◊ë◊ú◊ô', '◊™◊î◊†◊î', '◊©◊ô◊¢◊ï◊®◊ô', '◊õ◊ü', '◊ë◊ô◊û◊ô◊†◊ï', '◊®◊ï◊¶◊î', '◊ê◊¶◊ú◊ô', '◊ú◊ê◊ó◊®◊ô◊ù', '◊ò◊¢◊ï◊™', '◊ê◊ë◊ú', '◊û◊©◊ù', '◊§◊î', '◊©◊ê◊™◊ù', '◊î◊ú◊ö', '◊î◊¢◊ú◊™◊î', '◊û', '◊û◊®◊í◊ô◊©', '◊ô◊ê◊ó', '◊™◊®◊¶◊î', '◊¢◊ì◊ô◊£', '◊©◊û◊¢', '◊®◊¶◊ô◊†◊ô', '◊ú◊ñ◊î', '◊©◊ô◊§◊®◊ó', '◊ë◊°◊ì◊®', '◊ß◊ï◊®◊î', '◊î◊ï◊©◊û◊ò', '◊ß◊ô◊†◊í', '◊û◊ï◊¶◊§◊†◊ï◊™', '◊û◊ê◊û◊®◊ô◊ù', '..', '◊ß◊ú◊ê◊ë', '◊î◊ì◊®◊ö', '◊ë◊ê', '◊î◊ë◊ê', 'nlpü•≤', '◊©◊ê◊†◊ô', '◊©◊©◊ú◊ó◊ï', '\\u200f◊î◊™◊û◊ï◊†◊î', '◊î5', '◊©◊¢◊©◊ô◊™', '◊ì◊§◊ô◊ù', '◊û◊ß◊ï◊ù', '◊°◊§◊ô◊í◊î', '◊©◊†◊™', '◊ò◊†◊ß◊ô◊ï', '◊¢◊û◊ï◊ß◊î+', '◊ß◊ì◊ô◊û◊î', '◊ë', '◊©◊î◊ô◊î', '◊ê◊ô◊™◊ö', '◊ê◊ô◊ñ◊î', '◊ë◊ñ◊ï◊ù', '◊û◊ï◊¶◊ú◊ó', '◊°◊ë◊ë◊î', '◊ê◊§◊©◊®◊ï◊™', '◊†◊©◊ê◊®◊ï', '◊ô◊î◊ô◊î', '◊ú◊ö', '◊ê◊ï◊ï◊ô◊®◊î', '◊™◊©◊û◊¢', '◊î◊§◊¢◊ù', 'üôèüèª', '◊ô◊ê◊ú◊î', '◊ú◊ë◊¶◊¢', '◊õ◊ë◊®', '◊ê◊ó◊ñ◊ô◊®', '◊ú◊¢◊©◊ï◊™', '◊ê◊™', '◊ú2', '◊î◊†◊ï◊©◊ê', '◊ê◊™◊î‚ù§Ô∏è‚ù§Ô∏è', '◊î◊§◊°◊ß◊î', '◊î◊ó◊†◊ô◊î', '◊ë◊û◊î◊®◊î', '◊ê◊ï◊õ◊ú', '◊®◊ê◊ô◊™◊ô', '◊ï◊®◊ü', '◊û◊ê◊û◊ô◊ü', '◊©◊î◊ß◊ú◊ß◊ï◊ú'}\n"
     ]
    }
   ],
   "source": [
    "nltk_stemmed_set_whatsapp = text_functions.nltk_set_stemming(nltk_tokens_set_whatsapp)\n",
    "print(\"Number of unique stemms (NLTK):\", len(nltk_stemmed_set_whatsapp))\n",
    "\n",
    "print(nltk_stemmed_set_whatsapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of 17.6 and 17.7:\n",
    "\n",
    "Both NLTK and spaCy appear to lack support for lemmatization and stemming in the Hebrew language. As a result, their functionality for Hebrew text is limited to tokenization, producing identical outputs for tokenization, lemmatization, and stemming operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.8-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No message found that meets the criteria.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function definition\n",
    "def find_message_to_remove(df):\n",
    "    # Initialize stemmer and lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    # Function to preprocess and calculate unique tokens\n",
    "    def get_unique_tokens(messages):\n",
    "        stemmed_tokens = set()\n",
    "        lemmatized_tokens = set()\n",
    "        for msg in messages:\n",
    "            tokens = word_tokenize(msg.lower())\n",
    "            stemmed_tokens.update(stemmer.stem(token) for token in tokens)\n",
    "            lemmatized_tokens.update(lemmatizer.lemmatize(token) for token in tokens)\n",
    "        return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "    # Calculate original token sets\n",
    "    original_stemmed, original_lemmatized = get_unique_tokens(df[\"message\"])\n",
    "\n",
    "    #init flags and string holders for the message removed that make the condition true\n",
    "    msg_less_stem_equal_lemma = \"\"\n",
    "    msg_less_lemma_equal_stem = \"\"\n",
    "\n",
    "    found_msg_1_flag = False\n",
    "    found_msg_2_flag = False\n",
    "\n",
    "\n",
    "    # Find the message to remove\n",
    "    for idx, row in df.iterrows():\n",
    "        # Remove one message\n",
    "        remaining_messages = df.drop(index=idx)[\"message\"]\n",
    "        stemmed, lemmatized = get_unique_tokens(remaining_messages)\n",
    "        \n",
    "        # Check the conditions\n",
    "        if len(stemmed) < len(original_stemmed) and len(lemmatized) == len(original_lemmatized):\n",
    "            print(f\"Message  at index {idx}  causes a change in stemmed tokens but not in lemmatized tokens. \\nmessage: \", row[\"message\"])\n",
    "            #return row[\"message\"]\n",
    "            msg_less_stem_equal_lemma = row[\"message\"] \n",
    "            found_msg_1_flag = True\n",
    "        \n",
    "        if len(lemmatized) < len(original_lemmatized) and len(stemmed) == len(original_stemmed):\n",
    "             print(f\"Message at index {idx} causes a change in lemmatized tokens but not in stemmed tokens. \\nmessage:\", row[\"message\"])\n",
    "            # return row[\"message\"]  # Return the message that causes the change\n",
    "             msg_less_lemma_equal_stem = row[\"message\"] \n",
    "             found_msg_2_flag = True\n",
    "\n",
    "        if (found_msg_1_flag == True and found_msg_2_flag== True):\n",
    "            break # Break the loop if such messages found\n",
    "\n",
    "            \n",
    "\n",
    "    if  (found_msg_1_flag == False and found_msg_2_flag == False):      \n",
    "        print(\"No message found that meets the criteria.\")\n",
    "    \n",
    "  \n",
    "   \n",
    "    #return None\n",
    "\n",
    "\n",
    "find_message_to_remove(df_whatsapp_processed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results 17.8-9:\n",
    "\n",
    "For Hebrew messages, due to the limitations of the NLTK and library in handling the Hebrew language effectively, the results of lemmatization and stemming are often identical. This overlap makes it impossible to differentiate between the number of lemmas and stems within the same segment of text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17.10-11\n",
    "\n",
    "Both NLTK and spaCy lack support for lemmatization and stemming in the Hebrew language. As a result, their functionality for Hebrew text is limited to tokenization, producing identical outputs for tokenization, lemmatization, and stemming operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
